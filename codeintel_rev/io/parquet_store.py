"""Parquet storage for chunks and vectors using Arrow.

Stores chunks and embeddings in columnar Parquet format with FixedSizeList
for efficient vector storage and querying via DuckDB.
"""

from __future__ import annotations

from pathlib import Path
from typing import TYPE_CHECKING

import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq

if TYPE_CHECKING:
    from collections.abc import Sequence

    from codeintel_rev.indexing.cast_chunker import Chunk


def get_chunks_schema(vec_dim: int) -> pa.Schema:
    """Get Arrow schema for chunks table.

    Parameters
    ----------
    vec_dim : int
        Embedding dimension.

    Returns
    -------
    pa.Schema
        Arrow schema for chunks with embeddings.
    """
    return pa.schema(
        [
            pa.field("id", pa.int64()),
            pa.field("uri", pa.string()),
            pa.field("start_line", pa.int32()),
            pa.field("end_line", pa.int32()),
            pa.field("start_byte", pa.int64()),
            pa.field("end_byte", pa.int64()),
            pa.field("preview", pa.string()),
            pa.field("embedding", pa.list_(pa.float32(), vec_dim)),
        ]
    )


def write_chunks_parquet(
    output_path: Path,
    chunks: Sequence[Chunk],
    embeddings: np.ndarray,
    start_id: int = 0,
    vec_dim: int = 2560,
) -> None:
    """Write chunks and embeddings to Parquet.

    Parameters
    ----------
    output_path : Path
        Output Parquet file path.
    chunks : Sequence[Chunk]
        Chunk metadata.
    embeddings : np.ndarray
        Embeddings array of shape (len(chunks), vec_dim).
    start_id : int
        Starting ID for chunks.
    vec_dim : int
        Embedding dimension.

    Raises
    ------
    ValueError
        If chunks and embeddings length mismatch.
    """
    if len(chunks) != len(embeddings):
        msg = f"Chunks ({len(chunks)}) and embeddings ({len(embeddings)}) length mismatch"
        raise ValueError(msg)

    # Prepare data
    ids = list(range(start_id, start_id + len(chunks)))
    uris = [c.uri for c in chunks]
    start_lines = [c.start_line for c in chunks]
    end_lines = [c.end_line for c in chunks]
    start_bytes = [c.start_byte for c in chunks]
    end_bytes = [c.end_byte for c in chunks]
    previews = [c.text[:240] for c in chunks]

    # Convert embeddings to FixedSizeList
    embeddings_flat = embeddings.astype(np.float32).ravel()
    embedding_array = pa.FixedSizeListArray.from_arrays(
        pa.array(embeddings_flat, type=pa.float32()), vec_dim
    )

    # Create table
    table = pa.table(
        {
            "id": ids,
            "uri": uris,
            "start_line": start_lines,
            "end_line": end_lines,
            "start_byte": start_bytes,
            "end_byte": end_bytes,
            "preview": previews,
            "embedding": embedding_array,
        },
        schema=get_chunks_schema(vec_dim),
    )

    # Write with compression
    output_path.parent.mkdir(parents=True, exist_ok=True)
    pq.write_table(
        table,
        output_path,
        compression="snappy",
        use_dictionary=["uri"],
    )


def read_chunks_parquet(parquet_path: Path) -> pa.Table:
    """Read chunks from Parquet file.

    Parameters
    ----------
    parquet_path : Path
        Parquet file path.

    Returns
    -------
    pa.Table
        Chunks table.
    """
    return pq.read_table(parquet_path)


def extract_embeddings(table: pa.Table) -> np.ndarray:
    """Extract embeddings from chunks table.

    Parameters
    ----------
    table : pa.Table
        Chunks table with embedding column.

    Returns
    -------
    np.ndarray
        Embeddings array of shape (num_rows, vec_dim).
    """
    embedding_col = table["embedding"]
    # Convert FixedSizeList to numpy array
    flat_values = embedding_col.flatten().to_numpy()
    vec_dim = embedding_col.type.list_size
    return flat_values.reshape(-1, vec_dim)


__all__ = [
    "extract_embeddings",
    "get_chunks_schema",
    "read_chunks_parquet",
    "write_chunks_parquet",
]

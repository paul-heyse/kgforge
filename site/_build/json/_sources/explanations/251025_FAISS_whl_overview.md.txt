# Updated cuVS Addendum

Below is an **updated, implementation‑grade addendum** that reflects your **new FAISS wheel** with **cuVS integration compiled for**: **Flat**, **IVF‑Flat**, **IVF‑PQ**, **Graph CAGRA**, and **Binary‑CAGRA**, as indicated by the build artifacts:

```
CuvsFlatIndex.cu.o
CuvsIVFFlat.cu.o
CuvsIVFPQ.cu.o
CuvsCagra.cu.o
BinaryCuvsCagra.cu.o
utils/CuvsFilterConvert.cu.o
utils/CuvsUtils.cu.o
```

> **Key consequence:** In this wheel, **FAISS GPU indexes can dispatch to cuVS kernels** for the families above when you **enable cuVS** at construction/cloning time and the **cuVS/RAPIDS libraries are present** in the process. The runtime loader & resource rules for cuVS remain the same (preload `libcuvs`, `librmm`, logging; use a shared `Resources` handle if calling cuVS directly).  

---

## 1) Revised capability matrix (this wheel)

| Family / class    |                  CPU |                           GPU (FAISS kernels) |                                                                                    **cuVS through FAISS (this wheel)** |                **cuVS direct (Python)** |
| ----------------- | -------------------: | --------------------------------------------: | ---------------------------------------------------------------------------------------------------------------------: | --------------------------------------: |
| **Flat (exact)**  | ✅ `IndexFlat{L2,IP}` |                       ✅ `GpuIndexFlat{L2,IP}` | ✅ via **index‑level cuVS** (`GpuIndexFlat{L2,IP}` with `use_cuvs`) and via **bfKNN** (`knn_gpu/bfKnn` with `use_cuvs`) |         ✅ `cuvs.neighbors.brute_force`  |
| **IVF‑Flat**      |                    ✅ |                           ✅ `GpuIndexIVFFlat` |                                                             ✅ **index‑level cuVS** (`GpuIndexIVFFlat` with `use_cuvs`) |            ✅ `cuvs.neighbors.ivf_flat`  |
| **IVF‑PQ**        |                    ✅ |                             ✅ `GpuIndexIVFPQ` |                                                               ✅ **index‑level cuVS** (`GpuIndexIVFPQ` with `use_cuvs`) |              ✅ `cuvs.neighbors.ivf_pq`  |
| **IVF‑SQ**        |                    ✅ |                ✅ `GpuIndexIVFScalarQuantizer` |                                                                                                – (no cuVS kernel here) |                                       – |
| **Graph (HNSW)**  |                    ✅ |                                             – |                                                                                                                      – |                ✅ `cuvs.neighbors.hnsw`  |
| **Graph (CAGRA)** |   ✅ `IndexHNSWCagra` |                             ✅ `GpuIndexCagra` |                                                               ✅ **index‑level cuVS** (`GpuIndexCagra` with `use_cuvs`) | ✅ `cuvs.neighbors.cagra` (+ multi‑GPU)  |
| **Binary**        |                    ✅ | ✅ `GpuIndexBinaryFlat`, `GpuIndexBinaryCagra` |                                                    ✅ **Binary‑CAGRA** via **index‑level cuVS** (`GpuIndexBinaryCagra`) |                                       – |
| **Direct bfKNN**  |                    – |               ✅ `bfKnn(...)` / `knn_gpu(...)` |                                                          ✅ **set `use_cuvs=True`** (guard with `should_use_cuvs(...)`) |        ✅ `cuvs.distance` / brute_force  |

**What changed:** In earlier guidance, IVF‑Flat/IVF‑PQ/CAGRA/Binary‑CAGRA were marked “not via FAISS route”. In **this** wheel, those families **are** cuVS‑enabled inside FAISS when you opt in via the **index configs** or **GPU cloner options** described below (and cuVS libs are present). The bfKNN cuVS path remains available. The cuVS loader & environment remain as documented. 

---

## 2) How to enable cuVS (reliable patterns)

### 2.1. One‑line preload (cuVS/RAPIDS libs)

Call this **before** importing/initializing FAISS to ensure the shared libraries are resident:

```python
from libcuvs import load_library
load_library()  # loads libcuvs, librmm, rapids_logger; idempotent
```

This follows the integration guidance in your loader reference and prevents `OSError: libcuvs_c.so not found` surprises. 

---

### 2.2. Enabling cuVS via **GPU Cloner Options** (recommended)

The simplest and most portable way to engage cuVS for **Flat / IVF‑Flat / IVF‑PQ / CAGRA / Binary‑CAGRA** is to **build the index on CPU**, then **clone to GPU** with a cuVS‑aware cloner:

```python
import faiss

# 1) Build a CPU index (examples below)
cpu_index = faiss.index_factory(d, "OPQ64,IVF8192,PQ64", faiss.METRIC_INNER_PRODUCT)
cpu_index.train(train_vectors)

# 2) Prepare GPU resources
res = faiss.StandardGpuResources()

# 3) Clone with cuVS enabled (guarded)
co = faiss.GpuClonerOptions()
co.use_cuvs = True  # ask for cuVS kernels if available

try:
    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)
except RuntimeError:
    # Fallback if cuVS cannot be used for this combo
    co.use_cuvs = False
    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)
```

* `GpuClonerOptions.use_cuvs` is exposed in this wheel and directs FAISS to build the **cuVS‑backed** GPU index when possible. 
* This pattern works uniformly across Flat / IVF‑Flat / IVF‑PQ and also covers **CAGRA** if cloning from `IndexHNSWCagra` to `GpuIndexCagra` is desired. (You can also build `GpuIndexCagra` directly; see 2.3.)
* Keep your **existing memory tuning** (`setTempMemory`, `setPinnedMemory`) for throughput.

> You can also probe `faiss.should_use_cuvs(...)` before setting the flag; however, the **try/except** is the most robust guard when combining drivers, cards, and dims. 

---

### 2.3. Enabling cuVS via **index configs** (direct GPU constructors)

When you **construct GPU indexes directly**, pass a config whose base type is `GpuIndexConfig` and set `use_cuvs` there (the property is inherited by specific configs):

```python
# Flat
flat_cfg = faiss.GpuIndexFlatConfig()
flat_cfg.use_cuvs = True
gpu_flat = faiss.GpuIndexFlatIP(res, d, flat_cfg)  # or GpuIndexFlatL2

# IVF-FLAT / IVF-PQ
ivf_cfg = faiss.GpuIndexIVFConfig()
ivf_cfg.use_cuvs = True
# ... pass ivf_cfg to the appropriate GpuIndexIVF* constructor

# CAGRA
cagra_cfg = faiss.GpuIndexCagraConfig()
cagra_cfg.use_cuvs = True
gpu_cagra = faiss.GpuIndexCagra(res, d, cagra_cfg)

# Binary-CAGRA (if you build it directly)
# (config class may not be separate in the API; prefer the cloner path if unsure)
```

These configs inherit `use_cuvs` through `GpuIndexConfig` in this wheel. Use the cloner option (2.2) when you want one code path that works across all index types. 

---

### 2.4. Enabling cuVS for **direct GPU brute‑force kNN**

The **bfKNN** path can independently dispatch to cuVS kernels:

```python
from faiss import StandardGpuResources, METRIC_INNER_PRODUCT, GpuDistanceParams, knn_gpu

res = StandardGpuResources()
params = GpuDistanceParams(); params.metric = METRIC_INNER_PRODUCT; params.k = 10; params.dims = d

# guard with should_use_cuvs; fallback is automatic inside knn_gpu if you pass False
use_cuvs = True  # or: bool(faiss.should_use_cuvs(params))
D, I = knn_gpu(res, xq.astype('float32'), xb.astype('float32'),
               k=10, metric=METRIC_INNER_PRODUCT, use_cuvs=use_cuvs)
```

This remains a great baseline even when your main serving index is IVF‑PQ/CAGRA. 

---

## 3) Minimal “correct by construction” recipes per family

> These patterns **train on CPU** (deterministic), then **clone to GPU with cuVS**. You can switch to direct GPU construction later; the recall/latency behavior is identical once cuVS is engaged.

### 3.1. **IVF‑PQ (OPQ64,IVF8192,PQ64)** with cuVS

```python
import faiss, numpy as np

d = 2560
cpu = faiss.index_factory(d, "OPQ64,IVF8192,PQ64", faiss.METRIC_INNER_PRODUCT)

train = np.random.randn(10_000_000, d).astype('float32')
faiss.normalize_L2(train)
cpu.train(train)

res = faiss.StandardGpuResources()
co = faiss.GpuClonerOptions(); co.use_cuvs = True
gpu = faiss.index_cpu_to_gpu(res, 0, cpu, co)  # cuVS-backed IVFPQ

# add/search as usual
```

* Keeps your default “best‑in‑class” factory and determinism.
* The **cloner** chooses the cuVS implementation for the data path compiled in this wheel. 

### 3.2. **IVF‑Flat** with cuVS

```python
cpu = faiss.index_factory(d, "IVF8192,Flat", faiss.METRIC_INNER_PRODUCT)
cpu.train(train)

co = faiss.GpuClonerOptions(); co.use_cuvs = True
gpu = faiss.index_cpu_to_gpu(res, 0, cpu, co)  # cuVS-backed IVFFlat
```

### 3.3. **Flat (exact)** with cuVS

* **Index route:** `GpuIndexFlat{L2,IP}` with `GpuIndexFlatConfig.use_cuvs = True`.
* **Direct KNN:** `knn_gpu(..., use_cuvs=True)` (guarded). 

### 3.4. **CAGRA** with cuVS

* **Direct GPU build** (graph lives on GPU):

  ```python
  cfg = faiss.GpuIndexCagraConfig(); cfg.use_cuvs = True
  gpu_cagra = faiss.GpuIndexCagra(res, d, cfg)
  # gpu_cagra.add(n, xb) builds the graph; search as usual
  ```

* **CPU↔GPU interop:** `IndexHNSWCagra` exists on CPU for moving graph structure if needed (e.g., copy base level). Keep primary serving on GPU to avoid host <-> device hops.

### 3.5. **Binary‑CAGRA** with cuVS

* Prefer the **cloner path** when coming from CPU binary indexes; if you instantiate `GpuIndexBinaryCagra` directly, keep the same principle: construct with a config that enables cuVS (or clone with `GpuClonerOptions.use_cuvs=True`) and fall back on error.

---

## 4) Filters & selectors (what those `CuvsFilter*` objects mean for you)

The presence of `utils/CuvsFilterConvert.cu.o` indicates FAISS performs **selector/bitset conversion** into cuVS‑compatible filter representations when a filter is passed at search time (e.g., via `SearchParametersIVF.sel` / ID selectors). You do not need to change your Python calls:

* Keep using FAISS’ **search parameters** (e.g., `SearchParametersIVF`’s selector).
* When the index was constructed/cloned with **`use_cuvs=True`**, the FAISS runtime converts the filter to cuVS’ internal format and applies it during list scans.

This is transparent but worth noting for **filtered search** scenarios. (The general cuVS run‑time and loader expectations remain exactly as previously documented.) 

---

## 5) Operational reminders (unchanged but critical)

* **Preload cuVS** once per process (`libcuvs.load_library()`), so **RMM** and logging are initialized and all `.so` dependencies are resident. 
* Stick to **row‑major `float32`** contiguous arrays for FAISS; normalize vectors for cosine (IP).
* Keep GPU working set **≤ ~80% VRAM**; pre‑allocate **temp** and **pinned** memory on `StandardGpuResources`.
* For **cuVS direct** experimentation (IVF‑PQ, IVF‑Flat, CAGRA, HNSW, multi‑GPU), continue to use the Python surfaces under `cuvs.neighbors.*` with a shared `Resources` handle and device arrays, as documented in your cuVS reference. 

---

## 6) Quick “am I using cuVS right now?” probes

```python
# After calling libcuvs.load_library() and constructing your GPU index:

import faiss

# 1) For bfKNN:
p = faiss.GpuDistanceParams(); p.k=10; p.dims=d; p.metric=faiss.METRIC_INNER_PRODUCT
print("bfKNN should_use_cuvs:", faiss.should_use_cuvs(p))  # True → bfKNN will route to cuVS

# 2) For an index you plan to build:
cfg = faiss.GpuIndexIVFConfig()
print("Index should_use_cuvs:", faiss.should_use_cuvs(cfg))  # True → set cfg/co.use_cuvs = True
```

* If you skip `should_use_cuvs`, simply **set `use_cuvs=True`** and **catch** a `RuntimeError` on construction; re‑try with `False`. This keeps your code robust across environments while **preferring cuVS** whenever available. 

---

### Pointers (unchanged)

* **cuVS API & usage** (neighbors, distance, resources, device arrays) — your cuVS reference. 
* **cuVS loader & environment** (preload order, env vars, system vs wheel libs) — libcuvs loader reference. 
* **System architecture & defaults** (2560‑d, OPQ64/IVF8192/PQ64, persistence, observability) — high‑level architecture. 

---

## 7) Bottom line (what to change in your code)

1. **Call** `load_library()` **once at startup**. 
2. **When cloning** CPU→GPU, set `co.use_cuvs = True` and **fall back** on error. This now enables cuVS for **Flat**, **IVF‑Flat**, **IVF‑PQ**, **CAGRA**, and **Binary‑CAGRA** in this build.
3. **When constructing GPU indexes directly**, set `cfg.use_cuvs = True` on the appropriate **`GpuIndex*Config`** (or use the cloner).
4. For **bfKNN**, pass `use_cuvs=True` (or guard with `should_use_cuvs`). 
5. Keep the rest of the pipeline (factory string, normalization, memory planning, multi‑GPU strategy, persistence) **unchanged**. 



# addendum, base content on faiss gpu library

---

## 0) What changed vs. the prior wheel (and how to think about it)

**Hardware/ABI**

* CUDA runtime linked: **`libcudart.so.13`** → the wheel targets **CUDA 13** (Blackwell‑ready).
* GPU arch: **`sm_120` SASS + PTX** (“**`sm_120‑virtual`**”) for forward compatibility—native kernels when they match, PTX JIT otherwise.

**cuVS inside FAISS (this wheel)**

* The GPU SWIG layer exports **`use_cuvs` gates on index configs and cloner options** and dynamically links against cuVS/RAPIDS libs. With cuVS libraries **preloaded**, FAISS GPU indexes can route to **cuVS kernels** for **Flat, IVF‑Flat, IVF‑PQ, CAGRA, Binary‑CAGRA**; otherwise FAISS falls back to its own kernels. 

> **Reminder**: the kernels themselves live in the **cuVS** shared libraries; the FAISS wheel holds **dispatch hooks** and dynamic links. Ensure the cuVS loader is invoked at process start. 

---

## 1) CPU vs GPU vs cuVS — updated capability map (this wheel)

| Family / class    |                  CPU |                           GPU (FAISS kernels) |                                                                **cuVS through FAISS** (this wheel) |                **cuVS direct (Python)** |
| ----------------- | -------------------: | --------------------------------------------: | -------------------------------------------------------------------------------------------------: | --------------------------------------: |
| **Flat (exact)**  | ✅ `IndexFlat{L2,IP}` |                       ✅ `GpuIndexFlat{L2,IP}` | ✅ via **index‑level `use_cuvs`** on `GpuIndexFlat*` and via **bfKNN** (`knn_gpu/bfKnn` `use_cuvs`) |         ✅ `cuvs.neighbors.brute_force`  |
| **IVF‑Flat**      |                    ✅ |                           ✅ `GpuIndexIVFFlat` |                                                      ✅ via **index‑level `use_cuvs`** / GPU cloner |            ✅ `cuvs.neighbors.ivf_flat`  |
| **IVF‑PQ**        |                    ✅ |                             ✅ `GpuIndexIVFPQ` |                                                      ✅ via **index‑level `use_cuvs`** / GPU cloner |              ✅ `cuvs.neighbors.ivf_pq`  |
| **IVF‑SQ**        |                    ✅ |                ✅ `GpuIndexIVFScalarQuantizer` |                                                                                   – (no cuVS path) |                                       – |
| **Graph (HNSW)**  |                    ✅ |                                             – |                                                                                                  – |                ✅ `cuvs.neighbors.hnsw`  |
| **Graph (CAGRA)** |   ✅ `IndexHNSWCagra` |                             ✅ `GpuIndexCagra` |                                                                   ✅ via **index‑level `use_cuvs`** | ✅ `cuvs.neighbors.cagra` (+ multi‑GPU)  |
| **Binary**        |                    ✅ | ✅ `GpuIndexBinaryFlat`, `GpuIndexBinaryCagra` |                                                      ✅ **Binary‑CAGRA** via index‑level `use_cuvs` |                                       – |
| **Direct bfKNN**  |                    – |               ✅ `bfKnn(...)` / `knn_gpu(...)` |                                               ✅ **`use_cuvs=True`** (guard with `should_use_cuvs`) |        ✅ `cuvs.distance` / brute_force  |

**Filters & selectors:** your build includes `CuvsFilterConvert.cu.o`; FAISS will convert FAISS selectors/bitsets into cuVS’ filter format automatically when the index was created with `use_cuvs=True`. You keep using standard FAISS search params at the Python layer. 

---

## 2) Blackwell (RTX 5090) support: SASS **`sm_120`** + PTX **`sm_120‑virtual`**

* **Why it matters**: perfect performance when SASS matches; **PTX fallback** when driver/toolkit moves ahead (no “no kernel image” errors).
* **Ops tips**: leave `CUDA_CACHE_MAXSIZE` roomy to avoid first‑run JIT thrash; never force JIT in production.

---

## 3) cuVS enablement model (final patterns)

**Always preload cuVS/RAPIDS libs before FAISS**:

```python
from libcuvs import load_library
load_library()  # loads libcuvs, librmm, rapids_logger; idempotent. :contentReference[oaicite:12]{index=12}
```

**A) Index‑level enablement (recommended)**
Set **`use_cuvs=True`** on the GPU cloner or GPU index config; **catch & fallback**:

```python
import faiss

# Clone CPU → GPU (works uniformly for Flat / IVF-Flat / IVF-PQ / CAGRA / Binary-CAGRA)
co = faiss.GpuClonerOptions(); co.use_cuvs = True
try:
    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)
except RuntimeError:
    co.use_cuvs = False
    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)
```

Or **construct GPU indexes directly** with a config inheriting `GpuIndexConfig.use_cuvs`:

```python
flat_cfg = faiss.GpuIndexFlatConfig(); flat_cfg.use_cuvs = True
gpu_flat = faiss.GpuIndexFlatIP(res, d, flat_cfg)

ivf_cfg = faiss.GpuIndexIVFConfig(); ivf_cfg.use_cuvs = True
# pass ivf_cfg into the appropriate GpuIndexIVF* constructor

cagra_cfg = faiss.GpuIndexCagraConfig(); cagra_cfg.use_cuvs = True
gpu_cagra = faiss.GpuIndexCagra(res, d, cagra_cfg)
```

**B) Brute‑force KNN path**
Guard **`use_cuvs`** with the central probe (falls back if not viable):

```python
params = faiss.GpuDistanceParams(); params.dims=d; params.k=10; params.metric=faiss.METRIC_INNER_PRODUCT
use_cuvs = bool(faiss.should_use_cuvs(params))  # detects viability at runtime. :contentReference[oaicite:13]{index=13}
D, I = faiss.knn_gpu(res, xq.astype('float32'), xb.astype('float32'), 10,
                     metric=faiss.METRIC_INNER_PRODUCT, use_cuvs=use_cuvs)
```

**C) Direct cuVS ANN (optional)**
Use cuVS Python APIs for IVF‑Flat/IVF‑PQ/CAGRA/HNSW (especially when you want multi‑GPU `mg.*` variants): `cuvs.neighbors.{ivf_flat, ivf_pq, cagra, hnsw}` with `Resources` handle and device arrays. 

---

## 4) “Best‑in‑class” FAISS build on RTX 5090 (cosine, 2560‑d) — **now with cuVS toggles**

**Index choice**: `OPQ64,IVF8192,PQ64` (cosine via L2 normalization + IP). **Train** on up to **10 M** samples (seed=42). **Search** with `nprobe=64`. 

**GPU clone with cuVS** (preferred path):

```python
import faiss, numpy as np

d = 2560
cpu = faiss.index_factory(d, "OPQ64,IVF8192,PQ64", faiss.METRIC_INNER_PRODUCT)
train = np.random.randn(10_000_000, d).astype('float32'); faiss.normalize_L2(train)
cpu.train(train)

res = faiss.StandardGpuResources()
res.setTempMemory(1_200_000_000); res.setPinnedMemory(512_000_000)

co = faiss.GpuClonerOptions(); co.use_cuvs = True
try:
    gpu = faiss.index_cpu_to_gpu(res, 0, cpu, co)  # cuVS-backed IVFPQ in this build
except RuntimeError:
    co.use_cuvs = False
    gpu = faiss.index_cpu_to_gpu(res, 0, cpu, co)

# add / search as usual
```

**Notes & knobs**

* Precompute lookup tables on GPU if your PQ config benefits (via cloner/options); keep working set ≤ **80%** VRAM; reuse pinned host buffers for larger transfers. 

---

## 5) When to call cuVS directly (and when FAISS+cuVS is enough)

* **Stay inside FAISS** (with `use_cuvs=True`) for: **Flat, IVF‑Flat, IVF‑PQ, CAGRA, Binary‑CAGRA**; you get FAISS’ ergonomics and cuVS performance.
* **Call cuVS directly** when you need: **multi‑GPU `mg.*`** pipelines, algorithm‑specific parameters not exposed by FAISS’ wrappers, or you want to co‑locate with other RAFT pipelines with explicit stream choreography. 

**Direct IVF‑PQ example (unchanged)** — device arrays + `Resources`:

```python
from cuvs.common import Resources
from cuvs.neighbors import ivf_pq
from pylibraft.common import device_ndarray
# build/search as in your prior example … :contentReference[oaicite:18]{index=18}
```

---

## 6) Quick API map — **what you’ll call most often**

**CPU**
`IndexFlat{L2,IP}`, `IndexIVFFlat`, `IndexIVFPQ`, `IndexIVFScalarQuantizer`, `IndexHNSW*`, `IndexHNSWCagra`, `IndexBinary*`, `IndexPreTransform`, `OPQMatrix`, `PCAMatrix`, `IndexRefineFlat`, `IndexShards`, `IndexReplicas`.

**GPU (FAISS)**
`StandardGpuResources`, `GpuIndexFlat{L2,IP}`, `GpuIndexIVFFlat`, `GpuIndexIVFPQ`, `GpuIndexIVFScalarQuantizer`, `GpuIndexCagra`, `GpuIndexBinaryFlat`, `GpuIndexBinaryCagra`, `index_cpu_to_gpu[_multiple]`, `index_gpu_to_cpu`, `GpuClonerOptions`, `GpuParameterSpace`, `knn_gpu(...)`, `bfKnn(...)`.

**cuVS hooks inside FAISS**
`GpuClonerOptions.use_cuvs`, `GpuIndex*Config.use_cuvs`, `should_use_cuvs(...)` (for bfKNN probe), `knn_gpu(..., use_cuvs=...)`. 

**cuVS (direct)**
`cuvs.neighbors.{ivf_flat, ivf_pq, cagra, hnsw, brute_force, mg.*}`, `cuvs.cluster.kmeans`, `cuvs.distance.pairwise_distance`, `cuvs.common.Resources`. 

---

## 7) Validation checklist (runtime)

1. **Preload check** (did we load the libraries?):

```python
from libcuvs import load_library
print([h._name for h in load_library()])  # expect libcuvs*, librmm, rapids_logger. :contentReference[oaicite:21]{index=21}
```

2. **Index‑level cuVS** (construction guard):

* Clone/construct with `use_cuvs=True`. If it **raises**, retry with `False`. Log the final `use_cuvs` state per index.

3. **bfKNN cuVS probe** (fast sanity):

```python
from faiss import GpuDistanceParams, should_use_cuvs, METRIC_INNER_PRODUCT
p = GpuDistanceParams(); p.metric = METRIC_INNER_PRODUCT; p.dims = 2560; p.k = 10
print("bfKNN should_use_cuvs:", should_use_cuvs(p))  # True => bfKNN will route to cuVS. :contentReference[oaicite:22]{index=22}
```

4. **Direct cuVS smoke** (IVF‑PQ or brute‑force) with device arrays + `Resources`. 

---

## 8) Operational guidance (unchanged principles, cuVS‑aware)

* **Loader order**: preload RAPIDS/cuVS first; then import FAISS. 
* **Memory**: size `setTempMemory`/`setPinnedMemory` once; keep VRAM headroom ~**20%**; batch adds to avoid OOM; precompute tables where helpful. 
* **Streams**: cuVS direct calls accept a `Resources` handle (shared stream) and only sync when you call `handle.sync()`. FAISS tends to sync at call boundaries; overlap I/O with compute using pinned buffers. 
* **Indexes at rest**: save **CPU** index (`.faiss`) + `.ids`; rebuild GPU clones at startup (set `use_cuvs` again when cloning). 

---

## 9) Ready‑to‑paste helpers (with cuVS auto‑engage)

**A. GPU clone (cuVS‑first, safe fallback)**

```python
def to_gpu_cuvs_first(cpu_index, res, device=0):
    co = faiss.GpuClonerOptions(); co.use_cuvs = True
    try:
        return faiss.index_cpu_to_gpu(res, device, cpu_index, co), True
    except RuntimeError:
        co.use_cuvs = False
        return faiss.index_cpu_to_gpu(res, device, cpu_index, co), False
```

**B. bfKNN with automatic cuVS**

```python
def knn_gpu_auto(res, xq_f32, xb_f32, k=10, metric=faiss.METRIC_INNER_PRODUCT):
    params = faiss.GpuDistanceParams()
    params.metric = metric; params.k = k; params.dims = xq_f32.shape[1]
    use = faiss.should_use_cuvs(params)  # True when viable. :contentReference[oaicite:28]{index=28}
    return faiss.knn_gpu(res, xq_f32.astype('float32', copy=False),
                         xb_f32.astype('float32', copy=False),
                         k, metric=metric, use_cuvs=use)
```

**C. Direct cuVS IVF‑PQ (device arrays)** — unchanged. 

---

## 10) Bottom line

* **This wheel**: Blackwell‑ready (**`sm_120` + PTX**), CUDA‑13, and **cuVS‑enabled** inside FAISS for **Flat, IVF‑Flat, IVF‑PQ, CAGRA, Binary‑CAGRA**.
* **Your code**: preload cuVS; set **`use_cuvs=True`** on GPU clones/constructors; guard with try/except; use `should_use_cuvs(...)` for bfKNN.
* **Architecture & defaults** (factory, training, persistence, multi‑GPU) remain per plan; you simply get **cuVS speed paths** where available without changing the higher‑level design. 

---

# Addendum — Agent‑oriented details (updated to match the new cuVS coverage)

These sections expand the “how” with concrete interfaces, schemas, and runbooks across the stack and already assume **FAISS can route to cuVS** for the families above.

### A1. Process bootstrap (deterministic & cuVS‑ready)

```python
from libcuvs import load_library; load_library()   # ensures libcuvs/librmm/logging are loaded. :contentReference[oaicite:31]{index=31}
import faiss
res = faiss.StandardGpuResources()
res.setTempMemory(1_200_000_000); res.setPinnedMemory(512_000_000)
```

Keep a single `StandardGpuResources` per device for the process lifetime and configure it once. 

### A2. Zero‑copy interop (FAISS ↔ cuVS)

Use RAFT `Resources` and device arrays when calling cuVS directly; prefer pinned host arenas + contiguous `float32` for FAISS inputs to minimize hidden copies. 

### A3. Multi‑GPU: replicas and shards (unchanged)

Use `IndexReplicas` (QPS) or `IndexShards` (capacity). When cloning to multiple GPUs, set `use_cuvs=True` on the cloner options once; FAISS will apply it per device (fallback per GPU on error). 

### A4. Memory planning & batching

Budget **scratch + PQ tables + codes + ids + batch** ≤ **80%** VRAM. Tune batch add/search sizes empirically; precompute lookup tables where it helps latency. 

### A5. Persistence & registry

Persist **CPU** index (`.faiss`) + **`.ids`**; rebuild GPU clones at service start using `use_cuvs=True`. Record the final **applied** state (`cuvs=true/false`) in your registry’s `faiss_indexes` rows for observability. 

### A6. Observability & probes

* Log `{gpu_id, nlist, m, nprobe, k, batch, temp_mem, pinned_mem, use_cuvs}` for each search; expose p50/p95/p99 histograms. 
* Quick “are the libs loaded?” probe: `list(h._name for h in load_library())`. 
* Quick “will bfKNN use cuVS?” probe: `should_use_cuvs(GpuDistanceParams(...))`. 

### A7. Failure taxonomy & fallbacks (GPU paths)

* **Missing libs** → run `load_library()` first (or fix `LD_LIBRARY_PATH`); **resume** with FAISS kernels if cuVS fails to initialize. 
* **OOM** → halve batch; lower `setTempMemory`; retry twice; then quarantine batch. 

### A8. Direct cuVS catalog (unchanged surfaces)

`cuvs.neighbors.{ivf_flat, ivf_pq, cagra, hnsw, brute_force, mg.*}`, `cuvs.cluster.kmeans`, `cuvs.distance.pairwise_distance`, `cuvs.common.Resources` — consistent `IndexParams`/`SearchParams`/`build`/`extend`/`save`/`load` patterns. 

### A9. Architecture defaults (for the broader system)

Keep the **2560‑d** embedder, chunking settings, Parquet schemas, DuckDB catalog/migrations, and hybrid retrieval pipeline exactly as specified in your architecture doc; nothing about the cuVS enablement changes those contracts. 

---

If you’d like, I can also output a **drop‑in `vectorstore_faiss` module** (Python package) that wraps the **train/add/search/persist** flow with **cuVS‑first** cloning and automatic bfKNN fallback, plus a tiny **validator** script that prints library load locations, shows `should_use_cuvs(...)`, and runs a 10k‑vector bfKNN smoke test on the RTX 5090.


Below is a **comprehensive, example‑driven reference** that fills in the remaining FAISS and cuVS Python surfaces we hadn’t previously shown with code. It’s organized so an agent can copy/paste snippets and understand shapes, params, edge‑cases, and performance notes. Where relevant, I point to the cuVS and loader docs you provided.   

---

## A) FAISS — CPU‑side classes & utilities (with examples)

### A1) `IndexIDMap` / `IndexIDMap2` — stable IDs around any index

**Why:** FAISS stores integer IDs; these wrappers let you control them.

```python
import faiss, numpy as np

# Base ANN index (any type works)
d = 2560
base = faiss.index_factory(d, "IVF8192,PQ64", faiss.METRIC_INNER_PRODUCT)

# Wrap with an ID map so we add our own 64-bit IDs
idmap = faiss.IndexIDMap2(base)

# Train as usual (on base)
train = np.random.randn(1_000_000, d).astype('float32'); faiss.normalize_L2(train)
idmap.train(train)

# Add vectors with application-specific IDs
xb = np.random.randn(500_000, d).astype('float32'); faiss.normalize_L2(xb)
ids = np.arange(100_000, 100_000 + xb.shape[0], dtype='int64')
idmap.add_with_ids(xb, ids)

# Search returns your IDs
xq = np.random.randn(1000, d).astype('float32'); faiss.normalize_L2(xq)
D, I = idmap.search(xq, k=10)
```

**Notes**

* Use `IndexIDMap2` (64‑bit IDs) for large corpora.
* Persist **CPU** form (see A6) and keep a parallel `.ids` copy if you ever unwrap the map. 

---

### A2) `IndexRefineFlat` — refine ANN hits with exact scoring

**Why:** Get IVF/graph latency with exact cosine/IP re‑ranking.

```python
# Build a fast coarse index (e.g., IVF,PQ) then refine with Flat
coarse = faiss.index_factory(d, "IVF8192,PQ64", faiss.METRIC_INNER_PRODUCT)
coarse.train(train); coarse.add(xb)

refined = faiss.IndexRefineFlat(coarse)   # wraps the coarse index with an exact refiner
refined.kfactor = 2.0                     # search 2×k in coarse stage, re-rank down to k

D, I = refined.search(xq, k=10)
```

**Notes**

* On GPU you’ll typically run the coarse stage on device, then refine on device or CPU depending on memory; the wrapper manages the sequence.

---

### A3) `IndexPreTransform` + transforms (`OPQMatrix`, `PCAMatrix`)

**Why:** Pre‑rotate or reduce dimension **inside** the index for better PQ fit.

```python
# 1) Build transforms: PCA (optional) then OPQ
pca = faiss.PCAMatrix(d_in=2560, d_out=1024, eigen_power=-0.5)  # example PCA step
opq = faiss.OPQMatrix(d_out=1024, m=64)  # OPQ with 64 subquantizers

# 2) Chain transforms in a vector
vt = faiss.VectorTransformChain()
vt.append(pca)
vt.append(opq)

# 3) Base index AFTER transforms (note d=1024 now)
base = faiss.index_factory(1024, "IVF8192,PQ64", faiss.METRIC_INNER_PRODUCT)

# 4) Wrap into IndexPreTransform
pre = faiss.IndexPreTransform(vt, base)

# 5) Train on original-dim data; transform is applied internally
pre.train(train)       # 'train' runs PCA/OPQ fitting + IVF/PQ training
pre.add(xb)
D, I = pre.search(xq, 10)
```

**Notes**

* For our default blueprint we already use OPQ64; `IndexPreTransform` is the explicit form of that pipeline. Keep cosine/IP consistency: **normalize** before train/add/query. 

---

### A4) `Clustering` (K‑means) — build coarse quantizers explicitly

**Why:** Manual control when you don’t use `index_factory()`.

```python
# Train IVF coarse quantizer centroids explicitly with K-means
nlist = 8192
kmeans = faiss.Clustering(d, nlist)
kmeans.niter = 25; kmeans.verbose = True

quantizer = faiss.IndexFlatIP(d)        # cosine via normalization + IP
kmeans.train(train, quantizer)

# Use trained quantizer to build IVF, then attach PQ
ivf = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)
ivf.train(train)
ivf.add(xb)
```

**Notes**

* `index_factory()` automates this, but direct `Clustering` is useful for specialized training schedules.

---

### A5) `ParameterSpace` / `GpuParameterSpace` — named tuning knobs

**Why:** Set parameters (e.g., `nprobe`) by **name**, works across families.

```python
# CPU
ps = faiss.ParameterSpace()
ps.set_index_parameter(base, "nprobe", 64)

# GPU
gps = faiss.GpuParameterSpace(); gps.initialize(gpu_index)
gps.set_index_parameter(gpu_index, "nprobe", 64)   # same name on GPU
```

**Notes**

* Prefer these over ad‑hoc setters; helps keep code index‑agnostic.

---

### A6) Persistence — `write_index` / `read_index`

**Why:** Save CPU index for fast reload; re‑clone to GPU at runtime.

```python
faiss.write_index(idmap, "/data/faiss/shard_000.idx")          # save CPU index (any type)
idmap2 = faiss.read_index("/data/faiss/shard_000.idx")         # load CPU index
```

**Notes**

* Don’t save GPU indexes; reconstruct by cloning the CPU index on startup. Track paths and config in DuckDB (`faiss_indexes`). 

---

### A7) Range search — `range_search` (a.k.a. radius search)

**Why:** Get all neighbors within a distance threshold instead of top‑K.

```python
radius = 0.1  # for cosine/IP, pick meaningful threshold on normalized vectors
rs = faiss.RangeSearchResult(xq.shape[0])
idmap.range_search(xq, radius, rs)

# Extract result per query
lims, D, I = rs.lims, rs.distances, rs.labels
for qi in range(xq.shape[0]):
    begin, end = lims[qi], lims[qi+1]
    neigh_ids = I[begin:end]
    neigh_dist = D[begin:end]
```

**Notes**

* Range search is less common for large‑scale serving, but handy for thresholded neighbor graphs.

---

### A8) Filtered search — `IDSelector*` + `SearchParametersIVF`

**Why:** Exclude/allow subsets of IDs at **query time**.

```python
# Build a selector (ids in [lo, hi))
sel = faiss.IDSelectorRange(lo=100_000, hi=150_000)

# Tie selector to IVF search params
sp = faiss.SearchParametersIVF()
sp.nprobe = 64
sp.sel = sel

# Call the 3-arg search to pass params
D, I = idmap.search(xq, 10, params=sp)
```

**Notes**

* With cuVS‑backed GPU indexes enabled (`use_cuvs=True` when cloning/constructing), FAISS converts these selectors into cuVS filters under the hood (your build includes the converter). 

---

### A9) Deletions — `remove_ids`

**Why:** Remove a set of IDs (e.g., retractions, updates).

```python
to_remove = faiss.IDSelectorRange(200_000, 210_000)  # OR IDSelectorBatch([...])
n_removed = idmap.remove_ids(to_remove)
```

**Notes**

* IVF/PQ structures become “holey” over time; periodic rebuilds (or re‑add to a fresh shard) keep performance consistent.

---

### A10) Reconstruction — `reconstruct`, `reconstruct_n`

**Why:** Retrieve vector approximations (exact for Flat/HNSW; approximate for PQ).

```python
vec = idmap.reconstruct(int(I[0,0]))
mat = idmap.reconstruct_n(0, 100)   # first 100 vectors
```

---

### A11) CPU parallelism — `omp_set_num_threads`

**Why:** Pin thread count for CPU add/search/training.

```python
faiss.omp_set_num_threads(14)  # match your server threads
```

---

## B) FAISS — GPU & multi‑GPU functions (deeper, with cuVS enablement)

> **Reminder**: In your wheel, FAISS GPU indexes can dispatch to **cuVS** for **Flat, IVF‑Flat, IVF‑PQ, CAGRA, Binary‑CAGRA** when you set `use_cuvs=True` on the **GPU cloner** or the **GPU index config**, and the cuVS libraries are preloaded. 

### B1) CPU→GPU cloning (single & multi‑GPU)

```python
import faiss

# 1) Single GPU (cuVS-first, safe fallback)
co = faiss.GpuClonerOptions(); co.use_cuvs = True
try:
    gpu_index = faiss.index_cpu_to_gpu(res, 0, idmap, co)
except RuntimeError:
    co.use_cuvs = False
    gpu_index = faiss.index_cpu_to_gpu(res, 0, idmap, co)

# 2) Replicated (QPS-scale)
ngpu = faiss.get_num_gpus()
resources = [faiss.StandardGpuResources() for _ in range(ngpu)]
replicas = faiss.IndexReplicas()
for dev in range(ngpu):
    replicas.addIndex(faiss.index_cpu_to_gpu(resources[dev], dev, idmap, co))

# 3) Sharded (capacity-scale)
shards = faiss.IndexShards(d, threaded=True, successive_ids=False)
# Build per-shard CPU indexes first, then clone each with co.use_cuvs=True and add to 'shards'
```

**Notes**

* Replicas improve QPS; shards expand capacity; wrap both under one logical index in your registry and fan‑out search. 

---

### B2) Direct GPU constructors with `GpuIndex*Config.use_cuvs`

**Flat**

```python
cfg = faiss.GpuIndexFlatConfig(); cfg.use_cuvs = True
gpu_flat = faiss.GpuIndexFlatIP(res, d, cfg)
```

**IVF‑Flat / IVF‑PQ**

```python
ivf_cfg = faiss.GpuIndexIVFConfig(); ivf_cfg.use_cuvs = True
# Pass ivf_cfg to the appropriate GpuIndexIVF* constructor for your metric/quantizer
```

**CAGRA** (graph ANN)

```python
# Prefer CPU→GPU cloning unless you need specific GPU-only build options.
# If constructing directly, set the config that inherits GpuIndexConfig:
# cagra_cfg = faiss.GpuIndexCagraConfig(); cagra_cfg.use_cuvs = True
# gpu_cagra = faiss.GpuIndexCagra(res, d, cagra_cfg)
```

**Binary‑CAGRA**

* Use the **cloner** (`GpuClonerOptions.use_cuvs=True`) when moving from CPU binary graph indexes to GPU.

---

### B3) GPU distance primitives — bfKNN (`knn_gpu`/`bfKnn`)

**Exact GPU kNN** (cuVS‑aware):

```python
params = faiss.GpuDistanceParams(); params.metric = faiss.METRIC_INNER_PRODUCT
params.k = 10; params.dims = d
use_cuvs = bool(faiss.should_use_cuvs(params))     # central probe, then:

D, I = faiss.knn_gpu(res, xq.astype('float32'), xb.astype('float32'),
                     k=10, metric=faiss.METRIC_INNER_PRODUCT, use_cuvs=use_cuvs)
```

**Notes**

* Your build routes bfKNN to cuVS when possible; otherwise FAISS kernels are used. Preload cuVS so the probe can return True when viable. 

---

### B4) GPU tuning knobs — `GpuParameterSpace` & configs

* `GpuParameterSpace.set_index_parameter(gpu_index, "nprobe", 64)` — uniform way to set IVF probes.
* IVFPQ config highlights:

  * `usePrecomputedTables` — faster scans at higher memory.
  * `useFloat16LookupTables` — lower memory, slight precision trade‑off; useful for large `m` or tight VRAM budgets.

---

## C) FAISS contrib helpers you’ll likely use

*(These ship in the python package and are handy for pipelines.)*

* **`contrib.factory_tools`** — convenience builders & validators for factory strings.
* **`contrib.ivf_tools`** — IVF diagnostics, centroid utilities.
* **`contrib.big_batch_search`** — large‑batch search helpers to pipeline host↔device copies.
* **`contrib.ondisk`** — on‑disk inverted lists for huge CPU‑side corpora.

Use them to simplify scripts; your production path should still keep **CPU index persisted + GPU clone** at runtime as the source of truth. 

---

## D) cuVS — functions & modules we hadn’t exemplified (now with code)

> cuVS runs **entirely on GPU** with a consistent `build / extend / search / save / load` shape per algorithm. Provide a `Resources` handle to share a stream and avoid implicit syncs; pass **device arrays** for best performance. 

### D1) `neighbors.brute_force` — exact kNN (baseline & QA)

```python
import numpy as np
from cuvs.common import Resources
from cuvs.neighbors import brute_force
from pylibraft.common import device_ndarray

h = Resources()
xb = device_ndarray(np.random.rand(1_000_000, 2560).astype(np.float32))
xq = device_ndarray(np.random.rand(10_000,    2560).astype(np.float32))

index = brute_force.build(brute_force.IndexParams(metric="sqeuclidean"), xb, resources=h)

D = device_ndarray(np.empty((xq.shape[0], 10), np.float32))
I = device_ndarray(np.empty((xq.shape[0], 10), np.int64))
brute_force.search(brute_force.SearchParams(metric="sqeuclidean"),
                   index, xq, k=10, distances=D, neighbors=I, resources=h)

# Incremental ingest
xb2 = device_ndarray(np.random.rand(200_000, 2560).astype(np.float32))
brute_force.extend(index, xb2, resources=h)

# Persistence
brute_force.save(index, "/tmp/cuvs_bruteforce.idx")
index = brute_force.load("/tmp/cuvs_bruteforce.idx", resources=h)
h.sync()
```

**Notes**

* Good for **ground truth** during tuning and for small datasets. 

---

### D2) `neighbors.ivf_flat` — FAISS‑like IVF on GPU

```python
from cuvs.neighbors import ivf_flat

h = Resources()
xb = device_ndarray(np.random.rand(10_000, 2560).astype(np.float32))
xq = device_ndarray(np.random.rand(1_000,  2560).astype(np.float32))

idx = ivf_flat.build(ivf_flat.IndexParams(n_lists=8192, metric="sqeuclidean"), xb, resources=h)

D = device_ndarray(np.empty((xq.shape[0], 10), np.float32))
I = device_ndarray(np.empty((xq.shape[0], 10), np.int64))
ivf_flat.search(ivf_flat.SearchParams(n_probes=64), idx, xq, k=10,
                distances=D, neighbors=I, resources=h)

# Add more
ivf_flat.extend(idx, device_ndarray(np.random.rand(5_000, 2560).astype(np.float32)), resources=h)

# Save / load
ivf_flat.save(idx, "/tmp/cuvs_ivfflat.idx")
idx2 = ivf_flat.load("/tmp/cuvs_ivfflat.idx", resources=h)
h.sync()
```

**Notes**

* Parameter names mirror FAISS concepts: `n_lists`, `n_probes`, metric. 

---

### D3) `neighbors.ivf_pq` — IVF with Product Quantization

*(We’d shown build/search before; here’s `extend/save/load` and host output conversion.)* 

```python
from cuvs.neighbors import ivf_pq

h = Resources()
idx = ivf_pq.build(ivf_pq.IndexParams(n_lists=8192, metric="sqeuclidean", pq_bits=8),
                   xb, resources=h)

# Extend incrementally
ivf_pq.extend(idx, device_ndarray(np.random.rand(20_000, 2560).astype(np.float32)), resources=h)

# Search to host arrays (optional)
D_dev = device_ndarray(np.empty((xq.shape[0], 10), np.float32))
I_dev = device_ndarray(np.empty((xq.shape[0], 10), np.int64))
ivf_pq.search(ivf_pq.SearchParams(n_probes=64), idx, xq, k=10, distances=D_dev, neighbors=I_dev, resources=h)
D_host, I_host = D_dev.copy_to_host(), I_dev.copy_to_host()

# Save / load
ivf_pq.save(idx, "/tmp/cuvs_ivfpq.idx")
idx = ivf_pq.load("/tmp/cuvs_ivfpq.idx", resources=h)
h.sync()
```

---

### D4) `neighbors.cagra` — high‑recall graph ANN

```python
from cuvs.neighbors import cagra

h = Resources()
xb = device_ndarray(np.random.rand(2_000_000, 2560).astype(np.float32))
xq = device_ndarray(np.random.rand(20_000,    2560).astype(np.float32))

idx = cagra.build(cagra.IndexParams(metric="sqeuclidean"), xb, resources=h)

D = device_ndarray(np.empty((xq.shape[0], 10), np.float32))
I = device_ndarray(np.empty((xq.shape[0], 10), np.int64))
cagra.search(cagra.SearchParams(), idx, xq, k=10, distances=D, neighbors=I, resources=h)

# Extend / persist
cagra.extend(idx, device_ndarray(np.random.rand(100_000, 2560).astype(np.float32)), resources=h)
cagra.save(idx, "/tmp/cuvs_cagra.idx")
idx = cagra.load("/tmp/cuvs_cagra.idx", resources=h)
h.sync()
```

**Notes**

* CAGRA is excellent for tight latency at high recall; memory‑heavier than PQ. Use it for premium tiers. 

---

### D5) `neighbors.hnsw`

```python
from cuvs.neighbors import hnsw

h = Resources()
idx = hnsw.build(hnsw.IndexParams(metric="sqeuclidean"), xb, resources=h)
hnsw.search(hnsw.SearchParams(), idx, xq, k=10,
            distances=D, neighbors=I, resources=h)
hnsw.save(idx, "/tmp/cuvs_hnsw.idx")
```

**Notes**

* A familiar graph ANN baseline on GPU. Pick either **CAGRA** (higher perf) or **HNSW** depending on constraints. 

---

### D6) Filters, refine, tiered indexes (high‑level patterns)

* **Filters** — apply post‑search restrictions or pre‑filter candidates with `neighbors.filters` utilities (e.g., by ID set). Use them for **policy constraints** or **tenancy**. 
* **Refine** — re‑score ANN candidates with an exact step (`neighbors.refine`) for **accuracy boosts**. Good when you need Flat re‑ranking on a subset. 
* **Tiered index** — orchestrate multi‑stage pipelines (e.g., IVF‑PQ → refine) using `neighbors.tiered_index` helpers. 

*(Function names follow the `build/search/extend/save/load` theme; wire them like the examples above.)*

---

### D7) Multi‑GPU (`neighbors.mg`) — **distributed IVF‑PQ / IVF‑Flat / CAGRA**

**Conceptual usage** (keep the same `Resources` concept per device and use NCCL):

```python
from cuvs.neighbors.mg import ivf_pq as mg_ivfpq
# Build: mg_ivfpq.build(mg_ivfpq.IndexParams(...), dataset_per_gpu, resources=handles_per_gpu)
# Search: mg_ivfpq.search(..., k=..., resources=handles_per_gpu)
```

**Notes**

* Requires NCCL. Use this path when one GPU’s memory is insufficient or you need cluster‑like throughput while staying on a single multi‑GPU box. 

---

### D8) `distance.pairwise_distance` — GPU distance matrix

```python
from cuvs import distance
Dx = distance.pairwise_distance(xq, xb, metric="sqeuclidean", resources=h)  # Dx on device by default
```

**Notes**

* Handy for diagnostics, unit tests, or custom scoring layers. 

---

### D9) `cluster.kmeans` — GPU KMeans (centers, predict, cost)

```python
from cuvs.cluster import kmeans

centers, inertia, n_iters = kmeans.fit(kmeans.KMeansParams(n_clusters=8192), xb, resources=h)
labels = kmeans.predict(xb, centers, resources=h)
cost = kmeans.cluster_cost(xb, centers, resources=h)
```

**Notes**

* Use for **custom IVF training** or other clustering tasks; interoperates with device arrays. 

---

## E) libcuvs loader — functions & env you’ll use in code

> Preload cuVS and RAPIDS libs **once per process** before any FAISS/cuVS calls to guarantee symbol resolution and allocator setup. 

```python
from libcuvs import load_library
handles = load_library()
print("Loaded:", [h._name for h in handles])  # libcuvs.so, libcuvs_c.so, librmm.so, rapids_logger.so
```

**Environment knobs** (optional):

* `RAPIDS_LIBCUVS_PREFER_SYSTEM_LIBRARY=true` → prefer system libs over wheel‑bundled copies.
* HybridSearch’s `_ensure_cuvs_loader_path()` adds all `lib64/` dirs to `LD_LIBRARY_PATH` for child processes (inheritance is useful for workers). 

---

## F) End‑to‑end patterns that combine the above

### F1) **Filtered, refined IVF‑PQ** on GPU with cuVS

```python
# 0) Preload cuVS (once)
from libcuvs import load_library; load_library()                               # loader. :contentReference[oaicite:25]{index=25}
import faiss, numpy as np

d=2560
cpu = faiss.index_factory(d, "OPQ64,IVF8192,PQ64", faiss.METRIC_INNER_PRODUCT)
train = np.random.randn(10_000_000, d).astype('float32'); faiss.normalize_L2(train)
cpu.train(train)

# ID map to carry your IDs
idmap = faiss.IndexIDMap2(cpu)
xb  = np.random.randn(20_000_000, d).astype('float32'); faiss.normalize_L2(xb)
ids = np.arange(1, 1+xb.shape[0], dtype='int64')
idmap.add_with_ids(xb, ids)

# Clone to GPU with cuVS enabled (fallback safe)
res = faiss.StandardGpuResources(); res.setTempMemory(1_200_000_000); res.setPinnedMemory(512_000_000)
co  = faiss.GpuClonerOptions(); co.use_cuvs=True
try: gpu = faiss.index_cpu_to_gpu(res, 0, idmap, co)
except RuntimeError:
    co.use_cuvs=False; gpu = faiss.index_cpu_to_gpu(res, 0, idmap, co)

# Filter: only IDs in a range, then search & refine
sp = faiss.SearchParametersIVF(); sp.nprobe = 64; sp.sel = faiss.IDSelectorRange(100_000, 500_000)
refiner = faiss.IndexRefineFlat(gpu); refiner.kfactor = 2.0

xq = np.random.randn(1000, d).astype('float32'); faiss.normalize_L2(xq)
D, I = refiner.search(xq, 10, params=sp)
```

**Why this works well for you**

* Cosine via normalization + IP (2560‑d per architecture). 
* cuVS path engaged for IVF‑PQ when available; otherwise FAISS kernels. 
* Filter converted to cuVS filter internally in your build. 

---

### F2) **Direct cuVS** multi‑GPU IVF‑PQ (outline)

```python
# Pseudocode outline (patterns match single-GPU examples; use mg.ivf_pq)
from cuvs.neighbors.mg import ivf_pq as mg_ivfpq
# handles = [Resources() per GPU]; datasets split per GPU as device arrays
# idx = mg_ivfpq.build(mg_ivfpq.IndexParams(n_lists=8192, metric="sqeuclidean", pq_bits=8),
#                      datasets, resources=handles)
# mg_ivfpq.search(mg_ivfpq.SearchParams(n_probes=64), idx, queries, k=10,
#                 distances=..., neighbors=..., resources=handles)
```

**Notes**

* Reach for multi‑GPU cuVS when a single GPU’s VRAM is the bottleneck. 

---

## G) Quick checklist for agents (so you don’t miss a function again)

* **Always normalize** embeddings for cosine/IP.
* Use **`IndexIDMap2`** to own IDs; **`IndexRefineFlat`** when you need exact re‑ranking.
* Set knobs with **`ParameterSpace` / `GpuParameterSpace`**.
* For **filters**, use **`IDSelector*`** + `SearchParametersIVF`; your build converts to cuVS automatically when cuVS is enabled. 
* Persist **CPU** index with `write_index`; **re‑clone** to GPU on startup.
* Preload cuVS (`libcuvs.load_library()`), then set **`use_cuvs=True`** on **cloner/config** for **Flat, IVF‑Flat, IVF‑PQ, CAGRA, Binary‑CAGRA**; guard with try/except or `should_use_cuvs` for **bfKNN**. 
* For **direct cuVS**: use `build/extend/search/save/load` in `neighbors.{ivf_flat, ivf_pq, cagra, hnsw, brute_force}` with **device arrays** and a shared **`Resources`** handle. 
* Keep defaults from the architecture doc (2560‑d, `OPQ64,IVF8192,PQ64`, `nprobe=64`, shards ≤10 M vectors). 

---

If you want, I can also package these into a **reference Jupyter notebook** that runs each snippet end‑to‑end (with small synthetic data) and prints sanity checks (dimensions, top‑K shapes, cuVS enabled flags).

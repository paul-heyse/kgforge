{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"KGForge Documentation","text":"<p>Welcome to the KGForge documentation set. Use the sections below to find onboarding guides, deep dives, API reference material, and architecture decisions.</p> <pre><code>:maxdepth: 2\n:caption: Guide\ngetting-started\nhow-to/index\n</code></pre> <pre><code>:maxdepth: 2\n:caption: Explanations\nexplanations/index\n</code></pre> <pre><code>:maxdepth: 2\n:caption: Reference\nautoapi/index\narchitecture/index\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide walks you through the documentation workflow for KGForge.</p> <ol> <li>Install the documentation extras once: <code>pip install -e \".[docs,docs-agent]\"</code>.</li> <li>Generate docstrings and normalize formatting with <code>make docstrings</code>.</li> <li>Refresh package READMEs with deep links via <code>make readmes</code> (optionally set <code>DOCS_LINK_MODE</code> before running).</li> <li>Build the docs corpus with <code>make html</code>, <code>make json</code>, and <code>make symbols</code> as needed.</li> <li>Run <code>make watch</code> for live reloading while editing content or code.</li> </ol> <p>Export <code>DOCS_LINK_MODE=github</code> when you need GitHub permalinks instead of local editor links. Override <code>DOCS_EDITOR</code> to switch between VS Code and PyCharm deep links.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/docling/","title":"<code>docling</code>","text":"<p>Module for docling.init.</p>"},{"location":"api/docling/canonicalizer/","title":"<code>docling.canonicalizer</code>","text":"<p>Module for docling.canonicalizer.</p>"},{"location":"api/docling/canonicalizer/#docling.canonicalizer.canonicalize_text","title":"<code>canonicalize_text(blocks)</code>","text":"<p>Apply NFC, normalize whitespace and bullets, preserve single newlines between blocks.</p> Source code in <code>src/docling/canonicalizer.py</code> <pre><code>def canonicalize_text(blocks: list[str]) -&gt; str:\n    \"\"\"Apply NFC, normalize whitespace and bullets, preserve single newlines between blocks.\"\"\"\n\n    def norm(s: str) -&gt; str:\n        \"\"\"Norm.\n\n        Args:\n            s (str): TODO.\n\n        Returns:\n            str: TODO.\n        \"\"\"\n        s = unicodedata.normalize(\"NFC\", s)\n        s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n        s = re.sub(r\"[\\u2022\\u25E6\\u2013]\", \"-\", s)  # bullets/dashes\n        s = re.sub(r\"[\\x00-\\x1F]\", \" \", s)\n        s = re.sub(r\"\\s+\", \" \", s).strip()\n        return s\n\n    normed = [norm(b) for b in blocks if b.strip()]\n    return \"\\n\".join(normed)\n</code></pre>"},{"location":"api/docling/hybrid/","title":"<code>docling.hybrid</code>","text":"<p>Module for docling.hybrid.</p>"},{"location":"api/docling/hybrid/#docling.hybrid.HybridChunker","title":"<code>HybridChunker</code>","text":"<p>Placeholder hybrid chunker for Docling outputs.</p> Source code in <code>src/docling/hybrid.py</code> <pre><code>class HybridChunker:\n    \"\"\"Placeholder hybrid chunker for Docling outputs.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/docling/vlm/","title":"<code>docling.vlm</code>","text":"<p>Module for docling.vlm.</p>"},{"location":"api/docling/vlm/#docling.vlm.GraniteDoclingVLM","title":"<code>GraniteDoclingVLM</code>","text":"<p>Expose unified utilities for Docling-compatible VLMs.</p> Source code in <code>src/docling/vlm.py</code> <pre><code>class GraniteDoclingVLM:\n    \"\"\"Expose unified utilities for Docling-compatible VLMs.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/download/","title":"<code>download</code>","text":"<p>Module for download.init.</p>"},{"location":"api/download/cli/","title":"<code>download.cli</code>","text":"<p>Module for download.cli.</p>"},{"location":"api/download/cli/#download.cli.harvest","title":"<code>harvest(topic, years='&gt;=2018', max_works=20000)</code>","text":"<p>Harvest OA metadata and download PDFs (skeleton).</p> Source code in <code>src/download/cli.py</code> <pre><code>@app.command()\ndef harvest(topic: str, years: str = \"&gt;=2018\", max_works: int = 20000) -&gt; None:\n    \"\"\"Harvest OA metadata and download PDFs (skeleton).\"\"\"\n    typer.echo(f\"[dry-run] would harvest topic={topic!r}, years={years}, max_works={max_works}\")\n</code></pre>"},{"location":"api/download/harvester/","title":"<code>download.harvester</code>","text":"<p>Module for download.harvester.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester","title":"<code>OpenAccessHarvester</code>","text":"<p>Openaccessharvester.</p> Source code in <code>src/download/harvester.py</code> <pre><code>class OpenAccessHarvester:\n    \"\"\"Openaccessharvester.\"\"\"\n\n    def __init__(\n        self,\n        user_agent: str,\n        contact_email: str,\n        openalex_base: str = \"https://api.openalex.org\",\n        unpaywall_base: str = \"https://api.unpaywall.org\",\n        pdf_host_base: str | None = None,\n        out_dir: str = \"/data/pdfs\",\n    ):\n        \"\"\"Init.\n\n        Args:\n            user_agent (str): TODO.\n            contact_email (str): TODO.\n            openalex_base (str): TODO.\n            unpaywall_base (str): TODO.\n            pdf_host_base (Optional[str]): TODO.\n            out_dir (str): TODO.\n        \"\"\"\n        self.ua = user_agent\n        self.email = contact_email\n        self.openalex = openalex_base.rstrip(\"/\")\n        self.unpaywall = unpaywall_base.rstrip(\"/\")\n        self.pdf_host = (pdf_host_base or \"\").rstrip(\"/\")\n        self.out_dir = out_dir\n        os.makedirs(self.out_dir, exist_ok=True)\n        self.session = requests.Session()\n        self.session.headers.update({\"User-Agent\": f\"{self.ua} ({self.email})\"})\n\n    def search(self, topic: str, years: str, max_works: int) -&gt; list[dict[str, Any]]:\n        \"\"\"Search.\n\n        Args:\n            topic (str): TODO.\n            years (str): TODO.\n            max_works (int): TODO.\n\n        Returns:\n            list[dict]: TODO.\n        \"\"\"\n        url = f\"{self.openalex}/works\"\n        params = {\"topic\": topic, \"per_page\": min(200, max_works), \"cursor\": \"*\"}\n        r = self.session.get(url, params=params, timeout=30)\n        r.raise_for_status()\n        data = r.json()\n        return data.get(\"results\", [])[:max_works]\n\n    def resolve_pdf(self, work: dict[str, Any]) -&gt; str | None:\n        \"\"\"Resolve pdf.\n\n        Args:\n            work (dict): TODO.\n\n        Returns:\n            Optional[str]: TODO.\n        \"\"\"\n        best = work.get(\"best_oa_location\") or {}\n        if best and best.get(\"pdf_url\"):\n            return best[\"pdf_url\"]\n        for loc in work.get(\"locations\", []):\n            if loc.get(\"pdf_url\"):\n                return loc[\"pdf_url\"]\n        doi = work.get(\"doi\")\n        if doi:\n            r = self.session.get(\n                f\"{self.unpaywall}/v2/{doi}\", params={\"email\": self.email}, timeout=15\n            )\n            if r.ok:\n                j = r.json()\n                url = (j.get(\"best_oa_location\") or {}).get(\"url_for_pdf\")\n                if url:\n                    return url\n        if self.pdf_host and doi:\n            return f\"{self.pdf_host}/pdf/{doi.replace('/', '_')}.pdf\"\n        return None\n\n    def download_pdf(self, url: str, target_path: str) -&gt; str:\n        \"\"\"Download pdf.\n\n        Args:\n            url (str): TODO.\n            target_path (str): TODO.\n\n        Returns:\n            str: TODO.\n        \"\"\"\n        r = self.session.get(url, timeout=60)\n        if r.status_code != 200:\n            raise DownloadError(f\"Bad status {r.status_code} for {url}\")\n        ctype = r.headers.get(\"Content-Type\", \"application/pdf\")\n        if not ctype.startswith(\"application/\"):\n            raise UnsupportedMIMEError(f\"Not a PDF-like content-type: {ctype}\")\n        with open(target_path, \"wb\") as f:\n            f.write(r.content)\n        return target_path\n\n    def run(self, topic: str, years: str, max_works: int) -&gt; list[Doc]:\n        \"\"\"Run.\n\n        Args:\n            topic (str): TODO.\n            years (str): TODO.\n            max_works (int): TODO.\n\n        Returns:\n            List[Doc]: TODO.\n        \"\"\"\n        docs: list[Doc] = []\n        works = self.search(topic, years, max_works)\n        for w in works:\n            pdf_url = self.resolve_pdf(w)\n            if not pdf_url:\n                continue\n            filename = (w.get(\"doi\") or w.get(\"id\") or str(int(time.time() * 1000))).replace(\n                \"/\", \"_\"\n            ) + \".pdf\"\n            dest = os.path.join(self.out_dir, filename)\n            self.download_pdf(pdf_url, dest)\n            doc = Doc(\n                id=f\"urn:doc:source:openalex:{w.get('id','unknown')}\",\n                openalex_id=w.get(\"id\"),\n                doi=w.get(\"doi\"),\n                title=w.get(\"title\", \"\"),\n                authors=[],\n                pub_date=None,\n                license=None,\n                language=\"en\",\n                pdf_uri=dest,\n                source=\"openalex\",\n                content_hash=None,\n            )\n            docs.append(doc)\n        return docs\n</code></pre>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.__init__","title":"<code>__init__(user_agent, contact_email, openalex_base='https://api.openalex.org', unpaywall_base='https://api.unpaywall.org', pdf_host_base=None, out_dir='/data/pdfs')</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>user_agent</code> <code>str</code> <p>TODO.</p> required <code>contact_email</code> <code>str</code> <p>TODO.</p> required <code>openalex_base</code> <code>str</code> <p>TODO.</p> <code>'https://api.openalex.org'</code> <code>unpaywall_base</code> <code>str</code> <p>TODO.</p> <code>'https://api.unpaywall.org'</code> <code>pdf_host_base</code> <code>Optional[str]</code> <p>TODO.</p> <code>None</code> <code>out_dir</code> <code>str</code> <p>TODO.</p> <code>'/data/pdfs'</code> Source code in <code>src/download/harvester.py</code> <pre><code>def __init__(\n    self,\n    user_agent: str,\n    contact_email: str,\n    openalex_base: str = \"https://api.openalex.org\",\n    unpaywall_base: str = \"https://api.unpaywall.org\",\n    pdf_host_base: str | None = None,\n    out_dir: str = \"/data/pdfs\",\n):\n    \"\"\"Init.\n\n    Args:\n        user_agent (str): TODO.\n        contact_email (str): TODO.\n        openalex_base (str): TODO.\n        unpaywall_base (str): TODO.\n        pdf_host_base (Optional[str]): TODO.\n        out_dir (str): TODO.\n    \"\"\"\n    self.ua = user_agent\n    self.email = contact_email\n    self.openalex = openalex_base.rstrip(\"/\")\n    self.unpaywall = unpaywall_base.rstrip(\"/\")\n    self.pdf_host = (pdf_host_base or \"\").rstrip(\"/\")\n    self.out_dir = out_dir\n    os.makedirs(self.out_dir, exist_ok=True)\n    self.session = requests.Session()\n    self.session.headers.update({\"User-Agent\": f\"{self.ua} ({self.email})\"})\n</code></pre>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.download_pdf","title":"<code>download_pdf(url, target_path)</code>","text":"<p>Download pdf.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>TODO.</p> required <code>target_path</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>TODO.</p> Source code in <code>src/download/harvester.py</code> <pre><code>def download_pdf(self, url: str, target_path: str) -&gt; str:\n    \"\"\"Download pdf.\n\n    Args:\n        url (str): TODO.\n        target_path (str): TODO.\n\n    Returns:\n        str: TODO.\n    \"\"\"\n    r = self.session.get(url, timeout=60)\n    if r.status_code != 200:\n        raise DownloadError(f\"Bad status {r.status_code} for {url}\")\n    ctype = r.headers.get(\"Content-Type\", \"application/pdf\")\n    if not ctype.startswith(\"application/\"):\n        raise UnsupportedMIMEError(f\"Not a PDF-like content-type: {ctype}\")\n    with open(target_path, \"wb\") as f:\n        f.write(r.content)\n    return target_path\n</code></pre>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.resolve_pdf","title":"<code>resolve_pdf(work)</code>","text":"<p>Resolve pdf.</p> <p>Parameters:</p> Name Type Description Default <code>work</code> <code>dict</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Optional[str]: TODO.</p> Source code in <code>src/download/harvester.py</code> <pre><code>def resolve_pdf(self, work: dict[str, Any]) -&gt; str | None:\n    \"\"\"Resolve pdf.\n\n    Args:\n        work (dict): TODO.\n\n    Returns:\n        Optional[str]: TODO.\n    \"\"\"\n    best = work.get(\"best_oa_location\") or {}\n    if best and best.get(\"pdf_url\"):\n        return best[\"pdf_url\"]\n    for loc in work.get(\"locations\", []):\n        if loc.get(\"pdf_url\"):\n            return loc[\"pdf_url\"]\n    doi = work.get(\"doi\")\n    if doi:\n        r = self.session.get(\n            f\"{self.unpaywall}/v2/{doi}\", params={\"email\": self.email}, timeout=15\n        )\n        if r.ok:\n            j = r.json()\n            url = (j.get(\"best_oa_location\") or {}).get(\"url_for_pdf\")\n            if url:\n                return url\n    if self.pdf_host and doi:\n        return f\"{self.pdf_host}/pdf/{doi.replace('/', '_')}.pdf\"\n    return None\n</code></pre>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.run","title":"<code>run(topic, years, max_works)</code>","text":"<p>Run.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>TODO.</p> required <code>years</code> <code>str</code> <p>TODO.</p> required <code>max_works</code> <code>int</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>list[Doc]</code> <p>List[Doc]: TODO.</p> Source code in <code>src/download/harvester.py</code> <pre><code>def run(self, topic: str, years: str, max_works: int) -&gt; list[Doc]:\n    \"\"\"Run.\n\n    Args:\n        topic (str): TODO.\n        years (str): TODO.\n        max_works (int): TODO.\n\n    Returns:\n        List[Doc]: TODO.\n    \"\"\"\n    docs: list[Doc] = []\n    works = self.search(topic, years, max_works)\n    for w in works:\n        pdf_url = self.resolve_pdf(w)\n        if not pdf_url:\n            continue\n        filename = (w.get(\"doi\") or w.get(\"id\") or str(int(time.time() * 1000))).replace(\n            \"/\", \"_\"\n        ) + \".pdf\"\n        dest = os.path.join(self.out_dir, filename)\n        self.download_pdf(pdf_url, dest)\n        doc = Doc(\n            id=f\"urn:doc:source:openalex:{w.get('id','unknown')}\",\n            openalex_id=w.get(\"id\"),\n            doi=w.get(\"doi\"),\n            title=w.get(\"title\", \"\"),\n            authors=[],\n            pub_date=None,\n            license=None,\n            language=\"en\",\n            pdf_uri=dest,\n            source=\"openalex\",\n            content_hash=None,\n        )\n        docs.append(doc)\n    return docs\n</code></pre>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.search","title":"<code>search(topic, years, max_works)</code>","text":"<p>Search.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>TODO.</p> required <code>years</code> <code>str</code> <p>TODO.</p> required <code>max_works</code> <code>int</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>list[dict]: TODO.</p> Source code in <code>src/download/harvester.py</code> <pre><code>def search(self, topic: str, years: str, max_works: int) -&gt; list[dict[str, Any]]:\n    \"\"\"Search.\n\n    Args:\n        topic (str): TODO.\n        years (str): TODO.\n        max_works (int): TODO.\n\n    Returns:\n        list[dict]: TODO.\n    \"\"\"\n    url = f\"{self.openalex}/works\"\n    params = {\"topic\": topic, \"per_page\": min(200, max_works), \"cursor\": \"*\"}\n    r = self.session.get(url, params=params, timeout=30)\n    r.raise_for_status()\n    data = r.json()\n    return data.get(\"results\", [])[:max_works]\n</code></pre>"},{"location":"api/embeddings_dense/","title":"<code>embeddings_dense</code>","text":"<p>Module for embeddings_dense.init.</p>"},{"location":"api/embeddings_dense/base/","title":"<code>embeddings_dense.base</code>","text":"<p>Module for embeddings_dense.base.</p>"},{"location":"api/embeddings_dense/base/#embeddings_dense.base.DenseEmbedder","title":"<code>DenseEmbedder</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol describing a dense embedding provider.</p> Source code in <code>src/embeddings_dense/base.py</code> <pre><code>class DenseEmbedder(Protocol):\n    \"\"\"Protocol describing a dense embedding provider.\"\"\"\n\n    name: str\n    dim: int\n\n    def embed_texts(self, texts: list[str]) -&gt; np.ndarray:\n        \"\"\"Return embeddings for the supplied texts.\"\"\"\n\n        ...\n</code></pre>"},{"location":"api/embeddings_dense/base/#embeddings_dense.base.DenseEmbedder.embed_texts","title":"<code>embed_texts(texts)</code>","text":"<p>Return embeddings for the supplied texts.</p> Source code in <code>src/embeddings_dense/base.py</code> <pre><code>def embed_texts(self, texts: list[str]) -&gt; np.ndarray:\n    \"\"\"Return embeddings for the supplied texts.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/embeddings_dense/qwen3/","title":"<code>embeddings_dense.qwen3</code>","text":"<p>Module for embeddings_dense.qwen3.</p>"},{"location":"api/embeddings_dense/qwen3/#embeddings_dense.qwen3.Qwen3Embedder","title":"<code>Qwen3Embedder</code>","text":"<p>Adapter for Qwen3 dense embedding checkpoints.</p> Source code in <code>src/embeddings_dense/qwen3.py</code> <pre><code>class Qwen3Embedder:\n    \"\"\"Adapter for Qwen3 dense embedding checkpoints.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/embeddings_sparse/","title":"<code>embeddings_sparse</code>","text":"<p>Module for embeddings_sparse.init.</p>"},{"location":"api/embeddings_sparse/base/","title":"<code>embeddings_sparse.base</code>","text":"<p>Module for embeddings_sparse.base.</p>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseEncoder","title":"<code>SparseEncoder</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for sparse text encoders.</p> Source code in <code>src/embeddings_sparse/base.py</code> <pre><code>class SparseEncoder(Protocol):\n    \"\"\"Protocol for sparse text encoders.\"\"\"\n\n    name: str\n\n    def encode(self, texts: list[str]) -&gt; list[tuple[list[int], list[float]]]:\n        \"\"\"Return sparse encodings for the given texts.\"\"\"\n\n        ...\n</code></pre>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseEncoder.encode","title":"<code>encode(texts)</code>","text":"<p>Return sparse encodings for the given texts.</p> Source code in <code>src/embeddings_sparse/base.py</code> <pre><code>def encode(self, texts: list[str]) -&gt; list[tuple[list[int], list[float]]]:\n    \"\"\"Return sparse encodings for the given texts.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseIndex","title":"<code>SparseIndex</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol describing sparse index interactions.</p> Source code in <code>src/embeddings_sparse/base.py</code> <pre><code>class SparseIndex(Protocol):\n    \"\"\"Protocol describing sparse index interactions.\"\"\"\n\n    def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n        \"\"\"Build the index from the supplied documents.\"\"\"\n\n        ...\n\n    def search(\n        self, query: str, k: int, fields: Mapping[str, str] | None = None\n    ) -&gt; list[tuple[str, float]]:\n        \"\"\"Search the index and return (id, score) tuples.\"\"\"\n\n        ...\n</code></pre>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseIndex.build","title":"<code>build(docs_iterable)</code>","text":"<p>Build the index from the supplied documents.</p> Source code in <code>src/embeddings_sparse/base.py</code> <pre><code>def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n    \"\"\"Build the index from the supplied documents.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseIndex.search","title":"<code>search(query, k, fields=None)</code>","text":"<p>Search the index and return (id, score) tuples.</p> Source code in <code>src/embeddings_sparse/base.py</code> <pre><code>def search(\n    self, query: str, k: int, fields: Mapping[str, str] | None = None\n) -&gt; list[tuple[str, float]]:\n    \"\"\"Search the index and return (id, score) tuples.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/","title":"<code>embeddings_sparse.bm25</code>","text":"<p>Module for embeddings_sparse.bm25.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.BM25Doc","title":"<code>BM25Doc</code>  <code>dataclass</code>","text":"<p>Bm25doc.</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>@dataclass\nclass BM25Doc:\n    \"\"\"Bm25doc.\"\"\"\n\n    doc_id: str\n    length: int\n    fields: dict[str, str]\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25","title":"<code>LuceneBM25</code>","text":"<p>Pyserini-backed Lucene BM25 adapter.</p> <p>Lazily imported.</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>class LuceneBM25:\n    \"\"\"Pyserini-backed Lucene BM25 adapter.\n\n    Lazily imported.\n    \"\"\"\n\n    def __init__(\n        self,\n        index_dir: str,\n        k1: float = 0.9,\n        b: float = 0.4,\n        field_boosts: dict[str, float] | None = None,\n    ):\n        \"\"\"Init.\n\n        Args:\n            index_dir (str): TODO.\n            k1 (float): TODO.\n            b (float): TODO.\n            field_boosts (Optional[Dict[str,float]]): TODO.\n        \"\"\"\n        self.index_dir = index_dir\n        self.k1 = k1\n        self.b = b\n        self.field_boosts = field_boosts or {\"title\": 2.0, \"section\": 1.2, \"body\": 1.0}\n        self._searcher: Any | None = None\n\n    def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n        \"\"\"Build.\n\n        Args:\n            docs_iterable (Iterable[Tuple[str, Dict]]): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        try:\n            from pyserini.analysis import get_lucene_analyzer\n            from pyserini.index import IndexWriter\n        except Exception as e:\n            raise RuntimeError(\"Pyserini/Lucene not available\") from e\n        os.makedirs(self.index_dir, exist_ok=True)\n        analyzer = get_lucene_analyzer(stemmer=\"english\", stopwords=True)\n        writer = IndexWriter(self.index_dir, analyzer=analyzer, keep_stopwords=False)\n        for doc_id, fields in docs_iterable:\n            # combine fields with boosts in a \"contents\" field for simplicity\n            title = fields.get(\"title\", \"\")\n            section = fields.get(\"section\", \"\")\n            body = fields.get(\"body\", \"\")\n            contents = \" \".join(\n                [\n                    (title + \" \") * int(self.field_boosts.get(\"title\", 1.0)),\n                    (section + \" \") * int(self.field_boosts.get(\"section\", 1.0)),\n                    body,\n                ]\n            )\n            writer.add_document(docid=doc_id, contents=contents)\n        writer.close()\n\n    def _ensure_searcher(self) -&gt; None:\n        \"\"\"Ensure searcher.\"\"\"\n        if self._searcher is not None:\n            return\n        from pyserini.search.lucene import LuceneSearcher\n\n        self._searcher = LuceneSearcher(self.index_dir)\n        self._searcher.set_bm25(self.k1, self.b)\n\n    def search(\n        self, query: str, k: int, fields: dict[str, str] | None = None\n    ) -&gt; list[tuple[str, float]]:\n        \"\"\"Search.\n\n        Args:\n            query (str): TODO.\n            k (int): TODO.\n            fields (Dict | None): TODO.\n\n        Returns:\n            List[Tuple[str, float]]: TODO.\n        \"\"\"\n        self._ensure_searcher()\n        if self._searcher is None:\n            raise RuntimeError(\"Lucene searcher not initialized\")\n        hits = self._searcher.search(query, k=k)\n        return [(h.docid, float(h.score)) for h in hits]\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25.__init__","title":"<code>__init__(index_dir, k1=0.9, b=0.4, field_boosts=None)</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>index_dir</code> <code>str</code> <p>TODO.</p> required <code>k1</code> <code>float</code> <p>TODO.</p> <code>0.9</code> <code>b</code> <code>float</code> <p>TODO.</p> <code>0.4</code> <code>field_boosts</code> <code>Optional[Dict[str, float]]</code> <p>TODO.</p> <code>None</code> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def __init__(\n    self,\n    index_dir: str,\n    k1: float = 0.9,\n    b: float = 0.4,\n    field_boosts: dict[str, float] | None = None,\n):\n    \"\"\"Init.\n\n    Args:\n        index_dir (str): TODO.\n        k1 (float): TODO.\n        b (float): TODO.\n        field_boosts (Optional[Dict[str,float]]): TODO.\n    \"\"\"\n    self.index_dir = index_dir\n    self.k1 = k1\n    self.b = b\n    self.field_boosts = field_boosts or {\"title\": 2.0, \"section\": 1.2, \"body\": 1.0}\n    self._searcher: Any | None = None\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25.build","title":"<code>build(docs_iterable)</code>","text":"<p>Build.</p> <p>Parameters:</p> Name Type Description Default <code>docs_iterable</code> <code>Iterable[Tuple[str, Dict]]</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n    \"\"\"Build.\n\n    Args:\n        docs_iterable (Iterable[Tuple[str, Dict]]): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    try:\n        from pyserini.analysis import get_lucene_analyzer\n        from pyserini.index import IndexWriter\n    except Exception as e:\n        raise RuntimeError(\"Pyserini/Lucene not available\") from e\n    os.makedirs(self.index_dir, exist_ok=True)\n    analyzer = get_lucene_analyzer(stemmer=\"english\", stopwords=True)\n    writer = IndexWriter(self.index_dir, analyzer=analyzer, keep_stopwords=False)\n    for doc_id, fields in docs_iterable:\n        # combine fields with boosts in a \"contents\" field for simplicity\n        title = fields.get(\"title\", \"\")\n        section = fields.get(\"section\", \"\")\n        body = fields.get(\"body\", \"\")\n        contents = \" \".join(\n            [\n                (title + \" \") * int(self.field_boosts.get(\"title\", 1.0)),\n                (section + \" \") * int(self.field_boosts.get(\"section\", 1.0)),\n                body,\n            ]\n        )\n        writer.add_document(docid=doc_id, contents=contents)\n    writer.close()\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25.search","title":"<code>search(query, k, fields=None)</code>","text":"<p>Search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>TODO.</p> required <code>k</code> <code>int</code> <p>TODO.</p> required <code>fields</code> <code>Dict | None</code> <p>TODO.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[str, float]]</code> <p>List[Tuple[str, float]]: TODO.</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def search(\n    self, query: str, k: int, fields: dict[str, str] | None = None\n) -&gt; list[tuple[str, float]]:\n    \"\"\"Search.\n\n    Args:\n        query (str): TODO.\n        k (int): TODO.\n        fields (Dict | None): TODO.\n\n    Returns:\n        List[Tuple[str, float]]: TODO.\n    \"\"\"\n    self._ensure_searcher()\n    if self._searcher is None:\n        raise RuntimeError(\"Lucene searcher not initialized\")\n    hits = self._searcher.search(query, k=k)\n    return [(h.docid, float(h.score)) for h in hits]\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25","title":"<code>PurePythonBM25</code>","text":"<p>Simple offline BM25 builder &amp; searcher (Okapi BM25).</p> <p>Persisted as a pickle. Fields: title, section, body with configurable boosts.</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>class PurePythonBM25:\n    \"\"\"Simple offline BM25 builder &amp; searcher (Okapi BM25).\n\n    Persisted as a pickle.\n    Fields: title, section, body with configurable boosts.\n    \"\"\"\n\n    def __init__(\n        self,\n        index_dir: str,\n        k1: float = 0.9,\n        b: float = 0.4,\n        field_boosts: dict[str, float] | None = None,\n    ):\n        \"\"\"Init.\n\n        Args:\n            index_dir (str): TODO.\n            k1 (float): TODO.\n            b (float): TODO.\n            field_boosts (Optional[Dict[str, float]]): TODO.\n        \"\"\"\n        self.index_dir = index_dir\n        self.k1 = k1\n        self.b = b\n        self.field_boosts = field_boosts or {\"title\": 2.0, \"section\": 1.2, \"body\": 1.0}\n        self.df: dict[str, int] = {}\n        self.postings: dict[str, dict[str, int]] = {}\n        self.docs: dict[str, BM25Doc] = {}\n        self.N = 0\n        self.avgdl = 0.0\n\n    @staticmethod\n    def _tokenize(text: str) -&gt; list[str]:\n        \"\"\"Tokenize.\n\n        Args:\n            text (str): TODO.\n\n        Returns:\n            List[str]: TODO.\n        \"\"\"\n        return [t.lower() for t in TOKEN_RE.findall(text)]\n\n    def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n        \"\"\"Build.\n\n        Args:\n            docs_iterable (Iterable[Tuple[str, Dict]]): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        os.makedirs(self.index_dir, exist_ok=True)\n        df: dict[str, int] = defaultdict(int)\n        postings: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))\n        docs: dict[str, BM25Doc] = {}\n        lengths: list[int] = []\n        for doc_id, fields in docs_iterable:\n            body = fields.get(\"body\", \"\")\n            section = fields.get(\"section\", \"\")\n            title = fields.get(\"title\", \"\")\n            # field boosts applied at scoring time; here we merge for length calc\n            toks = self._tokenize(title + \" \" + section + \" \" + body)\n            lengths.append(len(toks))\n            docs[doc_id] = BM25Doc(\n                doc_id=doc_id,\n                length=len(toks),\n                fields={\"title\": title, \"section\": section, \"body\": body},\n            )\n            seen = set()\n            for tok in toks:\n                postings[tok][doc_id] += 1\n                if tok not in seen:\n                    df[tok] += 1\n                    seen.add(tok)\n        self.N = len(docs)\n        self.avgdl = sum(lengths) / max(1, len(lengths))\n        self.df = dict(df)\n        # convert defaultdicts\n        self.postings = {t: dict(ps) for t, ps in postings.items()}\n        self.docs = docs\n        # persist\n        with open(os.path.join(self.index_dir, \"pure_bm25.pkl\"), \"wb\") as f:\n            pickle.dump(\n                {\n                    \"k1\": self.k1,\n                    \"b\": self.b,\n                    \"field_boosts\": self.field_boosts,\n                    \"df\": self.df,\n                    \"postings\": self.postings,\n                    \"docs\": self.docs,\n                    \"N\": self.N,\n                    \"avgdl\": self.avgdl,\n                },\n                f,\n                protocol=pickle.HIGHEST_PROTOCOL,\n            )\n\n    def load(self) -&gt; None:\n        \"\"\"Load.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        path = os.path.join(self.index_dir, \"pure_bm25.pkl\")\n        with open(path, \"rb\") as f:\n            data = pickle.load(f)\n        self.k1 = data[\"k1\"]\n        self.b = data[\"b\"]\n        self.field_boosts = data[\"field_boosts\"]\n        self.df = data[\"df\"]\n        self.postings = data[\"postings\"]\n        self.docs = data[\"docs\"]\n        self.N = data[\"N\"]\n        self.avgdl = data[\"avgdl\"]\n\n    def _idf(self, term: str) -&gt; float:\n        \"\"\"Idf.\n\n        Args:\n            term (str): TODO.\n\n        Returns:\n            float: TODO.\n        \"\"\"\n        n_t = self.df.get(term, 0)\n        if n_t == 0:\n            return 0.0\n        # BM25 idf variant\n        return math.log((self.N - n_t + 0.5) / (n_t + 0.5) + 1.0)\n\n    def search(\n        self, query: str, k: int, fields: Mapping[str, str] | None = None\n    ) -&gt; list[tuple[str, float]]:\n        \"\"\"Search.\n\n        Args:\n            query (str): TODO.\n            k (int): TODO.\n            fields (Dict | None): TODO.\n\n        Returns:\n            List[Tuple[str, float]]: TODO.\n        \"\"\"\n        # naive field weighting at score aggregation (title/section/body contributions)\n        tokens = self._tokenize(query)\n        scores: dict[str, float] = defaultdict(float)\n        for term in tokens:\n            idf = self._idf(term)\n            postings = self.postings.get(term)\n            if not postings:\n                continue\n            for doc_id, tf in postings.items():\n                doc = self.docs[doc_id]\n                dl = doc.length or 1\n                denom = tf + self.k1 * (1 - self.b + self.b * (dl / self.avgdl))\n                contrib = idf * ((tf * (self.k1 + 1)) / (denom))\n                scores[doc_id] += contrib\n        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n        return ranked\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.__init__","title":"<code>__init__(index_dir, k1=0.9, b=0.4, field_boosts=None)</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>index_dir</code> <code>str</code> <p>TODO.</p> required <code>k1</code> <code>float</code> <p>TODO.</p> <code>0.9</code> <code>b</code> <code>float</code> <p>TODO.</p> <code>0.4</code> <code>field_boosts</code> <code>Optional[Dict[str, float]]</code> <p>TODO.</p> <code>None</code> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def __init__(\n    self,\n    index_dir: str,\n    k1: float = 0.9,\n    b: float = 0.4,\n    field_boosts: dict[str, float] | None = None,\n):\n    \"\"\"Init.\n\n    Args:\n        index_dir (str): TODO.\n        k1 (float): TODO.\n        b (float): TODO.\n        field_boosts (Optional[Dict[str, float]]): TODO.\n    \"\"\"\n    self.index_dir = index_dir\n    self.k1 = k1\n    self.b = b\n    self.field_boosts = field_boosts or {\"title\": 2.0, \"section\": 1.2, \"body\": 1.0}\n    self.df: dict[str, int] = {}\n    self.postings: dict[str, dict[str, int]] = {}\n    self.docs: dict[str, BM25Doc] = {}\n    self.N = 0\n    self.avgdl = 0.0\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.build","title":"<code>build(docs_iterable)</code>","text":"<p>Build.</p> <p>Parameters:</p> Name Type Description Default <code>docs_iterable</code> <code>Iterable[Tuple[str, Dict]]</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n    \"\"\"Build.\n\n    Args:\n        docs_iterable (Iterable[Tuple[str, Dict]]): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    os.makedirs(self.index_dir, exist_ok=True)\n    df: dict[str, int] = defaultdict(int)\n    postings: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))\n    docs: dict[str, BM25Doc] = {}\n    lengths: list[int] = []\n    for doc_id, fields in docs_iterable:\n        body = fields.get(\"body\", \"\")\n        section = fields.get(\"section\", \"\")\n        title = fields.get(\"title\", \"\")\n        # field boosts applied at scoring time; here we merge for length calc\n        toks = self._tokenize(title + \" \" + section + \" \" + body)\n        lengths.append(len(toks))\n        docs[doc_id] = BM25Doc(\n            doc_id=doc_id,\n            length=len(toks),\n            fields={\"title\": title, \"section\": section, \"body\": body},\n        )\n        seen = set()\n        for tok in toks:\n            postings[tok][doc_id] += 1\n            if tok not in seen:\n                df[tok] += 1\n                seen.add(tok)\n    self.N = len(docs)\n    self.avgdl = sum(lengths) / max(1, len(lengths))\n    self.df = dict(df)\n    # convert defaultdicts\n    self.postings = {t: dict(ps) for t, ps in postings.items()}\n    self.docs = docs\n    # persist\n    with open(os.path.join(self.index_dir, \"pure_bm25.pkl\"), \"wb\") as f:\n        pickle.dump(\n            {\n                \"k1\": self.k1,\n                \"b\": self.b,\n                \"field_boosts\": self.field_boosts,\n                \"df\": self.df,\n                \"postings\": self.postings,\n                \"docs\": self.docs,\n                \"N\": self.N,\n                \"avgdl\": self.avgdl,\n            },\n            f,\n            protocol=pickle.HIGHEST_PROTOCOL,\n        )\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.load","title":"<code>load()</code>","text":"<p>Load.</p> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    path = os.path.join(self.index_dir, \"pure_bm25.pkl\")\n    with open(path, \"rb\") as f:\n        data = pickle.load(f)\n    self.k1 = data[\"k1\"]\n    self.b = data[\"b\"]\n    self.field_boosts = data[\"field_boosts\"]\n    self.df = data[\"df\"]\n    self.postings = data[\"postings\"]\n    self.docs = data[\"docs\"]\n    self.N = data[\"N\"]\n    self.avgdl = data[\"avgdl\"]\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.search","title":"<code>search(query, k, fields=None)</code>","text":"<p>Search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>TODO.</p> required <code>k</code> <code>int</code> <p>TODO.</p> required <code>fields</code> <code>Dict | None</code> <p>TODO.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[str, float]]</code> <p>List[Tuple[str, float]]: TODO.</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def search(\n    self, query: str, k: int, fields: Mapping[str, str] | None = None\n) -&gt; list[tuple[str, float]]:\n    \"\"\"Search.\n\n    Args:\n        query (str): TODO.\n        k (int): TODO.\n        fields (Dict | None): TODO.\n\n    Returns:\n        List[Tuple[str, float]]: TODO.\n    \"\"\"\n    # naive field weighting at score aggregation (title/section/body contributions)\n    tokens = self._tokenize(query)\n    scores: dict[str, float] = defaultdict(float)\n    for term in tokens:\n        idf = self._idf(term)\n        postings = self.postings.get(term)\n        if not postings:\n            continue\n        for doc_id, tf in postings.items():\n            doc = self.docs[doc_id]\n            dl = doc.length or 1\n            denom = tf + self.k1 * (1 - self.b + self.b * (dl / self.avgdl))\n            contrib = idf * ((tf * (self.k1 + 1)) / (denom))\n            scores[doc_id] += contrib\n    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n    return ranked\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.get_bm25","title":"<code>get_bm25(backend, index_dir, **kwargs)</code>","text":"<p>Get bm25.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>TODO.</p> required <code>index_dir</code> <code>str</code> <p>TODO.</p> required <code>kwargs</code> <code>Any</code> <p>TODO.</p> <code>{}</code> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def get_bm25(backend: str, index_dir: str, **kwargs: Any) -&gt; PurePythonBM25 | LuceneBM25:\n    \"\"\"Get bm25.\n\n    Args:\n        backend (str): TODO.\n        index_dir (str): TODO.\n        kwargs: TODO.\n    \"\"\"\n    if backend == \"lucene\":\n        try:\n            return LuceneBM25(index_dir, **kwargs)\n        except Exception:\n            # allow fallback creation\n            pass\n    return PurePythonBM25(index_dir, **kwargs)\n</code></pre>"},{"location":"api/embeddings_sparse/splade/","title":"<code>embeddings_sparse.splade</code>","text":"<p>Module for embeddings_sparse.splade.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.LuceneImpactIndex","title":"<code>LuceneImpactIndex</code>","text":"<p>Pyserini SPLADE impact index wrapper.</p> <p>Requires Pyserini build step that writes an impact index on disk.</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>class LuceneImpactIndex:\n    \"\"\"Pyserini SPLADE impact index wrapper.\n\n    Requires Pyserini build step that writes an impact index on disk.\n    \"\"\"\n\n    def __init__(self, index_dir: str):\n        \"\"\"Init.\n\n        Args:\n            index_dir (str): TODO.\n        \"\"\"\n        self.index_dir = index_dir\n        self._searcher: Any | None = None\n\n    def _ensure(self) -&gt; None:\n        \"\"\"Ensure.\"\"\"\n        if self._searcher is not None:\n            return\n        try:\n            from pyserini.search.lucene import LuceneImpactSearcher\n        except Exception as e:\n            raise RuntimeError(\"Pyserini not available for SPLADE impact search\") from e\n        self._searcher = LuceneImpactSearcher(self.index_dir)\n\n    def search(self, query: str, k: int) -&gt; list[tuple[str, float]]:\n        \"\"\"Search.\n\n        Args:\n            query (str): TODO.\n            k (int): TODO.\n\n        Returns:\n            List[Tuple[str, float]]: TODO.\n        \"\"\"\n        self._ensure()\n        if self._searcher is None:\n            raise RuntimeError(\"Lucene impact searcher not initialized\")\n        hits = self._searcher.search(query, k=k)  # expects query to be SPLADE-encoded string\n        return [(h.docid, float(h.score)) for h in hits]\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.LuceneImpactIndex.__init__","title":"<code>__init__(index_dir)</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>index_dir</code> <code>str</code> <p>TODO.</p> required Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def __init__(self, index_dir: str):\n    \"\"\"Init.\n\n    Args:\n        index_dir (str): TODO.\n    \"\"\"\n    self.index_dir = index_dir\n    self._searcher: Any | None = None\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.LuceneImpactIndex.search","title":"<code>search(query, k)</code>","text":"<p>Search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>TODO.</p> required <code>k</code> <code>int</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, float]]</code> <p>List[Tuple[str, float]]: TODO.</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def search(self, query: str, k: int) -&gt; list[tuple[str, float]]:\n    \"\"\"Search.\n\n    Args:\n        query (str): TODO.\n        k (int): TODO.\n\n    Returns:\n        List[Tuple[str, float]]: TODO.\n    \"\"\"\n    self._ensure()\n    if self._searcher is None:\n        raise RuntimeError(\"Lucene impact searcher not initialized\")\n    hits = self._searcher.search(query, k=k)  # expects query to be SPLADE-encoded string\n    return [(h.docid, float(h.score)) for h in hits]\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex","title":"<code>PureImpactIndex</code>","text":"<p>Toy 'impact' index that approximates SPLADE with IDF/log weighting.</p> <ul> <li>Keeps the pipeline runnable without GPUs or Pyserini.</li> <li>Substitutes a simple tokenizer plus weighting scheme for the neural encoder.</li> </ul> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>class PureImpactIndex:\n    \"\"\"Toy 'impact' index that approximates SPLADE with IDF/log weighting.\n\n    - Keeps the pipeline runnable without GPUs or Pyserini.\n    - Substitutes a simple tokenizer plus weighting scheme for the neural encoder.\n    \"\"\"\n\n    def __init__(self, index_dir: str):\n        \"\"\"Init.\n\n        Args:\n            index_dir (str): TODO.\n        \"\"\"\n        self.index_dir = index_dir\n        self.df: dict[str, int] = {}\n        self.N = 0\n        self.postings: dict[str, dict[str, float]] = {}\n\n    @staticmethod\n    def _tokenize(text: str) -&gt; list[str]:\n        \"\"\"Tokenize.\n\n        Args:\n            text (str): TODO.\n\n        Returns:\n            List[str]: TODO.\n        \"\"\"\n        return [t.lower() for t in TOKEN_RE.findall(text)]\n\n    def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n        \"\"\"Build.\n\n        Args:\n            docs_iterable (Iterable[Tuple[str, Dict]]): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        os.makedirs(self.index_dir, exist_ok=True)\n        df: dict[str, int] = defaultdict(int)\n        postings: dict[str, dict[str, float]] = defaultdict(lambda: defaultdict(float))\n        N = 0\n        for doc_id, fields in docs_iterable:\n            text = \" \".join(\n                [fields.get(\"title\", \"\"), fields.get(\"section\", \"\"), fields.get(\"body\", \"\")]\n            )\n            toks = self._tokenize(text)\n            N += 1\n            c = Counter(toks)\n            for tok, tf in c.items():\n                df[tok] += 1\n                postings[tok][doc_id] = math.log1p(tf)\n        # compute idf and impact weights\n        self.N = N\n        self.df = dict(df)\n        self.postings = {\n            tok: {\n                doc: w * math.log((N - df[tok] + 0.5) / (df[tok] + 0.5) + 1.0)\n                for doc, w in docs.items()\n            }\n            for tok, docs in postings.items()\n        }\n        with open(os.path.join(self.index_dir, \"impact.pkl\"), \"wb\") as f:\n            pickle.dump(\n                {\"df\": self.df, \"N\": self.N, \"postings\": self.postings},\n                f,\n                protocol=pickle.HIGHEST_PROTOCOL,\n            )\n\n    def load(self) -&gt; None:\n        \"\"\"Load.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        with open(os.path.join(self.index_dir, \"impact.pkl\"), \"rb\") as f:\n            data = pickle.load(f)\n        self.df = data[\"df\"]\n        self.N = data[\"N\"]\n        self.postings = data[\"postings\"]\n\n    def search(self, query: str, k: int) -&gt; list[tuple[str, float]]:\n        \"\"\"Search.\n\n        Args:\n            query (str): TODO.\n            k (int): TODO.\n\n        Returns:\n            List[Tuple[str, float]]: TODO.\n        \"\"\"\n        toks = self._tokenize(query)\n        scores: dict[str, float] = defaultdict(float)\n        for t in toks:\n            posts = self.postings.get(t)\n            if not posts:\n                continue\n            for doc, w in posts.items():\n                scores[doc] += w\n        return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.__init__","title":"<code>__init__(index_dir)</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>index_dir</code> <code>str</code> <p>TODO.</p> required Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def __init__(self, index_dir: str):\n    \"\"\"Init.\n\n    Args:\n        index_dir (str): TODO.\n    \"\"\"\n    self.index_dir = index_dir\n    self.df: dict[str, int] = {}\n    self.N = 0\n    self.postings: dict[str, dict[str, float]] = {}\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.build","title":"<code>build(docs_iterable)</code>","text":"<p>Build.</p> <p>Parameters:</p> Name Type Description Default <code>docs_iterable</code> <code>Iterable[Tuple[str, Dict]]</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n    \"\"\"Build.\n\n    Args:\n        docs_iterable (Iterable[Tuple[str, Dict]]): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    os.makedirs(self.index_dir, exist_ok=True)\n    df: dict[str, int] = defaultdict(int)\n    postings: dict[str, dict[str, float]] = defaultdict(lambda: defaultdict(float))\n    N = 0\n    for doc_id, fields in docs_iterable:\n        text = \" \".join(\n            [fields.get(\"title\", \"\"), fields.get(\"section\", \"\"), fields.get(\"body\", \"\")]\n        )\n        toks = self._tokenize(text)\n        N += 1\n        c = Counter(toks)\n        for tok, tf in c.items():\n            df[tok] += 1\n            postings[tok][doc_id] = math.log1p(tf)\n    # compute idf and impact weights\n    self.N = N\n    self.df = dict(df)\n    self.postings = {\n        tok: {\n            doc: w * math.log((N - df[tok] + 0.5) / (df[tok] + 0.5) + 1.0)\n            for doc, w in docs.items()\n        }\n        for tok, docs in postings.items()\n    }\n    with open(os.path.join(self.index_dir, \"impact.pkl\"), \"wb\") as f:\n        pickle.dump(\n            {\"df\": self.df, \"N\": self.N, \"postings\": self.postings},\n            f,\n            protocol=pickle.HIGHEST_PROTOCOL,\n        )\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.load","title":"<code>load()</code>","text":"<p>Load.</p> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    with open(os.path.join(self.index_dir, \"impact.pkl\"), \"rb\") as f:\n        data = pickle.load(f)\n    self.df = data[\"df\"]\n    self.N = data[\"N\"]\n    self.postings = data[\"postings\"]\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.search","title":"<code>search(query, k)</code>","text":"<p>Search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>TODO.</p> required <code>k</code> <code>int</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, float]]</code> <p>List[Tuple[str, float]]: TODO.</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def search(self, query: str, k: int) -&gt; list[tuple[str, float]]:\n    \"\"\"Search.\n\n    Args:\n        query (str): TODO.\n        k (int): TODO.\n\n    Returns:\n        List[Tuple[str, float]]: TODO.\n    \"\"\"\n    toks = self._tokenize(query)\n    scores: dict[str, float] = defaultdict(float)\n    for t in toks:\n        posts = self.postings.get(t)\n        if not posts:\n            continue\n        for doc, w in posts.items():\n            scores[doc] += w\n    return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.SPLADEv3Encoder","title":"<code>SPLADEv3Encoder</code>","text":"<p>Spladev3encoder.</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>class SPLADEv3Encoder:\n    \"\"\"Spladev3encoder.\"\"\"\n\n    name = \"SPLADE-v3-distilbert\"\n\n    def __init__(\n        self,\n        model_id: str = \"naver/splade-v3-distilbert\",\n        device: str = \"cuda\",\n        topk: int = 256,\n        max_seq_len: int = 512,\n    ):\n        \"\"\"Init.\n\n        Args:\n            model_id (str): TODO.\n            device (str): TODO.\n            topk (int): TODO.\n            max_seq_len (int): TODO.\n        \"\"\"\n        self.model_id = model_id\n        self.device = device\n        self.topk = topk\n        self.max_seq_len = max_seq_len\n\n    def encode(self, texts: list[str]) -&gt; list[tuple[list[int], list[float]]]:\n        \"\"\"Encode.\n\n        Args:\n            texts (List[str]): TODO.\n\n        Returns:\n            List[Tuple[list[int], list[float]]]: TODO.\n        \"\"\"\n        # Placeholder in this skeleton. Real impl would run the HF model and build\n        # top-k (token_id, weight) pairs from the encoder output.\n        raise NotImplementedError(\n            \"SPLADE encoding is not implemented in the skeleton. Use the Lucene \"\n            \"impact index variant if available.\"\n        )\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.SPLADEv3Encoder.__init__","title":"<code>__init__(model_id='naver/splade-v3-distilbert', device='cuda', topk=256, max_seq_len=512)</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>TODO.</p> <code>'naver/splade-v3-distilbert'</code> <code>device</code> <code>str</code> <p>TODO.</p> <code>'cuda'</code> <code>topk</code> <code>int</code> <p>TODO.</p> <code>256</code> <code>max_seq_len</code> <code>int</code> <p>TODO.</p> <code>512</code> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def __init__(\n    self,\n    model_id: str = \"naver/splade-v3-distilbert\",\n    device: str = \"cuda\",\n    topk: int = 256,\n    max_seq_len: int = 512,\n):\n    \"\"\"Init.\n\n    Args:\n        model_id (str): TODO.\n        device (str): TODO.\n        topk (int): TODO.\n        max_seq_len (int): TODO.\n    \"\"\"\n    self.model_id = model_id\n    self.device = device\n    self.topk = topk\n    self.max_seq_len = max_seq_len\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.SPLADEv3Encoder.encode","title":"<code>encode(texts)</code>","text":"<p>Encode.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>list[tuple[list[int], list[float]]]</code> <p>List[Tuple[list[int], list[float]]]: TODO.</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def encode(self, texts: list[str]) -&gt; list[tuple[list[int], list[float]]]:\n    \"\"\"Encode.\n\n    Args:\n        texts (List[str]): TODO.\n\n    Returns:\n        List[Tuple[list[int], list[float]]]: TODO.\n    \"\"\"\n    # Placeholder in this skeleton. Real impl would run the HF model and build\n    # top-k (token_id, weight) pairs from the encoder output.\n    raise NotImplementedError(\n        \"SPLADE encoding is not implemented in the skeleton. Use the Lucene \"\n        \"impact index variant if available.\"\n    )\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.get_splade","title":"<code>get_splade(backend, index_dir)</code>","text":"<p>Get splade.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>TODO.</p> required <code>index_dir</code> <code>str</code> <p>TODO.</p> required Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def get_splade(backend: str, index_dir: str) -&gt; PureImpactIndex | LuceneImpactIndex:\n    \"\"\"Get splade.\n\n    Args:\n        backend (str): TODO.\n        index_dir (str): TODO.\n    \"\"\"\n    if backend == \"lucene\":\n        try:\n            return LuceneImpactIndex(index_dir)\n        except Exception:\n            pass\n    return PureImpactIndex(index_dir)\n</code></pre>"},{"location":"api/kg_builder/","title":"<code>kg_builder</code>","text":"<p>Module for kg_builder.init.</p>"},{"location":"api/kg_builder/mock_kg/","title":"<code>kg_builder.mock_kg</code>","text":"<p>Module for kg_builder.mock_kg.</p>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG","title":"<code>MockKG</code>","text":"<p>A tiny in-memory KG for demo.</p> <p>Maps chunk_id -&gt; set(concept_id) and concept adjacency.</p> Source code in <code>src/kg_builder/mock_kg.py</code> <pre><code>class MockKG:\n    \"\"\"A tiny in-memory KG for demo.\n\n    Maps chunk_id -&gt; set(concept_id) and concept adjacency.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Init.\"\"\"\n        self.chunk2concepts: dict[str, set[str]] = {}\n        self.neighbors: dict[str, set[str]] = {}\n\n    def add_mention(self, chunk_id: str, concept_id: str) -&gt; None:\n        \"\"\"Add mention.\n\n        Args:\n            chunk_id (str): TODO.\n            concept_id (str): TODO.\n        \"\"\"\n        self.chunk2concepts.setdefault(chunk_id, set()).add(concept_id)\n\n    def add_edge(self, a: str, b: str) -&gt; None:\n        \"\"\"Add edge.\n\n        Args:\n            a (str): TODO.\n            b (str): TODO.\n        \"\"\"\n        self.neighbors.setdefault(a, set()).add(b)\n        self.neighbors.setdefault(b, set()).add(a)\n\n    def linked_concepts(self, chunk_id: str) -&gt; list[str]:\n        \"\"\"Linked concepts.\n\n        Args:\n            chunk_id (str): TODO.\n\n        Returns:\n            List[str]: TODO.\n        \"\"\"\n        return sorted(self.chunk2concepts.get(chunk_id, set()))\n\n    def one_hop(self, concept_id: str) -&gt; list[str]:\n        \"\"\"One hop.\n\n        Args:\n            concept_id (str): TODO.\n\n        Returns:\n            List[str]: TODO.\n        \"\"\"\n        return sorted(self.neighbors.get(concept_id, set()))\n</code></pre>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.__init__","title":"<code>__init__()</code>","text":"<p>Init.</p> Source code in <code>src/kg_builder/mock_kg.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Init.\"\"\"\n    self.chunk2concepts: dict[str, set[str]] = {}\n    self.neighbors: dict[str, set[str]] = {}\n</code></pre>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.add_edge","title":"<code>add_edge(a, b)</code>","text":"<p>Add edge.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>str</code> <p>TODO.</p> required <code>b</code> <code>str</code> <p>TODO.</p> required Source code in <code>src/kg_builder/mock_kg.py</code> <pre><code>def add_edge(self, a: str, b: str) -&gt; None:\n    \"\"\"Add edge.\n\n    Args:\n        a (str): TODO.\n        b (str): TODO.\n    \"\"\"\n    self.neighbors.setdefault(a, set()).add(b)\n    self.neighbors.setdefault(b, set()).add(a)\n</code></pre>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.add_mention","title":"<code>add_mention(chunk_id, concept_id)</code>","text":"<p>Add mention.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_id</code> <code>str</code> <p>TODO.</p> required <code>concept_id</code> <code>str</code> <p>TODO.</p> required Source code in <code>src/kg_builder/mock_kg.py</code> <pre><code>def add_mention(self, chunk_id: str, concept_id: str) -&gt; None:\n    \"\"\"Add mention.\n\n    Args:\n        chunk_id (str): TODO.\n        concept_id (str): TODO.\n    \"\"\"\n    self.chunk2concepts.setdefault(chunk_id, set()).add(concept_id)\n</code></pre>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.linked_concepts","title":"<code>linked_concepts(chunk_id)</code>","text":"<p>Linked concepts.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_id</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: TODO.</p> Source code in <code>src/kg_builder/mock_kg.py</code> <pre><code>def linked_concepts(self, chunk_id: str) -&gt; list[str]:\n    \"\"\"Linked concepts.\n\n    Args:\n        chunk_id (str): TODO.\n\n    Returns:\n        List[str]: TODO.\n    \"\"\"\n    return sorted(self.chunk2concepts.get(chunk_id, set()))\n</code></pre>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.one_hop","title":"<code>one_hop(concept_id)</code>","text":"<p>One hop.</p> <p>Parameters:</p> Name Type Description Default <code>concept_id</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: TODO.</p> Source code in <code>src/kg_builder/mock_kg.py</code> <pre><code>def one_hop(self, concept_id: str) -&gt; list[str]:\n    \"\"\"One hop.\n\n    Args:\n        concept_id (str): TODO.\n\n    Returns:\n        List[str]: TODO.\n    \"\"\"\n    return sorted(self.neighbors.get(concept_id, set()))\n</code></pre>"},{"location":"api/kg_builder/neo4j_store/","title":"<code>kg_builder.neo4j_store</code>","text":"<p>Module for kg_builder.neo4j_store.</p>"},{"location":"api/kg_builder/neo4j_store/#kg_builder.neo4j_store.Neo4jStore","title":"<code>Neo4jStore</code>","text":"<p>Placeholder interface for a Neo4j-backed store.</p> Source code in <code>src/kg_builder/neo4j_store.py</code> <pre><code>class Neo4jStore:\n    \"\"\"Placeholder interface for a Neo4j-backed store.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgforge_common/","title":"<code>kgforge_common</code>","text":"<p>Module for kgforge_common.init.</p>"},{"location":"api/kgforge_common/config/","title":"<code>kgforge_common.config</code>","text":"<p>Module for kgforge_common.config.</p>"},{"location":"api/kgforge_common/config/#kgforge_common.config.load_config","title":"<code>load_config(path)</code>","text":"<p>Load config.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: TODO.</p> Source code in <code>src/kgforge_common/config.py</code> <pre><code>def load_config(path: str) -&gt; dict[str, Any]:\n    \"\"\"Load config.\n\n    Args:\n        path (str): TODO.\n\n    Returns:\n        Dict[str, Any]: TODO.\n    \"\"\"\n    with open(path, encoding=\"utf-8\") as f:\n        return yaml.safe_load(f)\n</code></pre>"},{"location":"api/kgforge_common/errors/","title":"<code>kgforge_common.errors</code>","text":"<p>Module for kgforge_common.errors.</p>"},{"location":"api/kgforge_common/errors/#kgforge_common.errors.ChunkingError","title":"<code>ChunkingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when chunk generation fails.</p> Source code in <code>src/kgforge_common/errors.py</code> <pre><code>class ChunkingError(Exception):\n    \"\"\"Raised when chunk generation fails.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgforge_common/errors/#kgforge_common.errors.DoclingError","title":"<code>DoclingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base Docling processing error.</p> Source code in <code>src/kgforge_common/errors.py</code> <pre><code>class DoclingError(Exception):\n    \"\"\"Base Docling processing error.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgforge_common/errors/#kgforge_common.errors.DownloadError","title":"<code>DownloadError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an external download fails.</p> Source code in <code>src/kgforge_common/errors.py</code> <pre><code>class DownloadError(Exception):\n    \"\"\"Raised when an external download fails.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgforge_common/errors/#kgforge_common.errors.EmbeddingError","title":"<code>EmbeddingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when embedding generation fails.</p> Source code in <code>src/kgforge_common/errors.py</code> <pre><code>class EmbeddingError(Exception):\n    \"\"\"Raised when embedding generation fails.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgforge_common/errors/#kgforge_common.errors.IndexBuildError","title":"<code>IndexBuildError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when index construction fails.</p> Source code in <code>src/kgforge_common/errors.py</code> <pre><code>class IndexBuildError(Exception):\n    \"\"\"Raised when index construction fails.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgforge_common/errors/#kgforge_common.errors.LinkerCalibrationError","title":"<code>LinkerCalibrationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when linker calibration cannot be performed.</p> Source code in <code>src/kgforge_common/errors.py</code> <pre><code>class LinkerCalibrationError(Exception):\n    \"\"\"Raised when linker calibration cannot be performed.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgforge_common/errors/#kgforge_common.errors.Neo4jError","title":"<code>Neo4jError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when Neo4j operations fail.</p> Source code in <code>src/kgforge_common/errors.py</code> <pre><code>class Neo4jError(Exception):\n    \"\"\"Raised when Neo4j operations fail.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgforge_common/errors/#kgforge_common.errors.OCRTimeout","title":"<code>OCRTimeout</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when OCR operations exceed time limits.</p> Source code in <code>src/kgforge_common/errors.py</code> <pre><code>class OCRTimeout(Exception):\n    \"\"\"Raised when OCR operations exceed time limits.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgforge_common/errors/#kgforge_common.errors.OntologyParseError","title":"<code>OntologyParseError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when ontology parsing fails.</p> Source code in <code>src/kgforge_common/errors.py</code> <pre><code>class OntologyParseError(Exception):\n    \"\"\"Raised when ontology parsing fails.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgforge_common/errors/#kgforge_common.errors.SpladeOOM","title":"<code>SpladeOOM</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when SPLADE runs out of memory.</p> Source code in <code>src/kgforge_common/errors.py</code> <pre><code>class SpladeOOM(Exception):\n    \"\"\"Raised when SPLADE runs out of memory.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgforge_common/errors/#kgforge_common.errors.UnsupportedMIMEError","title":"<code>UnsupportedMIMEError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an unsupported MIME type is encountered.</p> Source code in <code>src/kgforge_common/errors.py</code> <pre><code>class UnsupportedMIMEError(Exception):\n    \"\"\"Raised when an unsupported MIME type is encountered.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgforge_common/exceptions/","title":"<code>kgforge_common.exceptions</code>","text":"<p>Module for kgforge_common.exceptions.</p>"},{"location":"api/kgforge_common/exceptions/#kgforge_common.exceptions.DownloadError","title":"<code>DownloadError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an external download fails.</p> Source code in <code>src/kgforge_common/exceptions.py</code> <pre><code>class DownloadError(Exception):\n    \"\"\"Raised when an external download fails.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgforge_common/exceptions/#kgforge_common.exceptions.UnsupportedMIMEError","title":"<code>UnsupportedMIMEError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a MIME type is unsupported.</p> Source code in <code>src/kgforge_common/exceptions.py</code> <pre><code>class UnsupportedMIMEError(Exception):\n    \"\"\"Raised when a MIME type is unsupported.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgforge_common/ids/","title":"<code>kgforge_common.ids</code>","text":"<p>Module for kgforge_common.ids.</p>"},{"location":"api/kgforge_common/ids/#kgforge_common.ids.urn_chunk","title":"<code>urn_chunk(doc_hash, start, end)</code>","text":"<p>Urn chunk.</p> <p>Parameters:</p> Name Type Description Default <code>doc_hash</code> <code>str</code> <p>TODO.</p> required <code>start</code> <code>int</code> <p>TODO.</p> required <code>end</code> <code>int</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>TODO.</p> Source code in <code>src/kgforge_common/ids.py</code> <pre><code>def urn_chunk(doc_hash: str, start: int, end: int) -&gt; str:\n    \"\"\"Urn chunk.\n\n    Args:\n        doc_hash (str): TODO.\n        start (int): TODO.\n        end (int): TODO.\n\n    Returns:\n        str: TODO.\n    \"\"\"\n    return f\"urn:chunk:{doc_hash.split(':')[-1]}:{start}-{end}\"\n</code></pre>"},{"location":"api/kgforge_common/ids/#kgforge_common.ids.urn_doc_from_text","title":"<code>urn_doc_from_text(text)</code>","text":"<p>Urn doc from text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>TODO.</p> Source code in <code>src/kgforge_common/ids.py</code> <pre><code>def urn_doc_from_text(text: str) -&gt; str:\n    \"\"\"Urn doc from text.\n\n    Args:\n        text (str): TODO.\n\n    Returns:\n        str: TODO.\n    \"\"\"\n    h = hashlib.sha256(text.encode(\"utf-8\")).digest()[:16]\n    b32 = base64.b32encode(h).decode(\"ascii\").strip(\"=\").lower()\n    return f\"urn:doc:sha256:{b32}\"\n</code></pre>"},{"location":"api/kgforge_common/logging/","title":"<code>kgforge_common.logging</code>","text":"<p>Module for kgforge_common.logging.</p>"},{"location":"api/kgforge_common/logging/#kgforge_common.logging.JsonFormatter","title":"<code>JsonFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>Jsonformatter.</p> Source code in <code>src/kgforge_common/logging.py</code> <pre><code>class JsonFormatter(logging.Formatter):\n    \"\"\"Jsonformatter.\"\"\"\n\n    def format(self, record: logging.LogRecord) -&gt; str:\n        \"\"\"Format.\n\n        Args:\n            record (logging.LogRecord): TODO.\n\n        Returns:\n            str: TODO.\n        \"\"\"\n        data = {\n            \"ts\": self.formatTime(record, \"%Y-%m-%dT%H:%M:%S\"),\n            \"level\": record.levelname,\n            \"name\": record.name,\n            \"message\": record.getMessage(),\n        }\n        for k in (\"run_id\", \"doc_id\", \"chunk_id\"):\n            v = getattr(record, k, None)\n            if v:\n                data[k] = v\n        return json.dumps(data)\n</code></pre>"},{"location":"api/kgforge_common/logging/#kgforge_common.logging.JsonFormatter.format","title":"<code>format(record)</code>","text":"<p>Format.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>LogRecord</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>TODO.</p> Source code in <code>src/kgforge_common/logging.py</code> <pre><code>def format(self, record: logging.LogRecord) -&gt; str:\n    \"\"\"Format.\n\n    Args:\n        record (logging.LogRecord): TODO.\n\n    Returns:\n        str: TODO.\n    \"\"\"\n    data = {\n        \"ts\": self.formatTime(record, \"%Y-%m-%dT%H:%M:%S\"),\n        \"level\": record.levelname,\n        \"name\": record.name,\n        \"message\": record.getMessage(),\n    }\n    for k in (\"run_id\", \"doc_id\", \"chunk_id\"):\n        v = getattr(record, k, None)\n        if v:\n            data[k] = v\n    return json.dumps(data)\n</code></pre>"},{"location":"api/kgforge_common/logging/#kgforge_common.logging.setup_logging","title":"<code>setup_logging(level=logging.INFO)</code>","text":"<p>Setup logging.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>TODO.</p> <code>INFO</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/kgforge_common/logging.py</code> <pre><code>def setup_logging(level: int = logging.INFO) -&gt; None:\n    \"\"\"Setup logging.\n\n    Args:\n        level (int): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setFormatter(JsonFormatter())\n    logging.basicConfig(level=level, handlers=[handler])\n</code></pre>"},{"location":"api/kgforge_common/models/","title":"<code>kgforge_common.models</code>","text":"<p>Module for kgforge_common.models.</p>"},{"location":"api/kgforge_common/models/#kgforge_common.models.Chunk","title":"<code>Chunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Chunk.</p> Source code in <code>src/kgforge_common/models.py</code> <pre><code>class Chunk(BaseModel):\n    \"\"\"Chunk.\"\"\"\n\n    id: Id\n    doc_id: Id\n    section: str | None\n    start_char: int\n    end_char: int\n    tokens: int\n    doctags_span: dict[str, int]\n</code></pre>"},{"location":"api/kgforge_common/models/#kgforge_common.models.Doc","title":"<code>Doc</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Doc.</p> Source code in <code>src/kgforge_common/models.py</code> <pre><code>class Doc(BaseModel):\n    \"\"\"Doc.\"\"\"\n\n    id: Id\n    openalex_id: str | None = None\n    doi: str | None = None\n    arxiv_id: str | None = None\n    pmcid: str | None = None\n    title: str = \"\"\n    authors: list[str] = []\n    pub_date: str | None = None\n    license: str | None = None\n    language: str | None = \"en\"\n    pdf_uri: str = \"\"\n    source: str = \"unknown\"\n    content_hash: str | None = None\n</code></pre>"},{"location":"api/kgforge_common/models/#kgforge_common.models.DoctagsAsset","title":"<code>DoctagsAsset</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Doctagsasset.</p> Source code in <code>src/kgforge_common/models.py</code> <pre><code>class DoctagsAsset(BaseModel):\n    \"\"\"Doctagsasset.\"\"\"\n\n    doc_id: Id\n    doctags_uri: str\n    pages: int\n    vlm_model: str\n    vlm_revision: str\n    avg_logprob: float | None = None\n</code></pre>"},{"location":"api/kgforge_common/models/#kgforge_common.models.LinkAssertion","title":"<code>LinkAssertion</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Linkassertion.</p> Source code in <code>src/kgforge_common/models.py</code> <pre><code>class LinkAssertion(BaseModel):\n    \"\"\"Linkassertion.\"\"\"\n\n    id: Id\n    chunk_id: Id\n    concept_id: Id\n    score: float\n    decision: Literal[\"link\", \"reject\", \"uncertain\"]\n    evidence_span: str | None = None\n    features: dict[str, float] = {}\n    run_id: str\n</code></pre>"},{"location":"api/kgforge_common/parquet_io/","title":"<code>kgforge_common.parquet_io</code>","text":"<p>Module for kgforge_common.parquet_io.</p>"},{"location":"api/kgforge_common/parquet_io/#kgforge_common.parquet_io.ParquetChunkWriter","title":"<code>ParquetChunkWriter</code>","text":"<p>Parquetchunkwriter.</p> Source code in <code>src/kgforge_common/parquet_io.py</code> <pre><code>class ParquetChunkWriter:\n    \"\"\"Parquetchunkwriter.\"\"\"\n\n    @staticmethod\n    def chunk_schema() -&gt; pa.schema:\n        \"\"\"Chunk schema.\n\n        Returns:\n            pa.schema: TODO.\n        \"\"\"\n        return pa.schema(\n            [\n                pa.field(\"chunk_id\", pa.string()),\n                pa.field(\"doc_id\", pa.string()),\n                pa.field(\"section\", pa.string()),\n                pa.field(\"start_char\", pa.int32()),\n                pa.field(\"end_char\", pa.int32()),\n                pa.field(\n                    \"doctags_span\",\n                    pa.struct(\n                        [\n                            pa.field(\"node_id\", pa.string()),\n                            pa.field(\"start\", pa.int32()),\n                            pa.field(\"end\", pa.int32()),\n                        ]\n                    ),\n                ),\n                pa.field(\"text\", pa.string()),\n                pa.field(\"tokens\", pa.int32()),\n                pa.field(\"created_at\", pa.timestamp(\"ms\")),\n            ]\n        )\n\n    def __init__(self, root: str, model: str = \"docling_hybrid\", run_id: str = \"dev\"):\n        \"\"\"Init.\n\n        Args:\n            root (str): TODO.\n            model (str): TODO.\n            run_id (str): TODO.\n        \"\"\"\n        self.root = Path(root) / f\"model={model}\" / f\"run_id={run_id}\" / \"shard=00000\"\n        self.root.mkdir(parents=True, exist_ok=True)\n\n    def write(self, rows: Iterable[dict[str, Any]]) -&gt; str:\n        \"\"\"Write.\n\n        Args:\n            rows (Iterable[Dict[str, Any]]): TODO.\n\n        Returns:\n            str: TODO.\n        \"\"\"\n        table = pa.Table.from_pylist(list(rows), schema=self.chunk_schema())\n        pq.write_table(\n            table,\n            self.root / \"part-00000.parquet\",\n            compression=\"ZSTD\",\n            compression_level=ZSTD_LEVEL,\n            data_page_size=ROW_GROUP_SIZE,\n        )\n        return str(self.root.parent.parent)\n</code></pre>"},{"location":"api/kgforge_common/parquet_io/#kgforge_common.parquet_io.ParquetChunkWriter.__init__","title":"<code>__init__(root, model='docling_hybrid', run_id='dev')</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>TODO.</p> required <code>model</code> <code>str</code> <p>TODO.</p> <code>'docling_hybrid'</code> <code>run_id</code> <code>str</code> <p>TODO.</p> <code>'dev'</code> Source code in <code>src/kgforge_common/parquet_io.py</code> <pre><code>def __init__(self, root: str, model: str = \"docling_hybrid\", run_id: str = \"dev\"):\n    \"\"\"Init.\n\n    Args:\n        root (str): TODO.\n        model (str): TODO.\n        run_id (str): TODO.\n    \"\"\"\n    self.root = Path(root) / f\"model={model}\" / f\"run_id={run_id}\" / \"shard=00000\"\n    self.root.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/kgforge_common/parquet_io/#kgforge_common.parquet_io.ParquetChunkWriter.chunk_schema","title":"<code>chunk_schema()</code>  <code>staticmethod</code>","text":"<p>Chunk schema.</p> <p>Returns:</p> Type Description <code>schema</code> <p>pa.schema: TODO.</p> Source code in <code>src/kgforge_common/parquet_io.py</code> <pre><code>@staticmethod\ndef chunk_schema() -&gt; pa.schema:\n    \"\"\"Chunk schema.\n\n    Returns:\n        pa.schema: TODO.\n    \"\"\"\n    return pa.schema(\n        [\n            pa.field(\"chunk_id\", pa.string()),\n            pa.field(\"doc_id\", pa.string()),\n            pa.field(\"section\", pa.string()),\n            pa.field(\"start_char\", pa.int32()),\n            pa.field(\"end_char\", pa.int32()),\n            pa.field(\n                \"doctags_span\",\n                pa.struct(\n                    [\n                        pa.field(\"node_id\", pa.string()),\n                        pa.field(\"start\", pa.int32()),\n                        pa.field(\"end\", pa.int32()),\n                    ]\n                ),\n            ),\n            pa.field(\"text\", pa.string()),\n            pa.field(\"tokens\", pa.int32()),\n            pa.field(\"created_at\", pa.timestamp(\"ms\")),\n        ]\n    )\n</code></pre>"},{"location":"api/kgforge_common/parquet_io/#kgforge_common.parquet_io.ParquetChunkWriter.write","title":"<code>write(rows)</code>","text":"<p>Write.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>Iterable[Dict[str, Any]]</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>TODO.</p> Source code in <code>src/kgforge_common/parquet_io.py</code> <pre><code>def write(self, rows: Iterable[dict[str, Any]]) -&gt; str:\n    \"\"\"Write.\n\n    Args:\n        rows (Iterable[Dict[str, Any]]): TODO.\n\n    Returns:\n        str: TODO.\n    \"\"\"\n    table = pa.Table.from_pylist(list(rows), schema=self.chunk_schema())\n    pq.write_table(\n        table,\n        self.root / \"part-00000.parquet\",\n        compression=\"ZSTD\",\n        compression_level=ZSTD_LEVEL,\n        data_page_size=ROW_GROUP_SIZE,\n    )\n    return str(self.root.parent.parent)\n</code></pre>"},{"location":"api/kgforge_common/parquet_io/#kgforge_common.parquet_io.ParquetVectorWriter","title":"<code>ParquetVectorWriter</code>","text":"<p>Parquetvectorwriter.</p> Source code in <code>src/kgforge_common/parquet_io.py</code> <pre><code>class ParquetVectorWriter:\n    \"\"\"Parquetvectorwriter.\"\"\"\n\n    @staticmethod\n    def dense_schema(dim: int) -&gt; pa.schema:\n        \"\"\"Dense schema.\n\n        Args:\n            dim (int): TODO.\n\n        Returns:\n            pa.schema: TODO.\n        \"\"\"\n        return pa.schema(\n            [\n                pa.field(\"chunk_id\", pa.string()),\n                pa.field(\"model\", pa.string()),\n                pa.field(\"run_id\", pa.string()),\n                pa.field(\"dim\", pa.int16()),\n                pa.field(\"vector\", pa.list_(pa.float32(), list_size=dim)),\n                pa.field(\"l2_norm\", pa.float32()),\n                pa.field(\"created_at\", pa.timestamp(\"ms\")),\n            ]\n        )\n\n    def __init__(self, root: str):\n        \"\"\"Init.\n\n        Args:\n            root (str): TODO.\n        \"\"\"\n        self.root = Path(root)\n\n    def write_dense(\n        self,\n        model: str,\n        run_id: str,\n        dim: int,\n        records: Iterable[tuple[str, list[float], float]],\n        shard: int = 0,\n    ) -&gt; str:\n        \"\"\"Write dense.\n\n        Args:\n            model (str): TODO.\n            run_id (str): TODO.\n            dim (int): TODO.\n            records (Iterable[Tuple[str, List[float], float]]): TODO.\n            shard (int): TODO.\n\n        Returns:\n            str: TODO.\n        \"\"\"\n        part_dir = self.root / f\"model={model}\" / f\"run_id={run_id}\" / f\"shard={shard:05d}\"\n        part_dir.mkdir(parents=True, exist_ok=True)\n        now = int(dt.datetime.now(dt.UTC).timestamp() * 1000)\n        rows = [\n            {\n                \"chunk_id\": cid,\n                \"model\": model,\n                \"run_id\": run_id,\n                \"dim\": dim,\n                \"vector\": vec,\n                \"l2_norm\": float(l2),\n                \"created_at\": now,\n            }\n            for cid, vec, l2 in records\n        ]\n        table = pa.Table.from_pylist(rows, schema=self.dense_schema(dim))\n        pq.write_table(\n            table,\n            part_dir / \"part-00000.parquet\",\n            compression=\"ZSTD\",\n            compression_level=ZSTD_LEVEL,\n            data_page_size=ROW_GROUP_SIZE,\n        )\n        return str(self.root)\n\n    @staticmethod\n    def splade_schema() -&gt; pa.schema:\n        \"\"\"Splade schema.\n\n        Returns:\n            pa.schema: TODO.\n        \"\"\"\n        return pa.schema(\n            [\n                pa.field(\"chunk_id\", pa.string()),\n                pa.field(\"model\", pa.string()),\n                pa.field(\"run_id\", pa.string()),\n                pa.field(\"vocab_ids\", pa.list_(pa.int32())),\n                pa.field(\"weights\", pa.list_(pa.float32())),\n                pa.field(\"nnz\", pa.int16()),\n                pa.field(\"created_at\", pa.timestamp(\"ms\")),\n            ]\n        )\n\n    def write_splade(\n        self,\n        model: str,\n        run_id: str,\n        records: Iterable[tuple[str, list[int], list[float]]],\n        shard: int = 0,\n    ) -&gt; str:\n        \"\"\"Write splade.\n\n        Args:\n            model (str): TODO.\n            run_id (str): TODO.\n            records (Iterable[Tuple[str, List[int], List[float]]]): TODO.\n            shard (int): TODO.\n\n        Returns:\n            str: TODO.\n        \"\"\"\n        part_dir = self.root / f\"model={model}\" / f\"run_id={run_id}\" / f\"shard={shard:05d}\"\n        part_dir.mkdir(parents=True, exist_ok=True)\n        now = int(dt.datetime.now(dt.UTC).timestamp() * 1000)\n        rows = [\n            {\n                \"chunk_id\": cid,\n                \"model\": model,\n                \"run_id\": run_id,\n                \"vocab_ids\": ids,\n                \"weights\": wts,\n                \"nnz\": len(ids),\n                \"created_at\": now,\n            }\n            for cid, ids, wts in records\n        ]\n        table = pa.Table.from_pylist(rows, schema=self.splade_schema())\n        pq.write_table(\n            table,\n            part_dir / \"part-00000.parquet\",\n            compression=\"ZSTD\",\n            compression_level=ZSTD_LEVEL,\n            data_page_size=ROW_GROUP_SIZE,\n        )\n        return str(self.root)\n</code></pre>"},{"location":"api/kgforge_common/parquet_io/#kgforge_common.parquet_io.ParquetVectorWriter.__init__","title":"<code>__init__(root)</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>TODO.</p> required Source code in <code>src/kgforge_common/parquet_io.py</code> <pre><code>def __init__(self, root: str):\n    \"\"\"Init.\n\n    Args:\n        root (str): TODO.\n    \"\"\"\n    self.root = Path(root)\n</code></pre>"},{"location":"api/kgforge_common/parquet_io/#kgforge_common.parquet_io.ParquetVectorWriter.dense_schema","title":"<code>dense_schema(dim)</code>  <code>staticmethod</code>","text":"<p>Dense schema.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>schema</code> <p>pa.schema: TODO.</p> Source code in <code>src/kgforge_common/parquet_io.py</code> <pre><code>@staticmethod\ndef dense_schema(dim: int) -&gt; pa.schema:\n    \"\"\"Dense schema.\n\n    Args:\n        dim (int): TODO.\n\n    Returns:\n        pa.schema: TODO.\n    \"\"\"\n    return pa.schema(\n        [\n            pa.field(\"chunk_id\", pa.string()),\n            pa.field(\"model\", pa.string()),\n            pa.field(\"run_id\", pa.string()),\n            pa.field(\"dim\", pa.int16()),\n            pa.field(\"vector\", pa.list_(pa.float32(), list_size=dim)),\n            pa.field(\"l2_norm\", pa.float32()),\n            pa.field(\"created_at\", pa.timestamp(\"ms\")),\n        ]\n    )\n</code></pre>"},{"location":"api/kgforge_common/parquet_io/#kgforge_common.parquet_io.ParquetVectorWriter.splade_schema","title":"<code>splade_schema()</code>  <code>staticmethod</code>","text":"<p>Splade schema.</p> <p>Returns:</p> Type Description <code>schema</code> <p>pa.schema: TODO.</p> Source code in <code>src/kgforge_common/parquet_io.py</code> <pre><code>@staticmethod\ndef splade_schema() -&gt; pa.schema:\n    \"\"\"Splade schema.\n\n    Returns:\n        pa.schema: TODO.\n    \"\"\"\n    return pa.schema(\n        [\n            pa.field(\"chunk_id\", pa.string()),\n            pa.field(\"model\", pa.string()),\n            pa.field(\"run_id\", pa.string()),\n            pa.field(\"vocab_ids\", pa.list_(pa.int32())),\n            pa.field(\"weights\", pa.list_(pa.float32())),\n            pa.field(\"nnz\", pa.int16()),\n            pa.field(\"created_at\", pa.timestamp(\"ms\")),\n        ]\n    )\n</code></pre>"},{"location":"api/kgforge_common/parquet_io/#kgforge_common.parquet_io.ParquetVectorWriter.write_dense","title":"<code>write_dense(model, run_id, dim, records, shard=0)</code>","text":"<p>Write dense.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>TODO.</p> required <code>run_id</code> <code>str</code> <p>TODO.</p> required <code>dim</code> <code>int</code> <p>TODO.</p> required <code>records</code> <code>Iterable[Tuple[str, List[float], float]]</code> <p>TODO.</p> required <code>shard</code> <code>int</code> <p>TODO.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>TODO.</p> Source code in <code>src/kgforge_common/parquet_io.py</code> <pre><code>def write_dense(\n    self,\n    model: str,\n    run_id: str,\n    dim: int,\n    records: Iterable[tuple[str, list[float], float]],\n    shard: int = 0,\n) -&gt; str:\n    \"\"\"Write dense.\n\n    Args:\n        model (str): TODO.\n        run_id (str): TODO.\n        dim (int): TODO.\n        records (Iterable[Tuple[str, List[float], float]]): TODO.\n        shard (int): TODO.\n\n    Returns:\n        str: TODO.\n    \"\"\"\n    part_dir = self.root / f\"model={model}\" / f\"run_id={run_id}\" / f\"shard={shard:05d}\"\n    part_dir.mkdir(parents=True, exist_ok=True)\n    now = int(dt.datetime.now(dt.UTC).timestamp() * 1000)\n    rows = [\n        {\n            \"chunk_id\": cid,\n            \"model\": model,\n            \"run_id\": run_id,\n            \"dim\": dim,\n            \"vector\": vec,\n            \"l2_norm\": float(l2),\n            \"created_at\": now,\n        }\n        for cid, vec, l2 in records\n    ]\n    table = pa.Table.from_pylist(rows, schema=self.dense_schema(dim))\n    pq.write_table(\n        table,\n        part_dir / \"part-00000.parquet\",\n        compression=\"ZSTD\",\n        compression_level=ZSTD_LEVEL,\n        data_page_size=ROW_GROUP_SIZE,\n    )\n    return str(self.root)\n</code></pre>"},{"location":"api/kgforge_common/parquet_io/#kgforge_common.parquet_io.ParquetVectorWriter.write_splade","title":"<code>write_splade(model, run_id, records, shard=0)</code>","text":"<p>Write splade.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>TODO.</p> required <code>run_id</code> <code>str</code> <p>TODO.</p> required <code>records</code> <code>Iterable[Tuple[str, List[int], List[float]]]</code> <p>TODO.</p> required <code>shard</code> <code>int</code> <p>TODO.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>TODO.</p> Source code in <code>src/kgforge_common/parquet_io.py</code> <pre><code>def write_splade(\n    self,\n    model: str,\n    run_id: str,\n    records: Iterable[tuple[str, list[int], list[float]]],\n    shard: int = 0,\n) -&gt; str:\n    \"\"\"Write splade.\n\n    Args:\n        model (str): TODO.\n        run_id (str): TODO.\n        records (Iterable[Tuple[str, List[int], List[float]]]): TODO.\n        shard (int): TODO.\n\n    Returns:\n        str: TODO.\n    \"\"\"\n    part_dir = self.root / f\"model={model}\" / f\"run_id={run_id}\" / f\"shard={shard:05d}\"\n    part_dir.mkdir(parents=True, exist_ok=True)\n    now = int(dt.datetime.now(dt.UTC).timestamp() * 1000)\n    rows = [\n        {\n            \"chunk_id\": cid,\n            \"model\": model,\n            \"run_id\": run_id,\n            \"vocab_ids\": ids,\n            \"weights\": wts,\n            \"nnz\": len(ids),\n            \"created_at\": now,\n        }\n        for cid, ids, wts in records\n    ]\n    table = pa.Table.from_pylist(rows, schema=self.splade_schema())\n    pq.write_table(\n        table,\n        part_dir / \"part-00000.parquet\",\n        compression=\"ZSTD\",\n        compression_level=ZSTD_LEVEL,\n        data_page_size=ROW_GROUP_SIZE,\n    )\n    return str(self.root)\n</code></pre>"},{"location":"api/linking/","title":"<code>linking</code>","text":"<p>Module for linking.init.</p>"},{"location":"api/linking/calibration/","title":"<code>linking.calibration</code>","text":"<p>Module for linking.calibration.</p>"},{"location":"api/linking/calibration/#linking.calibration.isotonic_calibrate","title":"<code>isotonic_calibrate(pairs)</code>","text":"<p>Skeleton: calibrate scores (y=float in [0,1]) vs labels (0/1).</p> Source code in <code>src/linking/calibration.py</code> <pre><code>def isotonic_calibrate(pairs: list[tuple[float, int]]) -&gt; dict[str, object]:\n    \"\"\"Skeleton: calibrate scores (y=float in [0,1]) vs labels (0/1).\"\"\"\n    # TODO: fit isotonic regression parameters\n    return {\"kind\": \"isotonic\", \"params\": []}\n</code></pre>"},{"location":"api/linking/linker/","title":"<code>linking.linker</code>","text":"<p>Module for linking.linker.</p>"},{"location":"api/linking/linker/#linking.linker.Linker","title":"<code>Linker</code>","text":"<p>Abstract linker contract implemented by production pipelines.</p> Source code in <code>src/linking/linker.py</code> <pre><code>class Linker:\n    \"\"\"Abstract linker contract implemented by production pipelines.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/observability/","title":"<code>observability</code>","text":"<p>Module for observability.init.</p>"},{"location":"api/observability/metrics/","title":"<code>observability.metrics</code>","text":"<p>Module for observability.metrics.</p>"},{"location":"api/ontology/","title":"<code>ontology</code>","text":"<p>Module for ontology.init.</p>"},{"location":"api/ontology/catalog/","title":"<code>ontology.catalog</code>","text":"<p>Module for ontology.catalog.</p>"},{"location":"api/ontology/catalog/#ontology.catalog.OntologyCatalog","title":"<code>OntologyCatalog</code>","text":"<p>Ontologycatalog.</p> Source code in <code>src/ontology/catalog.py</code> <pre><code>class OntologyCatalog:\n    \"\"\"Ontologycatalog.\"\"\"\n\n    def __init__(self, concepts: list[Concept]):\n        \"\"\"Init.\n\n        Args:\n            concepts (List[Concept]): TODO.\n        \"\"\"\n        self.by_id = {c.id: c for c in concepts}\n\n    def neighbors(self, concept_id: str, depth: int = 1) -&gt; list[str]:\n        \"\"\"Neighbors.\n\n        Args:\n            concept_id (str): TODO.\n            depth (int): TODO.\n\n        Returns:\n            List[str]: TODO.\n        \"\"\"\n        # TODO: return neighbor concept IDs up to depth.\n        return []\n</code></pre>"},{"location":"api/ontology/catalog/#ontology.catalog.OntologyCatalog.__init__","title":"<code>__init__(concepts)</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>concepts</code> <code>List[Concept]</code> <p>TODO.</p> required Source code in <code>src/ontology/catalog.py</code> <pre><code>def __init__(self, concepts: list[Concept]):\n    \"\"\"Init.\n\n    Args:\n        concepts (List[Concept]): TODO.\n    \"\"\"\n    self.by_id = {c.id: c for c in concepts}\n</code></pre>"},{"location":"api/ontology/catalog/#ontology.catalog.OntologyCatalog.neighbors","title":"<code>neighbors(concept_id, depth=1)</code>","text":"<p>Neighbors.</p> <p>Parameters:</p> Name Type Description Default <code>concept_id</code> <code>str</code> <p>TODO.</p> required <code>depth</code> <code>int</code> <p>TODO.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: TODO.</p> Source code in <code>src/ontology/catalog.py</code> <pre><code>def neighbors(self, concept_id: str, depth: int = 1) -&gt; list[str]:\n    \"\"\"Neighbors.\n\n    Args:\n        concept_id (str): TODO.\n        depth (int): TODO.\n\n    Returns:\n        List[str]: TODO.\n    \"\"\"\n    # TODO: return neighbor concept IDs up to depth.\n    return []\n</code></pre>"},{"location":"api/ontology/loader/","title":"<code>ontology.loader</code>","text":"<p>Module for ontology.loader.</p>"},{"location":"api/ontology/loader/#ontology.loader.OntologyLoader","title":"<code>OntologyLoader</code>","text":"<p>Placeholder loader for ontology metadata.</p> Source code in <code>src/ontology/loader.py</code> <pre><code>class OntologyLoader:\n    \"\"\"Placeholder loader for ontology metadata.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/orchestration/","title":"<code>orchestration</code>","text":"<p>Module for orchestration.init.</p>"},{"location":"api/orchestration/cli/","title":"<code>orchestration.cli</code>","text":"<p>Module for orchestration.cli.</p>"},{"location":"api/orchestration/cli/#orchestration.cli.api","title":"<code>api(port=8080)</code>","text":"<p>Run the FastAPI app.</p> Source code in <code>src/orchestration/cli.py</code> <pre><code>@app.command()\ndef api(port: int = 8080) -&gt; None:\n    \"\"\"Run the FastAPI app.\"\"\"\n    import uvicorn\n\n    uvicorn.run(\"search_api.app:app\", host=\"0.0.0.0\", port=port, reload=False)\n</code></pre>"},{"location":"api/orchestration/cli/#orchestration.cli.index_bm25","title":"<code>index_bm25(chunks_parquet=typer.Argument(..., help='Path to Parquet/JSONL with chunks'), backend=typer.Option('lucene', help='lucene|pure'), index_dir=typer.Option('./_indices/bm25', help='Output index directory'))</code>","text":"<p>Build a BM25 index from chunk fixtures (id, title, section, body).</p> Source code in <code>src/orchestration/cli.py</code> <pre><code>@app.command()\ndef index_bm25(\n    chunks_parquet: str = typer.Argument(..., help=\"Path to Parquet/JSONL with chunks\"),\n    backend: str = typer.Option(\"lucene\", help=\"lucene|pure\"),\n    index_dir: str = typer.Option(\"./_indices/bm25\", help=\"Output index directory\"),\n) -&gt; None:\n    \"\"\"Build a BM25 index from chunk fixtures (id, title, section, body).\"\"\"\n    os.makedirs(index_dir, exist_ok=True)\n    # Very small loader that supports JSONL in this skeleton (Parquet in real pipeline).\n    docs: list[tuple[str, dict[str, str]]] = []\n    if chunks_parquet.endswith(\".jsonl\"):\n        with open(chunks_parquet, encoding=\"utf-8\") as fh:\n            for line in fh:\n                rec = json.loads(line)\n                docs.append(\n                    (\n                        rec[\"chunk_id\"],\n                        {\n                            \"title\": rec.get(\"title\", \"\"),\n                            \"section\": rec.get(\"section\", \"\"),\n                            \"body\": rec.get(\"text\", \"\"),\n                        },\n                    )\n                )\n    else:\n        # naive: expect a JSON file with list under skeleton; replace with Parquet\n        # reader in implementation\n        with open(chunks_parquet, encoding=\"utf-8\") as fh:\n            data = json.load(fh)\n        for rec in data:\n            docs.append(\n                (\n                    rec[\"chunk_id\"],\n                    {\n                        \"title\": rec.get(\"title\", \"\"),\n                        \"section\": rec.get(\"section\", \"\"),\n                        \"body\": rec.get(\"text\", \"\"),\n                    },\n                )\n            )\n    idx = get_bm25(backend, index_dir, k1=0.9, b=0.4)\n    idx.build(docs)\n    typer.echo(f\"BM25 index built at {index_dir} using backend={backend} ({type(idx).__name__})\")\n</code></pre>"},{"location":"api/orchestration/cli/#orchestration.cli.index_faiss","title":"<code>index_faiss(dense_vectors=typer.Argument(..., help='Path to dense vectors JSON (skeleton)'), index_path=typer.Option('./_indices/faiss/shard_000.idx', help='Output index (CPU .idx)'))</code>","text":"<p>Train &amp; build FAISS index from fixture dense vectors.</p> <p>In this skeleton we accept a JSON with entries: {key: str, vector: List[float]}.</p> Source code in <code>src/orchestration/cli.py</code> <pre><code>@app.command()\ndef index_faiss(\n    dense_vectors: str = typer.Argument(..., help=\"Path to dense vectors JSON (skeleton)\"),\n    index_path: str = typer.Option(\n        \"./_indices/faiss/shard_000.idx\", help=\"Output index (CPU .idx)\"\n    ),\n) -&gt; None:\n    \"\"\"Train &amp; build FAISS index from fixture dense vectors.\n\n    In this skeleton we accept a JSON with entries: {key: str, vector: List[float]}.\n    \"\"\"\n    os.makedirs(os.path.dirname(index_path), exist_ok=True)\n    with open(dense_vectors, encoding=\"utf-8\") as fh:\n        vecs = json.load(fh)\n    keys = [r[\"key\"] for r in vecs]\n    X = np.array([r[\"vector\"] for r in vecs], dtype=\"float32\")\n    # Train and add\n    vs = FaissGpuIndex()\n    vs.train(X[: min(len(X), 10000)])  # small train set\n    vs.add(keys, X)\n    # Save CPU form when possible\n    try:\n        vs.save(index_path, None)\n        typer.echo(f\"FAISS index saved to {index_path}\")\n    except Exception as e:\n        typer.echo(f\"Saved fallback matrix (npz) due to {e!r}\")\n        vs.save(index_path, None)\n</code></pre>"},{"location":"api/orchestration/fixture_flow/","title":"<code>orchestration.fixture_flow</code>","text":"<p>Module for orchestration.fixture_flow.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.fixture_pipeline","title":"<code>fixture_pipeline(root='/data', db_path='/data/catalog/catalog.duckdb')</code>","text":"<p>Fixture pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>TODO.</p> <code>'/data'</code> <code>db_path</code> <code>str</code> <p>TODO.</p> <code>'/data/catalog/catalog.duckdb'</code> Source code in <code>src/orchestration/fixture_flow.py</code> <pre><code>@flow(name=\"kgforge_fixture_pipeline\")\ndef fixture_pipeline(\n    root: str = \"/data\", db_path: str = \"/data/catalog/catalog.duckdb\"\n) -&gt; dict[str, list[str]]:\n    \"\"\"Fixture pipeline.\n\n    Args:\n        root (str): TODO.\n        db_path (str): TODO.\n    \"\"\"\n    t_prepare_dirs(root)\n    chunks_info = t_write_fixture_chunks(f\"{root}/parquet/chunks\")\n    dense_info = t_write_fixture_dense(f\"{root}/parquet/dense\")\n    sparse_info = t_write_fixture_splade(f\"{root}/parquet/sparse\")\n    summary = t_register_in_duckdb(db_path, chunks_info, dense_info, sparse_info)\n    return summary\n</code></pre>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_prepare_dirs","title":"<code>t_prepare_dirs(root)</code>","text":"<p>T prepare dirs.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, bool]</code> <p>TODO.</p> Source code in <code>src/orchestration/fixture_flow.py</code> <pre><code>@task\ndef t_prepare_dirs(root: str) -&gt; dict[str, bool]:\n    \"\"\"T prepare dirs.\n\n    Args:\n        root (str): TODO.\n\n    Returns:\n        dict: TODO.\n    \"\"\"\n    p = Path(root)\n    (p / \"parquet\" / \"dense\").mkdir(parents=True, exist_ok=True)\n    (p / \"parquet\" / \"sparse\").mkdir(parents=True, exist_ok=True)\n    (p / \"parquet\" / \"chunks\").mkdir(parents=True, exist_ok=True)\n    (p / \"catalog\").mkdir(parents=True, exist_ok=True)\n    return {\"ok\": True}\n</code></pre>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_register_in_duckdb","title":"<code>t_register_in_duckdb(db_path, chunks_info, dense_info, sparse_info)</code>","text":"<p>T register in duckdb.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>TODO.</p> required <code>chunks_info</code> <code>tuple[str, int]</code> <p>TODO.</p> required <code>dense_info</code> <code>tuple[str, int]</code> <p>TODO.</p> required <code>sparse_info</code> <code>tuple[str, int]</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, list[str]]</code> <p>TODO.</p> Source code in <code>src/orchestration/fixture_flow.py</code> <pre><code>@task\ndef t_register_in_duckdb(\n    db_path: str,\n    chunks_info: tuple[str, int],\n    dense_info: tuple[str, int],\n    sparse_info: tuple[str, int],\n) -&gt; dict[str, list[str]]:\n    \"\"\"T register in duckdb.\n\n    Args:\n        db_path (str): TODO.\n        chunks_info: TODO.\n        dense_info: TODO.\n        sparse_info: TODO.\n\n    Returns:\n        dict: TODO.\n    \"\"\"\n    reg = DuckDBRegistryHelper(db_path)\n    dense_run = reg.new_run(\"dense_embed\", \"Qwen3-Embedding-4B\", \"main\", {\"dim\": 2560})\n    sparse_run = reg.new_run(\"splade_encode\", \"SPLADE-v3-distilbert\", \"main\", {\"topk\": 256})\n    ds_chunks = reg.begin_dataset(\"chunks\", dense_run)\n    reg.commit_dataset(ds_chunks, chunks_info[0], rows=chunks_info[1])\n    ds_dense = reg.begin_dataset(\"dense\", dense_run)\n    reg.commit_dataset(ds_dense, dense_info[0], rows=dense_info[1])\n    ds_sparse = reg.begin_dataset(\"sparse\", sparse_run)\n    reg.commit_dataset(ds_sparse, sparse_info[0], rows=sparse_info[1])\n    reg.register_documents(\n        [\n            Doc(\n                id=\"urn:doc:fixture:0001\",\n                title=\"Fixture Doc\",\n                authors=[],\n                pdf_uri=\"/dev/null\",\n                source=\"fixture\",\n                openalex_id=None,\n                doi=None,\n                arxiv_id=None,\n                pmcid=None,\n                pub_date=None,\n                license=\"CC0\",\n                language=\"en\",\n                content_hash=\"\",\n            )\n        ]\n    )\n    reg.close_run(dense_run, True)\n    reg.close_run(sparse_run, True)\n    return {\"runs\": [dense_run, sparse_run]}\n</code></pre>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_chunks","title":"<code>t_write_fixture_chunks(chunks_root)</code>","text":"<p>T write fixture chunks.</p> <p>Parameters:</p> Name Type Description Default <code>chunks_root</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>tuple[str, int]</code> <p>tuple[str, int]: TODO.</p> Source code in <code>src/orchestration/fixture_flow.py</code> <pre><code>@task\ndef t_write_fixture_chunks(chunks_root: str) -&gt; tuple[str, int]:\n    \"\"\"T write fixture chunks.\n\n    Args:\n        chunks_root (str): TODO.\n\n    Returns:\n        tuple[str, int]: TODO.\n    \"\"\"\n    writer = ParquetChunkWriter(chunks_root, model=\"docling_hybrid\", run_id=\"fixture\")\n    rows = [\n        {\n            \"chunk_id\": \"urn:chunk:fixture:0-28\",\n            \"doc_id\": \"urn:doc:fixture:0001\",\n            \"section\": \"Intro\",\n            \"start_char\": 0,\n            \"end_char\": 28,\n            \"doctags_span\": {\"node_id\": \"n1\", \"start\": 0, \"end\": 28},\n            \"text\": \"Hello fixture text about LLMs\",\n            \"tokens\": 5,\n            \"created_at\": 0,\n        }\n    ]\n    dataset_root = writer.write(rows)\n    return dataset_root, len(rows)\n</code></pre>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_dense","title":"<code>t_write_fixture_dense(dense_root)</code>","text":"<p>T write fixture dense.</p> <p>Parameters:</p> Name Type Description Default <code>dense_root</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>tuple[str, int]</code> <p>tuple[str, int]: TODO.</p> Source code in <code>src/orchestration/fixture_flow.py</code> <pre><code>@task\ndef t_write_fixture_dense(dense_root: str) -&gt; tuple[str, int]:\n    \"\"\"T write fixture dense.\n\n    Args:\n        dense_root (str): TODO.\n\n    Returns:\n        tuple[str, int]: TODO.\n    \"\"\"\n    w = ParquetVectorWriter(dense_root)\n    vec = [0.0] * 2560\n    out_root = w.write_dense(\n        \"Qwen3-Embedding-4B\", \"fixture\", 2560, [(\"urn:chunk:fixture:0-28\", vec, 1.0)], shard=0\n    )\n    return out_root, 1\n</code></pre>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_splade","title":"<code>t_write_fixture_splade(sparse_root)</code>","text":"<p>T write fixture splade.</p> <p>Parameters:</p> Name Type Description Default <code>sparse_root</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>tuple[str, int]</code> <p>tuple[str, int]: TODO.</p> Source code in <code>src/orchestration/fixture_flow.py</code> <pre><code>@task\ndef t_write_fixture_splade(sparse_root: str) -&gt; tuple[str, int]:\n    \"\"\"T write fixture splade.\n\n    Args:\n        sparse_root (str): TODO.\n\n    Returns:\n        tuple[str, int]: TODO.\n    \"\"\"\n    w = ParquetVectorWriter(sparse_root)\n    out_root = w.write_splade(\n        \"SPLADE-v3-distilbert\",\n        \"fixture\",\n        [(\"urn:chunk:fixture:0-28\", [1, 7, 42], [0.3, 0.2, 0.1])],\n        shard=0,\n    )\n    return out_root, 1\n</code></pre>"},{"location":"api/orchestration/flows/","title":"<code>orchestration.flows</code>","text":"<p>Module for orchestration.flows.</p>"},{"location":"api/orchestration/flows/#orchestration.flows.e2e_flow","title":"<code>e2e_flow()</code>","text":"<p>E2e flow.</p> Source code in <code>src/orchestration/flows.py</code> <pre><code>@flow(name=\"kgforge_e2e_skeleton\")\ndef e2e_flow() -&gt; list[str]:\n    \"\"\"E2e flow.\"\"\"\n    return [\n        t_echo.submit(x).result()\n        for x in [\n            \"harvest\",\n            \"doctags\",\n            \"chunk\",\n            \"embed_dense\",\n            \"encode_splade\",\n            \"bm25\",\n            \"faiss\",\n            \"ontology\",\n            \"concept_embed\",\n            \"linker\",\n            \"kg\",\n        ]\n    ]\n</code></pre>"},{"location":"api/orchestration/flows/#orchestration.flows.t_echo","title":"<code>t_echo(msg)</code>","text":"<p>T echo.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>TODO.</p> Source code in <code>src/orchestration/flows.py</code> <pre><code>@task\ndef t_echo(msg: str) -&gt; str:\n    \"\"\"T echo.\n\n    Args:\n        msg (str): TODO.\n\n    Returns:\n        str: TODO.\n    \"\"\"\n    return msg\n</code></pre>"},{"location":"api/registry/","title":"<code>registry</code>","text":"<p>Module for registry.init.</p>"},{"location":"api/registry/api/","title":"<code>registry.api</code>","text":"<p>Module for registry.api.</p>"},{"location":"api/registry/api/#registry.api.Registry","title":"<code>Registry</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Registry protocol describing persistence operations.</p> Source code in <code>src/registry/api.py</code> <pre><code>class Registry(Protocol):\n    \"\"\"Registry protocol describing persistence operations.\"\"\"\n\n    def begin_dataset(self, kind: str, run_id: str) -&gt; str:\n        \"\"\"Begin tracking a dataset build for the given run.\"\"\"\n\n        ...\n\n    def commit_dataset(self, dataset_id: str, parquet_root: str, rows: int) -&gt; None:\n        \"\"\"Finalize a dataset build.\"\"\"\n\n        ...\n\n    def rollback_dataset(self, dataset_id: str) -&gt; None:\n        \"\"\"Rollback a dataset build after failure.\"\"\"\n\n        ...\n\n    def insert_run(\n        self,\n        purpose: str,\n        model_id: str | None,\n        revision: str | None,\n        config: Mapping[str, object],\n    ) -&gt; str:\n        \"\"\"Register a new processing run.\"\"\"\n\n        ...\n\n    def close_run(self, run_id: str, success: bool, notes: str | None = None) -&gt; None:\n        \"\"\"Mark a run as complete.\"\"\"\n\n        ...\n\n    def register_documents(self, docs: list[Doc]) -&gt; None:\n        \"\"\"Register document metadata with the registry.\"\"\"\n\n        ...\n\n    def register_doctags(self, assets: list[DoctagsAsset]) -&gt; None:\n        \"\"\"Register DocTags assets with the registry.\"\"\"\n\n        ...\n\n    def emit_event(self, event_name: str, subject_id: str, payload: Mapping[str, object]) -&gt; None:\n        \"\"\"Emit an audit event for monitoring.\"\"\"\n\n        ...\n\n    def incident(self, event: str, subject_id: str, error_class: str, message: str) -&gt; None:\n        \"\"\"Record an incident for visibility.\"\"\"\n\n        ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.begin_dataset","title":"<code>begin_dataset(kind, run_id)</code>","text":"<p>Begin tracking a dataset build for the given run.</p> Source code in <code>src/registry/api.py</code> <pre><code>def begin_dataset(self, kind: str, run_id: str) -&gt; str:\n    \"\"\"Begin tracking a dataset build for the given run.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.close_run","title":"<code>close_run(run_id, success, notes=None)</code>","text":"<p>Mark a run as complete.</p> Source code in <code>src/registry/api.py</code> <pre><code>def close_run(self, run_id: str, success: bool, notes: str | None = None) -&gt; None:\n    \"\"\"Mark a run as complete.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.commit_dataset","title":"<code>commit_dataset(dataset_id, parquet_root, rows)</code>","text":"<p>Finalize a dataset build.</p> Source code in <code>src/registry/api.py</code> <pre><code>def commit_dataset(self, dataset_id: str, parquet_root: str, rows: int) -&gt; None:\n    \"\"\"Finalize a dataset build.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.emit_event","title":"<code>emit_event(event_name, subject_id, payload)</code>","text":"<p>Emit an audit event for monitoring.</p> Source code in <code>src/registry/api.py</code> <pre><code>def emit_event(self, event_name: str, subject_id: str, payload: Mapping[str, object]) -&gt; None:\n    \"\"\"Emit an audit event for monitoring.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.incident","title":"<code>incident(event, subject_id, error_class, message)</code>","text":"<p>Record an incident for visibility.</p> Source code in <code>src/registry/api.py</code> <pre><code>def incident(self, event: str, subject_id: str, error_class: str, message: str) -&gt; None:\n    \"\"\"Record an incident for visibility.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.insert_run","title":"<code>insert_run(purpose, model_id, revision, config)</code>","text":"<p>Register a new processing run.</p> Source code in <code>src/registry/api.py</code> <pre><code>def insert_run(\n    self,\n    purpose: str,\n    model_id: str | None,\n    revision: str | None,\n    config: Mapping[str, object],\n) -&gt; str:\n    \"\"\"Register a new processing run.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.register_doctags","title":"<code>register_doctags(assets)</code>","text":"<p>Register DocTags assets with the registry.</p> Source code in <code>src/registry/api.py</code> <pre><code>def register_doctags(self, assets: list[DoctagsAsset]) -&gt; None:\n    \"\"\"Register DocTags assets with the registry.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.register_documents","title":"<code>register_documents(docs)</code>","text":"<p>Register document metadata with the registry.</p> Source code in <code>src/registry/api.py</code> <pre><code>def register_documents(self, docs: list[Doc]) -&gt; None:\n    \"\"\"Register document metadata with the registry.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.rollback_dataset","title":"<code>rollback_dataset(dataset_id)</code>","text":"<p>Rollback a dataset build after failure.</p> Source code in <code>src/registry/api.py</code> <pre><code>def rollback_dataset(self, dataset_id: str) -&gt; None:\n    \"\"\"Rollback a dataset build after failure.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/registry/duckdb_registry/","title":"<code>registry.duckdb_registry</code>","text":"<p>Module for registry.duckdb_registry.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry","title":"<code>DuckDBRegistry</code>","text":"<p>Minimal DuckDB-backed registry (skeleton).</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>class DuckDBRegistry:\n    \"\"\"Minimal DuckDB-backed registry (skeleton).\"\"\"\n\n    def __init__(self, db_path: str):\n        \"\"\"Init.\n\n        Args:\n            db_path (str): TODO.\n        \"\"\"\n        self.db_path = db_path\n        self.con = duckdb.connect(db_path, read_only=False)\n        self.con.execute(\"PRAGMA threads=14\")\n\n    def begin_dataset(self, kind: str, run_id: str) -&gt; str:\n        \"\"\"Begin dataset.\n\n        Args:\n            kind (str): TODO.\n            run_id (str): TODO.\n\n        Returns:\n            str: TODO.\n        \"\"\"\n        dsid = str(uuid.uuid4())\n        self.con.execute(\n            (\n                \"INSERT INTO datasets(\"\n                \"dataset_id, kind, parquet_root, run_id, created_at\"\n                \") VALUES (?, ?, '', ?, now())\"\n            ),\n            [dsid, kind, run_id],\n        )\n        return dsid\n\n    def commit_dataset(self, dataset_id: str, parquet_root: str, rows: int) -&gt; None:\n        \"\"\"Commit dataset.\n\n        Args:\n            dataset_id (str): TODO.\n            parquet_root (str): TODO.\n            rows (int): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        self.con.execute(\n            \"UPDATE datasets SET parquet_root=? WHERE dataset_id=?\", [parquet_root, dataset_id]\n        )\n\n    def rollback_dataset(self, dataset_id: str) -&gt; None:\n        \"\"\"Rollback dataset.\n\n        Args:\n            dataset_id (str): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        self.con.execute(\"DELETE FROM datasets WHERE dataset_id=?\", [dataset_id])\n\n    def insert_run(\n        self,\n        purpose: str,\n        model_id: str | None,\n        revision: str | None,\n        config: Mapping[str, object],\n    ) -&gt; str:\n        \"\"\"Insert run.\n\n        Args:\n            purpose (str): TODO.\n            model_id (str | None): TODO.\n            revision (str | None): TODO.\n            config (dict): TODO.\n\n        Returns:\n            str: TODO.\n        \"\"\"\n        run_id = str(uuid.uuid4())\n        self.con.execute(\n            (\n                \"INSERT INTO runs(\"\n                \"run_id, purpose, model_id, revision, started_at, config\"\n                \") VALUES (?, ?, ?, ?, now(), ?)\"\n            ),\n            [run_id, purpose, model_id, revision, json.dumps(config)],\n        )\n        return run_id\n\n    def close_run(self, run_id: str, success: bool, notes: str | None = None) -&gt; None:\n        \"\"\"Close run.\n\n        Args:\n            run_id (str): TODO.\n            success (bool): TODO.\n            notes (str | None): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        self.con.execute(\"UPDATE runs SET finished_at=now() WHERE run_id=?\", [run_id])\n\n    def register_documents(self, docs: list[Doc]) -&gt; None:\n        \"\"\"Register documents.\n\n        Args:\n            docs (List[Doc]): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        for d in docs:\n            self.con.execute(\n                (\n                    \"INSERT OR REPLACE INTO documents(\"\n                    \"doc_id, openalex_id, doi, arxiv_id, pmcid, title, authors, \"\n                    \"pub_date, license, language, pdf_uri, source, content_hash, created_at\"\n                    \") VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, now())\"\n                ),\n                [\n                    d.id,\n                    d.openalex_id,\n                    d.doi,\n                    d.arxiv_id,\n                    d.pmcid,\n                    d.title,\n                    json.dumps(d.authors),\n                    d.pub_date,\n                    d.license,\n                    d.language,\n                    d.pdf_uri,\n                    d.source,\n                    d.content_hash,\n                ],\n            )\n\n    def register_doctags(self, assets: list[DoctagsAsset]) -&gt; None:\n        \"\"\"Register doctags.\n\n        Args:\n            assets (List[DoctagsAsset]): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        for a in assets:\n            self.con.execute(\n                (\n                    \"INSERT OR REPLACE INTO doctags(\"\n                    \"doc_id, doctags_uri, pages, vlm_model, vlm_revision, avg_logprob, created_at\"\n                    \") VALUES (?, ?, ?, ?, ?, ?, now())\"\n                ),\n                [a.doc_id, a.doctags_uri, a.pages, a.vlm_model, a.vlm_revision, a.avg_logprob],\n            )\n\n    def emit_event(self, event_name: str, subject_id: str, payload: Mapping[str, object]) -&gt; None:\n        \"\"\"Emit event.\n\n        Args:\n            event_name (str): TODO.\n            subject_id (str): TODO.\n            payload (Dict): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        self.con.execute(\n            (\n                \"INSERT INTO pipeline_events(\"\n                \"event_id, event_name, subject_id, payload, created_at\"\n                \") VALUES (gen_random_uuid(), ?, ?, ?, now())\"\n            ),\n            [event_name, subject_id, json.dumps(payload)],\n        )\n\n    def incident(self, event: str, subject_id: str, error_class: str, message: str) -&gt; None:\n        \"\"\"Incident.\n\n        Args:\n            event (str): TODO.\n            subject_id (str): TODO.\n            error_class (str): TODO.\n            message (str): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        self.con.execute(\n            (\n                \"INSERT INTO incidents(\"\n                \"id, event, subject_id, error_class, message, created_at\"\n                \") VALUES (gen_random_uuid(), ?, ?, ?, ?, now())\"\n            ),\n            [event, subject_id, error_class, message],\n        )\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.__init__","title":"<code>__init__(db_path)</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>TODO.</p> required Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def __init__(self, db_path: str):\n    \"\"\"Init.\n\n    Args:\n        db_path (str): TODO.\n    \"\"\"\n    self.db_path = db_path\n    self.con = duckdb.connect(db_path, read_only=False)\n    self.con.execute(\"PRAGMA threads=14\")\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.begin_dataset","title":"<code>begin_dataset(kind, run_id)</code>","text":"<p>Begin dataset.</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>TODO.</p> required <code>run_id</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>TODO.</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def begin_dataset(self, kind: str, run_id: str) -&gt; str:\n    \"\"\"Begin dataset.\n\n    Args:\n        kind (str): TODO.\n        run_id (str): TODO.\n\n    Returns:\n        str: TODO.\n    \"\"\"\n    dsid = str(uuid.uuid4())\n    self.con.execute(\n        (\n            \"INSERT INTO datasets(\"\n            \"dataset_id, kind, parquet_root, run_id, created_at\"\n            \") VALUES (?, ?, '', ?, now())\"\n        ),\n        [dsid, kind, run_id],\n    )\n    return dsid\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.close_run","title":"<code>close_run(run_id, success, notes=None)</code>","text":"<p>Close run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>TODO.</p> required <code>success</code> <code>bool</code> <p>TODO.</p> required <code>notes</code> <code>str | None</code> <p>TODO.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def close_run(self, run_id: str, success: bool, notes: str | None = None) -&gt; None:\n    \"\"\"Close run.\n\n    Args:\n        run_id (str): TODO.\n        success (bool): TODO.\n        notes (str | None): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    self.con.execute(\"UPDATE runs SET finished_at=now() WHERE run_id=?\", [run_id])\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.commit_dataset","title":"<code>commit_dataset(dataset_id, parquet_root, rows)</code>","text":"<p>Commit dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>TODO.</p> required <code>parquet_root</code> <code>str</code> <p>TODO.</p> required <code>rows</code> <code>int</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def commit_dataset(self, dataset_id: str, parquet_root: str, rows: int) -&gt; None:\n    \"\"\"Commit dataset.\n\n    Args:\n        dataset_id (str): TODO.\n        parquet_root (str): TODO.\n        rows (int): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    self.con.execute(\n        \"UPDATE datasets SET parquet_root=? WHERE dataset_id=?\", [parquet_root, dataset_id]\n    )\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.emit_event","title":"<code>emit_event(event_name, subject_id, payload)</code>","text":"<p>Emit event.</p> <p>Parameters:</p> Name Type Description Default <code>event_name</code> <code>str</code> <p>TODO.</p> required <code>subject_id</code> <code>str</code> <p>TODO.</p> required <code>payload</code> <code>Dict</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def emit_event(self, event_name: str, subject_id: str, payload: Mapping[str, object]) -&gt; None:\n    \"\"\"Emit event.\n\n    Args:\n        event_name (str): TODO.\n        subject_id (str): TODO.\n        payload (Dict): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    self.con.execute(\n        (\n            \"INSERT INTO pipeline_events(\"\n            \"event_id, event_name, subject_id, payload, created_at\"\n            \") VALUES (gen_random_uuid(), ?, ?, ?, now())\"\n        ),\n        [event_name, subject_id, json.dumps(payload)],\n    )\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.incident","title":"<code>incident(event, subject_id, error_class, message)</code>","text":"<p>Incident.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>str</code> <p>TODO.</p> required <code>subject_id</code> <code>str</code> <p>TODO.</p> required <code>error_class</code> <code>str</code> <p>TODO.</p> required <code>message</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def incident(self, event: str, subject_id: str, error_class: str, message: str) -&gt; None:\n    \"\"\"Incident.\n\n    Args:\n        event (str): TODO.\n        subject_id (str): TODO.\n        error_class (str): TODO.\n        message (str): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    self.con.execute(\n        (\n            \"INSERT INTO incidents(\"\n            \"id, event, subject_id, error_class, message, created_at\"\n            \") VALUES (gen_random_uuid(), ?, ?, ?, ?, now())\"\n        ),\n        [event, subject_id, error_class, message],\n    )\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.insert_run","title":"<code>insert_run(purpose, model_id, revision, config)</code>","text":"<p>Insert run.</p> <p>Parameters:</p> Name Type Description Default <code>purpose</code> <code>str</code> <p>TODO.</p> required <code>model_id</code> <code>str | None</code> <p>TODO.</p> required <code>revision</code> <code>str | None</code> <p>TODO.</p> required <code>config</code> <code>dict</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>TODO.</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def insert_run(\n    self,\n    purpose: str,\n    model_id: str | None,\n    revision: str | None,\n    config: Mapping[str, object],\n) -&gt; str:\n    \"\"\"Insert run.\n\n    Args:\n        purpose (str): TODO.\n        model_id (str | None): TODO.\n        revision (str | None): TODO.\n        config (dict): TODO.\n\n    Returns:\n        str: TODO.\n    \"\"\"\n    run_id = str(uuid.uuid4())\n    self.con.execute(\n        (\n            \"INSERT INTO runs(\"\n            \"run_id, purpose, model_id, revision, started_at, config\"\n            \") VALUES (?, ?, ?, ?, now(), ?)\"\n        ),\n        [run_id, purpose, model_id, revision, json.dumps(config)],\n    )\n    return run_id\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.register_doctags","title":"<code>register_doctags(assets)</code>","text":"<p>Register doctags.</p> <p>Parameters:</p> Name Type Description Default <code>assets</code> <code>List[DoctagsAsset]</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def register_doctags(self, assets: list[DoctagsAsset]) -&gt; None:\n    \"\"\"Register doctags.\n\n    Args:\n        assets (List[DoctagsAsset]): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    for a in assets:\n        self.con.execute(\n            (\n                \"INSERT OR REPLACE INTO doctags(\"\n                \"doc_id, doctags_uri, pages, vlm_model, vlm_revision, avg_logprob, created_at\"\n                \") VALUES (?, ?, ?, ?, ?, ?, now())\"\n            ),\n            [a.doc_id, a.doctags_uri, a.pages, a.vlm_model, a.vlm_revision, a.avg_logprob],\n        )\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.register_documents","title":"<code>register_documents(docs)</code>","text":"<p>Register documents.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Doc]</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def register_documents(self, docs: list[Doc]) -&gt; None:\n    \"\"\"Register documents.\n\n    Args:\n        docs (List[Doc]): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    for d in docs:\n        self.con.execute(\n            (\n                \"INSERT OR REPLACE INTO documents(\"\n                \"doc_id, openalex_id, doi, arxiv_id, pmcid, title, authors, \"\n                \"pub_date, license, language, pdf_uri, source, content_hash, created_at\"\n                \") VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, now())\"\n            ),\n            [\n                d.id,\n                d.openalex_id,\n                d.doi,\n                d.arxiv_id,\n                d.pmcid,\n                d.title,\n                json.dumps(d.authors),\n                d.pub_date,\n                d.license,\n                d.language,\n                d.pdf_uri,\n                d.source,\n                d.content_hash,\n            ],\n        )\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.rollback_dataset","title":"<code>rollback_dataset(dataset_id)</code>","text":"<p>Rollback dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def rollback_dataset(self, dataset_id: str) -&gt; None:\n    \"\"\"Rollback dataset.\n\n    Args:\n        dataset_id (str): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    self.con.execute(\"DELETE FROM datasets WHERE dataset_id=?\", [dataset_id])\n</code></pre>"},{"location":"api/registry/helper/","title":"<code>registry.helper</code>","text":"<p>Module for registry.helper.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper","title":"<code>DuckDBRegistryHelper</code>","text":"<p>Duckdbregistryhelper.</p> Source code in <code>src/registry/helper.py</code> <pre><code>class DuckDBRegistryHelper:\n    \"\"\"Duckdbregistryhelper.\"\"\"\n\n    def __init__(self, db_path: str):\n        \"\"\"Init.\n\n        Args:\n            db_path (str): TODO.\n        \"\"\"\n        self.db_path = db_path\n\n    def _con(self) -&gt; duckdb.DuckDBPyConnection:\n        \"\"\"Con.\"\"\"\n        return duckdb.connect(self.db_path)\n\n    def new_run(\n        self,\n        purpose: str,\n        model_id: str | None,\n        revision: str | None,\n        config: Mapping[str, object],\n    ) -&gt; str:\n        \"\"\"New run.\n\n        Args:\n            purpose (str): TODO.\n            model_id (Optional[str]): TODO.\n            revision (Optional[str]): TODO.\n            config (dict): TODO.\n\n        Returns:\n            str: TODO.\n        \"\"\"\n        run_id = str(uuid.uuid4())\n        con = self._con()\n        con.execute(\n            (\n                \"INSERT INTO runs \"\n                \"(run_id,purpose,model_id,revision,started_at,config) \"\n                \"VALUES (?,?,?,?,CURRENT_TIMESTAMP,?)\"\n            ),\n            [run_id, purpose, model_id, revision, json.dumps(config)],\n        )\n        con.close()\n        return run_id\n\n    def close_run(self, run_id: str, success: bool, notes: str | None = None) -&gt; None:\n        \"\"\"Close run.\n\n        Args:\n            run_id (str): TODO.\n            success (bool): TODO.\n            notes (Optional[str]): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        con = self._con()\n        con.execute(\"UPDATE runs SET finished_at=CURRENT_TIMESTAMP WHERE run_id=?\", [run_id])\n        con.execute(\n            \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n            [\n                str(uuid.uuid4()),\n                \"RunClosed\",\n                run_id,\n                json.dumps({\"success\": success, \"notes\": notes or \"\"}),\n            ],\n        )\n        con.close()\n\n    def begin_dataset(self, kind: str, run_id: str) -&gt; str:\n        \"\"\"Begin dataset.\n\n        Args:\n            kind (str): TODO.\n            run_id (str): TODO.\n\n        Returns:\n            str: TODO.\n        \"\"\"\n        dataset_id = str(uuid.uuid4())\n        con = self._con()\n        con.execute(\n            (\n                \"INSERT INTO datasets \"\n                \"(dataset_id,kind,parquet_root,run_id,created_at) \"\n                \"VALUES (?,?,?,?,CURRENT_TIMESTAMP)\"\n            ),\n            [dataset_id, kind, \"\", run_id],\n        )\n        con.close()\n        return dataset_id\n\n    def commit_dataset(self, dataset_id: str, parquet_root: str, rows: int) -&gt; None:\n        \"\"\"Commit dataset.\n\n        Args:\n            dataset_id (str): TODO.\n            parquet_root (str): TODO.\n            rows (int): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        con = self._con()\n        con.execute(\n            \"UPDATE datasets SET parquet_root=? WHERE dataset_id=?\", [parquet_root, dataset_id]\n        )\n        con.execute(\n            \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n            [\n                str(uuid.uuid4()),\n                \"DatasetCommitted\",\n                dataset_id,\n                json.dumps({\"rows\": rows, \"root\": parquet_root}),\n            ],\n        )\n        con.close()\n\n    def rollback_dataset(self, dataset_id: str) -&gt; None:\n        \"\"\"Rollback dataset.\n\n        Args:\n            dataset_id (str): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        con = self._con()\n        con.execute(\"DELETE FROM datasets WHERE dataset_id=?\", [dataset_id])\n        con.execute(\n            \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n            [str(uuid.uuid4()), \"DatasetRolledBack\", dataset_id, \"{}\"],\n        )\n        con.close()\n\n    def register_documents(self, docs: list[Doc]) -&gt; None:\n        \"\"\"Register documents.\n\n        Args:\n            docs (List[Doc]): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        con = self._con()\n        for d in docs:\n            con.execute(\n                \"\"\"INSERT OR REPLACE INTO documents\n                (doc_id, openalex_id, doi, arxiv_id, pmcid, title, authors, pub_date, license,\n                 language, pdf_uri, source, content_hash, created_at)\n                VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,CURRENT_TIMESTAMP)\"\"\",\n                [\n                    d.id,\n                    d.openalex_id,\n                    d.doi,\n                    d.arxiv_id,\n                    d.pmcid,\n                    d.title,\n                    json.dumps(d.authors),\n                    d.pub_date,\n                    d.license,\n                    d.language,\n                    d.pdf_uri,\n                    d.source,\n                    d.content_hash or \"\",\n                ],\n            )\n        con.close()\n\n    def register_doctags(self, assets: list[DoctagsAsset]) -&gt; None:\n        \"\"\"Register doctags.\n\n        Args:\n            assets (List[DoctagsAsset]): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        con = self._con()\n        for a in assets:\n            con.execute(\n                \"INSERT OR REPLACE INTO doctags VALUES (?,?,?,?,?,?,CURRENT_TIMESTAMP)\",\n                [a.doc_id, a.doctags_uri, a.pages, a.vlm_model, a.vlm_revision, a.avg_logprob],\n            )\n        con.close()\n\n    def emit_event(self, event_name: str, subject_id: str, payload: Mapping[str, object]) -&gt; None:\n        \"\"\"Emit event.\n\n        Args:\n            event_name (str): TODO.\n            subject_id (str): TODO.\n            payload (Dict): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        con = self._con()\n        con.execute(\n            \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n            [str(uuid.uuid4()), event_name, subject_id, json.dumps(payload)],\n        )\n        con.close()\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.__init__","title":"<code>__init__(db_path)</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>TODO.</p> required Source code in <code>src/registry/helper.py</code> <pre><code>def __init__(self, db_path: str):\n    \"\"\"Init.\n\n    Args:\n        db_path (str): TODO.\n    \"\"\"\n    self.db_path = db_path\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.begin_dataset","title":"<code>begin_dataset(kind, run_id)</code>","text":"<p>Begin dataset.</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>TODO.</p> required <code>run_id</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>TODO.</p> Source code in <code>src/registry/helper.py</code> <pre><code>def begin_dataset(self, kind: str, run_id: str) -&gt; str:\n    \"\"\"Begin dataset.\n\n    Args:\n        kind (str): TODO.\n        run_id (str): TODO.\n\n    Returns:\n        str: TODO.\n    \"\"\"\n    dataset_id = str(uuid.uuid4())\n    con = self._con()\n    con.execute(\n        (\n            \"INSERT INTO datasets \"\n            \"(dataset_id,kind,parquet_root,run_id,created_at) \"\n            \"VALUES (?,?,?,?,CURRENT_TIMESTAMP)\"\n        ),\n        [dataset_id, kind, \"\", run_id],\n    )\n    con.close()\n    return dataset_id\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.close_run","title":"<code>close_run(run_id, success, notes=None)</code>","text":"<p>Close run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>TODO.</p> required <code>success</code> <code>bool</code> <p>TODO.</p> required <code>notes</code> <code>Optional[str]</code> <p>TODO.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/registry/helper.py</code> <pre><code>def close_run(self, run_id: str, success: bool, notes: str | None = None) -&gt; None:\n    \"\"\"Close run.\n\n    Args:\n        run_id (str): TODO.\n        success (bool): TODO.\n        notes (Optional[str]): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    con = self._con()\n    con.execute(\"UPDATE runs SET finished_at=CURRENT_TIMESTAMP WHERE run_id=?\", [run_id])\n    con.execute(\n        \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n        [\n            str(uuid.uuid4()),\n            \"RunClosed\",\n            run_id,\n            json.dumps({\"success\": success, \"notes\": notes or \"\"}),\n        ],\n    )\n    con.close()\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.commit_dataset","title":"<code>commit_dataset(dataset_id, parquet_root, rows)</code>","text":"<p>Commit dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>TODO.</p> required <code>parquet_root</code> <code>str</code> <p>TODO.</p> required <code>rows</code> <code>int</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/registry/helper.py</code> <pre><code>def commit_dataset(self, dataset_id: str, parquet_root: str, rows: int) -&gt; None:\n    \"\"\"Commit dataset.\n\n    Args:\n        dataset_id (str): TODO.\n        parquet_root (str): TODO.\n        rows (int): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    con = self._con()\n    con.execute(\n        \"UPDATE datasets SET parquet_root=? WHERE dataset_id=?\", [parquet_root, dataset_id]\n    )\n    con.execute(\n        \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n        [\n            str(uuid.uuid4()),\n            \"DatasetCommitted\",\n            dataset_id,\n            json.dumps({\"rows\": rows, \"root\": parquet_root}),\n        ],\n    )\n    con.close()\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.emit_event","title":"<code>emit_event(event_name, subject_id, payload)</code>","text":"<p>Emit event.</p> <p>Parameters:</p> Name Type Description Default <code>event_name</code> <code>str</code> <p>TODO.</p> required <code>subject_id</code> <code>str</code> <p>TODO.</p> required <code>payload</code> <code>Dict</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/registry/helper.py</code> <pre><code>def emit_event(self, event_name: str, subject_id: str, payload: Mapping[str, object]) -&gt; None:\n    \"\"\"Emit event.\n\n    Args:\n        event_name (str): TODO.\n        subject_id (str): TODO.\n        payload (Dict): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    con = self._con()\n    con.execute(\n        \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n        [str(uuid.uuid4()), event_name, subject_id, json.dumps(payload)],\n    )\n    con.close()\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.new_run","title":"<code>new_run(purpose, model_id, revision, config)</code>","text":"<p>New run.</p> <p>Parameters:</p> Name Type Description Default <code>purpose</code> <code>str</code> <p>TODO.</p> required <code>model_id</code> <code>Optional[str]</code> <p>TODO.</p> required <code>revision</code> <code>Optional[str]</code> <p>TODO.</p> required <code>config</code> <code>dict</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>TODO.</p> Source code in <code>src/registry/helper.py</code> <pre><code>def new_run(\n    self,\n    purpose: str,\n    model_id: str | None,\n    revision: str | None,\n    config: Mapping[str, object],\n) -&gt; str:\n    \"\"\"New run.\n\n    Args:\n        purpose (str): TODO.\n        model_id (Optional[str]): TODO.\n        revision (Optional[str]): TODO.\n        config (dict): TODO.\n\n    Returns:\n        str: TODO.\n    \"\"\"\n    run_id = str(uuid.uuid4())\n    con = self._con()\n    con.execute(\n        (\n            \"INSERT INTO runs \"\n            \"(run_id,purpose,model_id,revision,started_at,config) \"\n            \"VALUES (?,?,?,?,CURRENT_TIMESTAMP,?)\"\n        ),\n        [run_id, purpose, model_id, revision, json.dumps(config)],\n    )\n    con.close()\n    return run_id\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.register_doctags","title":"<code>register_doctags(assets)</code>","text":"<p>Register doctags.</p> <p>Parameters:</p> Name Type Description Default <code>assets</code> <code>List[DoctagsAsset]</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/registry/helper.py</code> <pre><code>def register_doctags(self, assets: list[DoctagsAsset]) -&gt; None:\n    \"\"\"Register doctags.\n\n    Args:\n        assets (List[DoctagsAsset]): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    con = self._con()\n    for a in assets:\n        con.execute(\n            \"INSERT OR REPLACE INTO doctags VALUES (?,?,?,?,?,?,CURRENT_TIMESTAMP)\",\n            [a.doc_id, a.doctags_uri, a.pages, a.vlm_model, a.vlm_revision, a.avg_logprob],\n        )\n    con.close()\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.register_documents","title":"<code>register_documents(docs)</code>","text":"<p>Register documents.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Doc]</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/registry/helper.py</code> <pre><code>def register_documents(self, docs: list[Doc]) -&gt; None:\n    \"\"\"Register documents.\n\n    Args:\n        docs (List[Doc]): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    con = self._con()\n    for d in docs:\n        con.execute(\n            \"\"\"INSERT OR REPLACE INTO documents\n            (doc_id, openalex_id, doi, arxiv_id, pmcid, title, authors, pub_date, license,\n             language, pdf_uri, source, content_hash, created_at)\n            VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,CURRENT_TIMESTAMP)\"\"\",\n            [\n                d.id,\n                d.openalex_id,\n                d.doi,\n                d.arxiv_id,\n                d.pmcid,\n                d.title,\n                json.dumps(d.authors),\n                d.pub_date,\n                d.license,\n                d.language,\n                d.pdf_uri,\n                d.source,\n                d.content_hash or \"\",\n            ],\n        )\n    con.close()\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.rollback_dataset","title":"<code>rollback_dataset(dataset_id)</code>","text":"<p>Rollback dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/registry/helper.py</code> <pre><code>def rollback_dataset(self, dataset_id: str) -&gt; None:\n    \"\"\"Rollback dataset.\n\n    Args:\n        dataset_id (str): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    con = self._con()\n    con.execute(\"DELETE FROM datasets WHERE dataset_id=?\", [dataset_id])\n    con.execute(\n        \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n        [str(uuid.uuid4()), \"DatasetRolledBack\", dataset_id, \"{}\"],\n    )\n    con.close()\n</code></pre>"},{"location":"api/registry/migrate/","title":"<code>registry.migrate</code>","text":"<p>Module for registry.migrate.</p>"},{"location":"api/registry/migrate/#registry.migrate.apply","title":"<code>apply(db, migrations_dir)</code>","text":"<p>Apply.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>str</code> <p>TODO.</p> required <code>migrations_dir</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/registry/migrate.py</code> <pre><code>def apply(db: str, migrations_dir: str) -&gt; None:\n    \"\"\"Apply.\n\n    Args:\n        db (str): TODO.\n        migrations_dir (str): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    con = duckdb.connect(db)\n    for p in sorted(pathlib.Path(migrations_dir).glob(\"*.sql\")):\n        con.execute(p.read_text())\n    con.close()\n</code></pre>"},{"location":"api/registry/migrate/#registry.migrate.main","title":"<code>main()</code>","text":"<p>Main.</p> Source code in <code>src/registry/migrate.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main.\"\"\"\n    ap = argparse.ArgumentParser()\n    sp = ap.add_subparsers(dest=\"cmd\", required=True)\n    a = sp.add_parser(\"apply\")\n    a.add_argument(\"--db\", required=True)\n    a.add_argument(\"--migrations\", required=True)\n    ns = ap.parse_args()\n    if ns.cmd == \"apply\":\n        apply(ns.db, ns.migrations)\n</code></pre>"},{"location":"api/search_api/","title":"<code>search_api</code>","text":"<p>Module for search_api.init.</p>"},{"location":"api/search_api/app/","title":"<code>search_api.app</code>","text":"<p>Module for search_api.app.</p>"},{"location":"api/search_api/app/#search_api.app.apply_kg_boosts","title":"<code>apply_kg_boosts(cands, query, direct=0.08, one_hop=0.04)</code>","text":"<p>Apply kg boosts.</p> <p>Parameters:</p> Name Type Description Default <code>cands</code> <code>Dict[str, float]</code> <p>TODO.</p> required <code>query</code> <code>str</code> <p>TODO.</p> required <code>direct</code> <code>float</code> <p>TODO.</p> <code>0.08</code> <code>one_hop</code> <code>float</code> <p>TODO.</p> <code>0.04</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dict[str, float]: TODO.</p> Source code in <code>src/search_api/app.py</code> <pre><code>def apply_kg_boosts(\n    cands: dict[str, float],\n    query: str,\n    direct: float = 0.08,\n    one_hop: float = 0.04,\n) -&gt; dict[str, float]:\n    \"\"\"Apply kg boosts.\n\n    Args:\n        cands (Dict[str, float]): TODO.\n        query (str): TODO.\n        direct: TODO.\n        one_hop: TODO.\n\n    Returns:\n        Dict[str, float]: TODO.\n    \"\"\"\n    # toy: map words 'concept42' to concept id\n    q_concepts = set()\n    for w in query.lower().split():\n        if w.startswith(\"concept\"):\n            q_concepts.add(f\"C:{w.replace('concept','')}\")\n    out = dict(cands)\n    for chunk_id, base in cands.items():\n        linked = set(kg.linked_concepts(chunk_id))\n        boost = 0.0\n        if linked &amp; q_concepts:\n            boost += direct\n        else:\n            for c in linked:\n                if set(kg.one_hop(c)) &amp; q_concepts:\n                    boost += one_hop\n                    break\n        out[chunk_id] = base + boost\n    return out\n</code></pre>"},{"location":"api/search_api/app/#search_api.app.auth","title":"<code>auth(authorization=Header(default=None))</code>","text":"<p>Auth.</p> <p>Parameters:</p> Name Type Description Default <code>authorization</code> <code>Optional[str]</code> <p>TODO.</p> <code>Header(default=None)</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/search_api/app.py</code> <pre><code>def auth(authorization: str | None = Header(default=None)) -&gt; None:\n    \"\"\"Auth.\n\n    Args:\n        authorization (Optional[str]): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    if not API_KEYS:\n        return  # disabled in skeleton\n    if not authorization or not authorization.startswith(\"Bearer \"):\n        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n    token = authorization.split(\" \", 1)[1]\n    if token not in API_KEYS:\n        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n</code></pre>"},{"location":"api/search_api/app/#search_api.app.graph_concepts","title":"<code>graph_concepts(body, _=Depends(auth))</code>","text":"<p>Graph concepts.</p> <p>Parameters:</p> Name Type Description Default <code>body</code> <code>dict</code> <p>TODO.</p> required <code>_</code> <code>None</code> <p>TODO.</p> <code>Depends(auth)</code> Source code in <code>src/search_api/app.py</code> <pre><code>@app.post(\"/graph/concepts\", response_model=dict)\ndef graph_concepts(body: Mapping[str, Any], _: None = Depends(auth)) -&gt; dict[str, Any]:\n    \"\"\"Graph concepts.\n\n    Args:\n        body (dict): TODO.\n        _: TODO.\n    \"\"\"\n    q = (body or {}).get(\"q\", \"\").lower()\n    # toy: return nodes that contain the query substring\n    concepts = [\n        {\"concept_id\": c, \"label\": c}\n        for c in sorted({c for cs in kg.chunk2concepts.values() for c in cs})\n        if q in c.lower()\n    ][: body.get(\"limit\", 50)]\n    return {\"concepts\": concepts}\n</code></pre>"},{"location":"api/search_api/app/#search_api.app.healthz","title":"<code>healthz()</code>","text":"<p>Healthz.</p> Source code in <code>src/search_api/app.py</code> <pre><code>@app.get(\"/healthz\")\ndef healthz() -&gt; dict[str, Any]:\n    \"\"\"Healthz.\"\"\"\n    return {\n        \"status\": \"ok\",\n        \"components\": {\n            \"faiss\": (\"loaded\" if faiss is not None else \"missing\"),\n            \"bm25\": type(bm25).__name__,\n            \"splade\": type(splade).__name__,\n            \"vllm_embeddings\": \"mocked\",\n            \"neo4j\": \"mocked\",\n        },\n    }\n</code></pre>"},{"location":"api/search_api/app/#search_api.app.rrf_fuse","title":"<code>rrf_fuse(lists, k_rrf)</code>","text":"<p>Rrf fuse.</p> <p>Parameters:</p> Name Type Description Default <code>lists</code> <code>List[List[Tuple[str, float]]]</code> <p>TODO.</p> required <code>k_rrf</code> <code>int</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dict[str, float]: TODO.</p> Source code in <code>src/search_api/app.py</code> <pre><code>def rrf_fuse(lists: list[list[tuple[str, float]]], k_rrf: int) -&gt; dict[str, float]:\n    \"\"\"Rrf fuse.\n\n    Args:\n        lists (List[List[Tuple[str, float]]]): TODO.\n        k_rrf (int): TODO.\n\n    Returns:\n        Dict[str, float]: TODO.\n    \"\"\"\n    scores: dict[str, float] = {}\n    for hits in lists:\n        for rank, (doc_id, _score) in enumerate(hits, start=1):\n            scores[doc_id] = scores.get(doc_id, 0.0) + 1.0 / (k_rrf + rank)\n    return scores\n</code></pre>"},{"location":"api/search_api/app/#search_api.app.search","title":"<code>search(req, _=Depends(auth))</code>","text":"<p>Search.</p> <p>Parameters:</p> Name Type Description Default <code>req</code> <code>SearchRequest</code> <p>TODO.</p> required <code>_</code> <code>None</code> <p>TODO.</p> <code>Depends(auth)</code> Source code in <code>src/search_api/app.py</code> <pre><code>@app.post(\"/search\", response_model=dict)\ndef search(req: SearchRequest, _: None = Depends(auth)) -&gt; dict[str, Any]:\n    \"\"\"Search.\n\n    Args:\n        req (SearchRequest): TODO.\n        _: TODO.\n    \"\"\"\n    # Retrieve from each channel\n    dense_hits: list[tuple[str, float]] = (\n        []\n    )  # we don't have a query embedder here; fallback to empty or demo vector\n    # sparse via BM25 (preferred) and SPLADE\n    bm25_hits = bm25.search(req.query, k=CFG[\"search\"][\"sparse_candidates\"]) if bm25 else []\n    try:\n        splade_hits = (\n            splade.search(req.query, k=CFG[\"search\"][\"sparse_candidates\"]) if splade else []\n        )\n    except Exception:\n        splade_hits = []\n\n    # RRF fusion\n    fused = rrf_fuse([dense_hits, bm25_hits, splade_hits], k_rrf=int(CFG[\"search\"][\"rrf_k\"]))\n    # KG boosts\n    boosted = apply_kg_boosts(\n        fused,\n        req.query,\n        direct=CFG[\"search\"][\"kg_boosts\"][\"direct\"],\n        one_hop=CFG[\"search\"][\"kg_boosts\"][\"one_hop\"],\n    )\n    # Rank and craft results\n    top = sorted(boosted.items(), key=lambda x: x[1], reverse=True)[: req.k]\n    results: list[dict[str, Any]] = []\n    for chunk_id, score in top:\n        # In real system we'd hydrate title/section via DuckDB; here we echo ids\n        results.append(\n            SearchResult(\n                doc_id=f\"doc-of-{chunk_id}\",\n                chunk_id=chunk_id,\n                title=f\"Title for {chunk_id}\",\n                section=\"Methods\",\n                score=float(score),\n                signals={\n                    \"rrf\": float(fused.get(chunk_id, 0.0)),\n                    \"kg_boost\": float(boosted[chunk_id] - fused.get(chunk_id, 0.0)),\n                },\n                spans={\"start_char\": 0, \"end_char\": 50},\n                concepts=[\n                    {\n                        \"concept_id\": c,\n                        \"label\": c,\n                        \"match\": (\"direct\" if c in req.query else \"nearby\"),\n                    }\n                    for c in kg.linked_concepts(chunk_id)\n                ],\n            ).model_dump()\n        )\n    return {\"results\": results}\n</code></pre>"},{"location":"api/search_api/bm25_index/","title":"<code>search_api.bm25_index</code>","text":"<p>Module for search_api.bm25_index.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Doc","title":"<code>BM25Doc</code>  <code>dataclass</code>","text":"<p>Bm25doc.</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>@dataclass\nclass BM25Doc:\n    \"\"\"Bm25doc.\"\"\"\n\n    chunk_id: str\n    doc_id: str\n    title: str\n    section: str\n    tf: dict[str, float]\n    dl: float\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index","title":"<code>BM25Index</code>","text":"<p>Bm25index.</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>class BM25Index:\n    \"\"\"Bm25index.\"\"\"\n\n    def __init__(self, k1: float = 0.9, b: float = 0.4):\n        \"\"\"Init.\n\n        Args:\n            k1 (float): TODO.\n            b (float): TODO.\n        \"\"\"\n        self.k1 = k1\n        self.b = b\n        self.docs: list[BM25Doc] = []\n        self.df: dict[str, int] = {}\n        self.N = 0\n        self.avgdl = 0.0\n\n    @classmethod\n    def build_from_duckdb(cls, db_path: str) -&gt; BM25Index:\n        \"\"\"Build from duckdb.\n\n        Args:\n            db_path (str): TODO.\n\n        Returns:\n            \"BM25Index\": TODO.\n        \"\"\"\n        idx = cls()\n        con = duckdb.connect(db_path)\n        try:\n            ds = con.execute(\n                \"SELECT parquet_root FROM datasets \"\n                \"WHERE kind='chunks' ORDER BY created_at DESC LIMIT 1\"\n            ).fetchone()\n            if not ds:\n                return idx\n            root = ds[0]\n            rows = con.execute(\n                f\"\"\"\n                SELECT c.chunk_id, c.doc_id, coalesce(c.section,''), c.text, coalesce(d.title,'')\n                FROM read_parquet('{root}/*/*.parquet', union_by_name=true) AS c\n                LEFT JOIN documents d ON c.doc_id = d.doc_id\n            \"\"\"\n            ).fetchall()\n        finally:\n            con.close()\n        idx._build(rows)\n        return idx\n\n    def _build(self, rows: Iterable[tuple[str, str, str, str, str]]) -&gt; None:\n        \"\"\"Build.\n\n        Args:\n            rows: TODO.\n        \"\"\"\n        self.docs.clear()\n        self.df.clear()\n        dl_sum = 0.0\n        for chunk_id, doc_id, section, body, title in rows:\n            tf: dict[str, float] = {}\n            for t in toks(body or \"\"):\n                tf[t] = tf.get(t, 0.0) + 1.0\n            for t in toks(title or \"\"):\n                tf[t] = tf.get(t, 0.0) + 2.0\n            for t in toks(section or \"\"):\n                tf[t] = tf.get(t, 0.0) + 1.2\n            dl = sum(tf.values())\n            self.docs.append(\n                BM25Doc(\n                    chunk_id=chunk_id,\n                    doc_id=doc_id or \"urn:doc:fixture\",\n                    title=title or \"Fixture\",\n                    section=section or \"\",\n                    tf=tf,\n                    dl=dl,\n                )\n            )\n            dl_sum += dl\n            for term in set(tf.keys()):\n                self.df[term] = self.df.get(term, 0) + 1\n        self.N = len(self.docs)\n        self.avgdl = (dl_sum / self.N) if self.N &gt; 0 else 0.0\n\n    def save(self, path: str) -&gt; None:\n        \"\"\"Save.\n\n        Args:\n            path (str): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        Path(path).parent.mkdir(parents=True, exist_ok=True)\n        with open(path, \"wb\") as f:\n            pickle.dump(\n                {\n                    \"k1\": self.k1,\n                    \"b\": self.b,\n                    \"N\": self.N,\n                    \"avgdl\": self.avgdl,\n                    \"df\": self.df,\n                    \"docs\": self.docs,\n                },\n                f,\n            )\n\n    @classmethod\n    def load(cls, path: str) -&gt; BM25Index:\n        \"\"\"Load.\n\n        Args:\n            path (str): TODO.\n\n        Returns:\n            \"BM25Index\": TODO.\n        \"\"\"\n        with open(path, \"rb\") as f:\n            d = pickle.load(f)\n        idx = cls(d.get(\"k1\", 0.9), d.get(\"b\", 0.4))\n        idx.N = d[\"N\"]\n        idx.avgdl = d[\"avgdl\"]\n        idx.df = d[\"df\"]\n        idx.docs = d[\"docs\"]\n        return idx\n\n    def _idf(self, term: str) -&gt; float:\n        \"\"\"Idf.\n\n        Args:\n            term (str): TODO.\n\n        Returns:\n            float: TODO.\n        \"\"\"\n        df = self.df.get(term, 0)\n        return math.log((self.N - df + 0.5) / (df + 0.5) + 1.0) if self.N &gt; 0 and df &gt; 0 else 0.0\n\n    def search(self, query: str, k: int = 10) -&gt; list[tuple[int, float]]:\n        \"\"\"Search.\n\n        Args:\n            query (str): TODO.\n            k (int): TODO.\n        \"\"\"\n        if self.N == 0:\n            return []\n        terms = toks(query)\n        scores = [0.0] * self.N\n        for i, d in enumerate(self.docs):\n            s = 0.0\n            for t in terms:\n                tf = d.tf.get(t, 0.0)\n                if tf &lt;= 0.0:\n                    continue\n                idf = self._idf(t)\n                denom = tf + self.k1 * (1.0 - self.b + self.b * (d.dl / (self.avgdl or 1.0)))\n                s += idf * ((tf * (self.k1 + 1.0)) / denom)\n            scores[i] = s\n        ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n        return [(i, s) for i, s in ranked[:k] if s &gt; 0.0]\n\n    def doc(self, idx: int) -&gt; BM25Doc:\n        \"\"\"Doc.\n\n        Args:\n            idx (int): TODO.\n\n        Returns:\n            BM25Doc: TODO.\n        \"\"\"\n        return self.docs[idx]\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.__init__","title":"<code>__init__(k1=0.9, b=0.4)</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>k1</code> <code>float</code> <p>TODO.</p> <code>0.9</code> <code>b</code> <code>float</code> <p>TODO.</p> <code>0.4</code> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>def __init__(self, k1: float = 0.9, b: float = 0.4):\n    \"\"\"Init.\n\n    Args:\n        k1 (float): TODO.\n        b (float): TODO.\n    \"\"\"\n    self.k1 = k1\n    self.b = b\n    self.docs: list[BM25Doc] = []\n    self.df: dict[str, int] = {}\n    self.N = 0\n    self.avgdl = 0.0\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.build_from_duckdb","title":"<code>build_from_duckdb(db_path)</code>  <code>classmethod</code>","text":"<p>Build from duckdb.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>BM25Index</code> <p>\"BM25Index\": TODO.</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>@classmethod\ndef build_from_duckdb(cls, db_path: str) -&gt; BM25Index:\n    \"\"\"Build from duckdb.\n\n    Args:\n        db_path (str): TODO.\n\n    Returns:\n        \"BM25Index\": TODO.\n    \"\"\"\n    idx = cls()\n    con = duckdb.connect(db_path)\n    try:\n        ds = con.execute(\n            \"SELECT parquet_root FROM datasets \"\n            \"WHERE kind='chunks' ORDER BY created_at DESC LIMIT 1\"\n        ).fetchone()\n        if not ds:\n            return idx\n        root = ds[0]\n        rows = con.execute(\n            f\"\"\"\n            SELECT c.chunk_id, c.doc_id, coalesce(c.section,''), c.text, coalesce(d.title,'')\n            FROM read_parquet('{root}/*/*.parquet', union_by_name=true) AS c\n            LEFT JOIN documents d ON c.doc_id = d.doc_id\n        \"\"\"\n        ).fetchall()\n    finally:\n        con.close()\n    idx._build(rows)\n    return idx\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.doc","title":"<code>doc(idx)</code>","text":"<p>Doc.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>BM25Doc</code> <code>BM25Doc</code> <p>TODO.</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>def doc(self, idx: int) -&gt; BM25Doc:\n    \"\"\"Doc.\n\n    Args:\n        idx (int): TODO.\n\n    Returns:\n        BM25Doc: TODO.\n    \"\"\"\n    return self.docs[idx]\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>BM25Index</code> <p>\"BM25Index\": TODO.</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>@classmethod\ndef load(cls, path: str) -&gt; BM25Index:\n    \"\"\"Load.\n\n    Args:\n        path (str): TODO.\n\n    Returns:\n        \"BM25Index\": TODO.\n    \"\"\"\n    with open(path, \"rb\") as f:\n        d = pickle.load(f)\n    idx = cls(d.get(\"k1\", 0.9), d.get(\"b\", 0.4))\n    idx.N = d[\"N\"]\n    idx.avgdl = d[\"avgdl\"]\n    idx.df = d[\"df\"]\n    idx.docs = d[\"docs\"]\n    return idx\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.save","title":"<code>save(path)</code>","text":"<p>Save.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"Save.\n\n    Args:\n        path (str): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    Path(path).parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"wb\") as f:\n        pickle.dump(\n            {\n                \"k1\": self.k1,\n                \"b\": self.b,\n                \"N\": self.N,\n                \"avgdl\": self.avgdl,\n                \"df\": self.df,\n                \"docs\": self.docs,\n            },\n            f,\n        )\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.search","title":"<code>search(query, k=10)</code>","text":"<p>Search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>TODO.</p> required <code>k</code> <code>int</code> <p>TODO.</p> <code>10</code> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>def search(self, query: str, k: int = 10) -&gt; list[tuple[int, float]]:\n    \"\"\"Search.\n\n    Args:\n        query (str): TODO.\n        k (int): TODO.\n    \"\"\"\n    if self.N == 0:\n        return []\n    terms = toks(query)\n    scores = [0.0] * self.N\n    for i, d in enumerate(self.docs):\n        s = 0.0\n        for t in terms:\n            tf = d.tf.get(t, 0.0)\n            if tf &lt;= 0.0:\n                continue\n            idf = self._idf(t)\n            denom = tf + self.k1 * (1.0 - self.b + self.b * (d.dl / (self.avgdl or 1.0)))\n            s += idf * ((tf * (self.k1 + 1.0)) / denom)\n        scores[i] = s\n    ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n    return [(i, s) for i, s in ranked[:k] if s &gt; 0.0]\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.toks","title":"<code>toks(s)</code>","text":"<p>Toks.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: TODO.</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>def toks(s: str) -&gt; list[str]:\n    \"\"\"Toks.\n\n    Args:\n        s (str): TODO.\n\n    Returns:\n        List[str]: TODO.\n    \"\"\"\n    return [t.lower() for t in TOKEN_RE.findall(s or \"\")]\n</code></pre>"},{"location":"api/search_api/faiss_adapter/","title":"<code>search_api.faiss_adapter</code>","text":"<p>Module for search_api.faiss_adapter.</p>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.DenseVecs","title":"<code>DenseVecs</code>  <code>dataclass</code>","text":"<p>Densevecs.</p> Source code in <code>src/search_api/faiss_adapter.py</code> <pre><code>@dataclass\nclass DenseVecs:\n    \"\"\"Densevecs.\"\"\"\n\n    ids: list[str]\n    mat: np.ndarray\n</code></pre>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter","title":"<code>FaissAdapter</code>","text":"<p>Faissadapter.</p> Source code in <code>src/search_api/faiss_adapter.py</code> <pre><code>class FaissAdapter:\n    \"\"\"Faissadapter.\"\"\"\n\n    def __init__(self, db_path: str, factory: str = \"OPQ64,IVF8192,PQ64\", metric: str = \"ip\"):\n        \"\"\"Init.\n\n        Args:\n            db_path (str): TODO.\n            factory (str): TODO.\n            metric (str): TODO.\n        \"\"\"\n        self.db_path = db_path\n        self.factory = factory\n        self.metric = metric\n        self.index: Any | None = None\n        self.idmap: list[str] | None = None\n        self.vecs: DenseVecs | None = None\n\n    def _load_dense_parquet(self) -&gt; DenseVecs:\n        \"\"\"Load dense parquet.\n\n        Returns:\n            DenseVecs: TODO.\n        \"\"\"\n        if not Path(self.db_path).exists():\n            raise RuntimeError(\"DuckDB registry not found\")\n        con = duckdb.connect(self.db_path)\n        try:\n            dr = con.execute(\n                \"SELECT parquet_root, dim FROM dense_runs ORDER BY created_at DESC LIMIT 1\"\n            ).fetchone()\n            if not dr:\n                raise RuntimeError(\"No dense_runs found\")\n            root = dr[0]\n            rows = con.execute(\n                f\"\"\"\n              SELECT chunk_id, vector FROM read_parquet('{root}/*/*.parquet', union_by_name=true)\n            \"\"\"\n            ).fetchall()\n        finally:\n            con.close()\n        ids = [r[0] for r in rows]\n        mat = np.stack([np.array(r[1], dtype=np.float32) for r in rows])\n        # normalize for cosine/IP\n        norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-9\n        mat = mat / norms\n        return DenseVecs(ids=ids, mat=mat)\n\n    def build(self) -&gt; None:\n        \"\"\"Build.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        vecs = self._load_dense_parquet()\n        self.vecs = vecs\n        if not HAVE_FAISS:\n            return\n        d = vecs.mat.shape[1]\n        metric = faiss.METRIC_INNER_PRODUCT if self.metric == \"ip\" else faiss.METRIC_L2\n        cpu = faiss.index_factory(d, self.factory, metric)\n        cpu = faiss.IndexIDMap2(cpu)\n        train = vecs.mat[: min(100000, vecs.mat.shape[0])].copy()\n        faiss.normalize_L2(train)\n        cpu.train(train)\n        ids64 = np.arange(vecs.mat.shape[0], dtype=np.int64)\n        cpu.add_with_ids(vecs.mat, ids64)\n        res = faiss.StandardGpuResources()\n        co = faiss.GpuClonerOptions()\n        co.use_cuvs = True\n        try:\n            self.index = faiss.index_cpu_to_gpu(res, 0, cpu, co)\n        except Exception:\n            co.use_cuvs = False\n            self.index = faiss.index_cpu_to_gpu(res, 0, cpu, co)\n        self.idmap = vecs.ids\n\n    def load_or_build(self, cpu_index_path: str | None = None) -&gt; None:\n        \"\"\"Load or build.\n\n        Args:\n            cpu_index_path (Optional[str]): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        try:\n            if HAVE_FAISS and cpu_index_path and Path(cpu_index_path).exists():\n                cpu = faiss.read_index(cpu_index_path)\n                res = faiss.StandardGpuResources()\n                co = faiss.GpuClonerOptions()\n                co.use_cuvs = True\n                try:\n                    self.index = faiss.index_cpu_to_gpu(res, 0, cpu, co)\n                except Exception:\n                    co.use_cuvs = False\n                    self.index = faiss.index_cpu_to_gpu(res, 0, cpu, co)\n                dv = self._load_dense_parquet()\n                self.vecs = dv\n                self.idmap = dv.ids\n                return\n        except Exception:\n            pass\n        self.build()\n\n    def search(self, qvec: np.ndarray, k: int = 10) -&gt; list[tuple[str, float]]:\n        \"\"\"Search.\n\n        Args:\n            qvec (np.ndarray): TODO.\n            k (int): TODO.\n\n        Returns:\n            List[Tuple[str, float]]: TODO.\n        \"\"\"\n        if self.vecs is None and self.index is None:\n            return []\n        if HAVE_FAISS and self.index is not None:\n            if self.idmap is None:\n                raise RuntimeError(\"ID mapping not loaded for FAISS index\")\n            q = qvec[None, :].astype(np.float32, copy=False)\n            distances, indices = self.index.search(q, k)\n            out = []\n            for idx, score in zip(indices[0], distances[0], strict=False):\n                if idx &lt; 0:\n                    continue\n                out.append((self.idmap[int(idx)], float(score)))\n            return out\n        # numpy brute force\n        if self.vecs is None:\n            raise RuntimeError(\"Dense vectors not loaded\")\n        mat = self.vecs.mat\n        q = qvec.astype(np.float32, copy=False)\n        q /= np.linalg.norm(q) + 1e-9\n        sims = mat @ q\n        topk = np.argsort(-sims)[:k]\n        ids = self.vecs.ids\n        return [(ids[i], float(sims[i])) for i in topk]\n</code></pre>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.__init__","title":"<code>__init__(db_path, factory='OPQ64,IVF8192,PQ64', metric='ip')</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>TODO.</p> required <code>factory</code> <code>str</code> <p>TODO.</p> <code>'OPQ64,IVF8192,PQ64'</code> <code>metric</code> <code>str</code> <p>TODO.</p> <code>'ip'</code> Source code in <code>src/search_api/faiss_adapter.py</code> <pre><code>def __init__(self, db_path: str, factory: str = \"OPQ64,IVF8192,PQ64\", metric: str = \"ip\"):\n    \"\"\"Init.\n\n    Args:\n        db_path (str): TODO.\n        factory (str): TODO.\n        metric (str): TODO.\n    \"\"\"\n    self.db_path = db_path\n    self.factory = factory\n    self.metric = metric\n    self.index: Any | None = None\n    self.idmap: list[str] | None = None\n    self.vecs: DenseVecs | None = None\n</code></pre>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.build","title":"<code>build()</code>","text":"<p>Build.</p> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/search_api/faiss_adapter.py</code> <pre><code>def build(self) -&gt; None:\n    \"\"\"Build.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    vecs = self._load_dense_parquet()\n    self.vecs = vecs\n    if not HAVE_FAISS:\n        return\n    d = vecs.mat.shape[1]\n    metric = faiss.METRIC_INNER_PRODUCT if self.metric == \"ip\" else faiss.METRIC_L2\n    cpu = faiss.index_factory(d, self.factory, metric)\n    cpu = faiss.IndexIDMap2(cpu)\n    train = vecs.mat[: min(100000, vecs.mat.shape[0])].copy()\n    faiss.normalize_L2(train)\n    cpu.train(train)\n    ids64 = np.arange(vecs.mat.shape[0], dtype=np.int64)\n    cpu.add_with_ids(vecs.mat, ids64)\n    res = faiss.StandardGpuResources()\n    co = faiss.GpuClonerOptions()\n    co.use_cuvs = True\n    try:\n        self.index = faiss.index_cpu_to_gpu(res, 0, cpu, co)\n    except Exception:\n        co.use_cuvs = False\n        self.index = faiss.index_cpu_to_gpu(res, 0, cpu, co)\n    self.idmap = vecs.ids\n</code></pre>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.load_or_build","title":"<code>load_or_build(cpu_index_path=None)</code>","text":"<p>Load or build.</p> <p>Parameters:</p> Name Type Description Default <code>cpu_index_path</code> <code>Optional[str]</code> <p>TODO.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/search_api/faiss_adapter.py</code> <pre><code>def load_or_build(self, cpu_index_path: str | None = None) -&gt; None:\n    \"\"\"Load or build.\n\n    Args:\n        cpu_index_path (Optional[str]): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    try:\n        if HAVE_FAISS and cpu_index_path and Path(cpu_index_path).exists():\n            cpu = faiss.read_index(cpu_index_path)\n            res = faiss.StandardGpuResources()\n            co = faiss.GpuClonerOptions()\n            co.use_cuvs = True\n            try:\n                self.index = faiss.index_cpu_to_gpu(res, 0, cpu, co)\n            except Exception:\n                co.use_cuvs = False\n                self.index = faiss.index_cpu_to_gpu(res, 0, cpu, co)\n            dv = self._load_dense_parquet()\n            self.vecs = dv\n            self.idmap = dv.ids\n            return\n    except Exception:\n        pass\n    self.build()\n</code></pre>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.search","title":"<code>search(qvec, k=10)</code>","text":"<p>Search.</p> <p>Parameters:</p> Name Type Description Default <code>qvec</code> <code>ndarray</code> <p>TODO.</p> required <code>k</code> <code>int</code> <p>TODO.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[tuple[str, float]]</code> <p>List[Tuple[str, float]]: TODO.</p> Source code in <code>src/search_api/faiss_adapter.py</code> <pre><code>def search(self, qvec: np.ndarray, k: int = 10) -&gt; list[tuple[str, float]]:\n    \"\"\"Search.\n\n    Args:\n        qvec (np.ndarray): TODO.\n        k (int): TODO.\n\n    Returns:\n        List[Tuple[str, float]]: TODO.\n    \"\"\"\n    if self.vecs is None and self.index is None:\n        return []\n    if HAVE_FAISS and self.index is not None:\n        if self.idmap is None:\n            raise RuntimeError(\"ID mapping not loaded for FAISS index\")\n        q = qvec[None, :].astype(np.float32, copy=False)\n        distances, indices = self.index.search(q, k)\n        out = []\n        for idx, score in zip(indices[0], distances[0], strict=False):\n            if idx &lt; 0:\n                continue\n            out.append((self.idmap[int(idx)], float(score)))\n        return out\n    # numpy brute force\n    if self.vecs is None:\n        raise RuntimeError(\"Dense vectors not loaded\")\n    mat = self.vecs.mat\n    q = qvec.astype(np.float32, copy=False)\n    q /= np.linalg.norm(q) + 1e-9\n    sims = mat @ q\n    topk = np.argsort(-sims)[:k]\n    ids = self.vecs.ids\n    return [(ids[i], float(sims[i])) for i in topk]\n</code></pre>"},{"location":"api/search_api/fixture_index/","title":"<code>search_api.fixture_index</code>","text":"<p>Module for search_api.fixture_index.</p>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureDoc","title":"<code>FixtureDoc</code>  <code>dataclass</code>","text":"<p>Fixturedoc.</p> Source code in <code>src/search_api/fixture_index.py</code> <pre><code>@dataclass\nclass FixtureDoc:\n    \"\"\"Fixturedoc.\"\"\"\n\n    chunk_id: str\n    doc_id: str\n    title: str\n    section: str\n    text: str\n</code></pre>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex","title":"<code>FixtureIndex</code>","text":"<p>Fixtureindex.</p> Source code in <code>src/search_api/fixture_index.py</code> <pre><code>class FixtureIndex:\n    \"\"\"Fixtureindex.\"\"\"\n\n    def __init__(self, root: str = \"/data\", db_path: str = \"/data/catalog/catalog.duckdb\"):\n        \"\"\"Init.\n\n        Args:\n            root (str): TODO.\n            db_path (str): TODO.\n        \"\"\"\n        self.root = Path(root)\n        self.db_path = db_path\n        self.docs: list[FixtureDoc] = []\n        self.df: dict[str, int] = {}\n        self.tf: list[dict[str, int]] = []\n        self._load_from_duckdb()\n\n    def _load_from_duckdb(self) -&gt; None:\n        \"\"\"Load from duckdb.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        if not Path(self.db_path).exists():\n            return\n        con = duckdb.connect(self.db_path)\n        try:\n            ds = con.execute(\n                \"\"\"\n              SELECT parquet_root FROM datasets\n              WHERE kind='chunks'\n              ORDER BY created_at DESC\n              LIMIT 1\n            \"\"\"\n            ).fetchone()\n            if not ds:\n                return\n            root = ds[0]\n            rows = con.execute(\n                f\"\"\"\n                SELECT c.chunk_id, c.doc_id, coalesce(c.section,''), c.text,\n                       coalesce(d.title,'') AS title\n                FROM read_parquet('{root}/*/*.parquet', union_by_name=true) AS c\n                LEFT JOIN documents d ON c.doc_id = d.doc_id\n            \"\"\"\n            ).fetchall()\n        finally:\n            con.close()\n\n        for chunk_id, doc_id, section, text, title in rows:\n            self.docs.append(\n                FixtureDoc(\n                    chunk_id=chunk_id,\n                    doc_id=doc_id or \"urn:doc:fixture\",\n                    title=title or \"Fixture\",\n                    section=section or \"\",\n                    text=text or \"\",\n                )\n            )\n\n        self._build_lex()\n\n    def _build_lex(self) -&gt; None:\n        \"\"\"Build lex.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        self.tf.clear()\n        self.df.clear()\n        for doc in self.docs:\n            toks = tokenize(doc.text)\n            tf_counts: dict[str, int] = {}\n            for t in toks:\n                tf_counts[t] = tf_counts.get(t, 0) + 1\n            self.tf.append(tf_counts)\n            for t in set(toks):\n                self.df[t] = self.df.get(t, 0) + 1\n        self.N = len(self.docs)\n\n    def search(self, query: str, k: int = 10) -&gt; list[tuple[int, float]]:\n        \"\"\"Search.\n\n        Args:\n            query (str): TODO.\n            k (int): TODO.\n\n        Returns:\n            List[Tuple[int, float]]: TODO.\n        \"\"\"\n        if getattr(self, \"N\", 0) == 0:\n            return []\n        qtoks = tokenize(query)\n        if not qtoks:\n            return []\n        scores = [0.0] * self.N\n        for i, tf in enumerate(self.tf):\n            s = 0.0\n            for t in qtoks:\n                if t not in self.df:\n                    continue\n                idf = math.log((self.N + 1) / (self.df[t] + 0.5) + 1.0)\n                s += idf * tf.get(t, 0)\n            scores[i] = s\n        ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n        return [(i, s) for i, s in ranked[:k] if s &gt; 0.0]\n\n    def doc(self, idx: int) -&gt; FixtureDoc:\n        \"\"\"Doc.\n\n        Args:\n            idx (int): TODO.\n\n        Returns:\n            FixtureDoc: TODO.\n        \"\"\"\n        return self.docs[idx]\n</code></pre>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex.__init__","title":"<code>__init__(root='/data', db_path='/data/catalog/catalog.duckdb')</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>TODO.</p> <code>'/data'</code> <code>db_path</code> <code>str</code> <p>TODO.</p> <code>'/data/catalog/catalog.duckdb'</code> Source code in <code>src/search_api/fixture_index.py</code> <pre><code>def __init__(self, root: str = \"/data\", db_path: str = \"/data/catalog/catalog.duckdb\"):\n    \"\"\"Init.\n\n    Args:\n        root (str): TODO.\n        db_path (str): TODO.\n    \"\"\"\n    self.root = Path(root)\n    self.db_path = db_path\n    self.docs: list[FixtureDoc] = []\n    self.df: dict[str, int] = {}\n    self.tf: list[dict[str, int]] = []\n    self._load_from_duckdb()\n</code></pre>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex.doc","title":"<code>doc(idx)</code>","text":"<p>Doc.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>FixtureDoc</code> <code>FixtureDoc</code> <p>TODO.</p> Source code in <code>src/search_api/fixture_index.py</code> <pre><code>def doc(self, idx: int) -&gt; FixtureDoc:\n    \"\"\"Doc.\n\n    Args:\n        idx (int): TODO.\n\n    Returns:\n        FixtureDoc: TODO.\n    \"\"\"\n    return self.docs[idx]\n</code></pre>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex.search","title":"<code>search(query, k=10)</code>","text":"<p>Search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>TODO.</p> required <code>k</code> <code>int</code> <p>TODO.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[tuple[int, float]]</code> <p>List[Tuple[int, float]]: TODO.</p> Source code in <code>src/search_api/fixture_index.py</code> <pre><code>def search(self, query: str, k: int = 10) -&gt; list[tuple[int, float]]:\n    \"\"\"Search.\n\n    Args:\n        query (str): TODO.\n        k (int): TODO.\n\n    Returns:\n        List[Tuple[int, float]]: TODO.\n    \"\"\"\n    if getattr(self, \"N\", 0) == 0:\n        return []\n    qtoks = tokenize(query)\n    if not qtoks:\n        return []\n    scores = [0.0] * self.N\n    for i, tf in enumerate(self.tf):\n        s = 0.0\n        for t in qtoks:\n            if t not in self.df:\n                continue\n            idf = math.log((self.N + 1) / (self.df[t] + 0.5) + 1.0)\n            s += idf * tf.get(t, 0)\n        scores[i] = s\n    ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n    return [(i, s) for i, s in ranked[:k] if s &gt; 0.0]\n</code></pre>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.tokenize","title":"<code>tokenize(text)</code>","text":"<p>Tokenize.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: TODO.</p> Source code in <code>src/search_api/fixture_index.py</code> <pre><code>def tokenize(text: str) -&gt; list[str]:\n    \"\"\"Tokenize.\n\n    Args:\n        text (str): TODO.\n\n    Returns:\n        List[str]: TODO.\n    \"\"\"\n    return [t.lower() for t in TOKEN_RE.findall(text or \"\")]\n</code></pre>"},{"location":"api/search_api/fusion/","title":"<code>search_api.fusion</code>","text":"<p>Module for search_api.fusion.</p>"},{"location":"api/search_api/fusion/#search_api.fusion.rrf_fuse","title":"<code>rrf_fuse(rankers, k=60)</code>","text":"<p>Rrf fuse.</p> <p>Parameters:</p> Name Type Description Default <code>rankers</code> <code>List[List[Tuple[str, float]]]</code> <p>TODO.</p> required <code>k</code> <code>int</code> <p>TODO.</p> <code>60</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dict[str, float]: TODO.</p> Source code in <code>src/search_api/fusion.py</code> <pre><code>def rrf_fuse(rankers: list[list[tuple[str, float]]], k: int = 60) -&gt; dict[str, float]:\n    \"\"\"Rrf fuse.\n\n    Args:\n        rankers (List[List[Tuple[str, float]]]): TODO.\n        k (int): TODO.\n\n    Returns:\n        Dict[str, float]: TODO.\n    \"\"\"\n    agg: dict[str, float] = {}\n    for ranked in rankers:\n        for r, (key, _score) in enumerate(ranked, start=1):\n            agg[key] = agg.get(key, 0.0) + 1.0 / (k + r)\n    return agg\n</code></pre>"},{"location":"api/search_api/kg_mock/","title":"<code>search_api.kg_mock</code>","text":"<p>Module for search_api.kg_mock.</p>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.detect_query_concepts","title":"<code>detect_query_concepts(query)</code>","text":"<p>Detect query concepts.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: TODO.</p> Source code in <code>src/search_api/kg_mock.py</code> <pre><code>def detect_query_concepts(query: str) -&gt; list[str]:\n    \"\"\"Detect query concepts.\n\n    Args:\n        query (str): TODO.\n\n    Returns:\n        List[str]: TODO.\n    \"\"\"\n    q = query.lower()\n    hits = []\n    for cid, meta in _CONCEPTS.items():\n        if any(kw in q for kw in meta[\"keywords\"]):\n            hits.append(cid)\n    return hits\n</code></pre>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.kg_boost","title":"<code>kg_boost(query_concepts, chunk_concepts, direct=0.08, one_hop=0.04)</code>","text":"<p>Kg boost.</p> <p>Parameters:</p> Name Type Description Default <code>query_concepts</code> <code>List[str]</code> <p>TODO.</p> required <code>chunk_concepts</code> <code>List[str]</code> <p>TODO.</p> required <code>direct</code> <code>float</code> <p>TODO.</p> <code>0.08</code> <code>one_hop</code> <code>float</code> <p>TODO.</p> <code>0.04</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>TODO.</p> Source code in <code>src/search_api/kg_mock.py</code> <pre><code>def kg_boost(\n    query_concepts: list[str],\n    chunk_concepts: list[str],\n    direct: float = 0.08,\n    one_hop: float = 0.04,\n) -&gt; float:\n    \"\"\"Kg boost.\n\n    Args:\n        query_concepts (List[str]): TODO.\n        chunk_concepts (List[str]): TODO.\n        direct (float): TODO.\n        one_hop (float): TODO.\n\n    Returns:\n        float: TODO.\n    \"\"\"\n    return direct if set(query_concepts) &amp; set(chunk_concepts) else 0.0\n</code></pre>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.linked_concepts_for_text","title":"<code>linked_concepts_for_text(text)</code>","text":"<p>Linked concepts for text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: TODO.</p> Source code in <code>src/search_api/kg_mock.py</code> <pre><code>def linked_concepts_for_text(text: str) -&gt; list[str]:\n    \"\"\"Linked concepts for text.\n\n    Args:\n        text (str): TODO.\n\n    Returns:\n        List[str]: TODO.\n    \"\"\"\n    t = text.lower()\n    hits = []\n    for cid, meta in _CONCEPTS.items():\n        if any(kw in t for kw in meta[\"keywords\"]):\n            hits.append(cid)\n    return hits\n</code></pre>"},{"location":"api/search_api/schemas/","title":"<code>search_api.schemas</code>","text":"<p>Module for search_api.schemas.</p>"},{"location":"api/search_api/schemas/#search_api.schemas.SearchRequest","title":"<code>SearchRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Searchrequest.</p> Source code in <code>src/search_api/schemas.py</code> <pre><code>class SearchRequest(BaseModel):\n    \"\"\"Searchrequest.\"\"\"\n\n    query: str = Field(min_length=1)\n    k: int = 10\n    filters: dict[str, object] | None = None\n    explain: bool = False\n</code></pre>"},{"location":"api/search_api/schemas/#search_api.schemas.SearchResult","title":"<code>SearchResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Searchresult.</p> Source code in <code>src/search_api/schemas.py</code> <pre><code>class SearchResult(BaseModel):\n    \"\"\"Searchresult.\"\"\"\n\n    doc_id: str\n    chunk_id: str\n    title: str\n    section: str\n    score: float\n    signals: dict[str, float] = {}\n    spans: dict[str, int] = {}\n    concepts: list[dict[str, str]] = []\n</code></pre>"},{"location":"api/search_api/service/","title":"<code>search_api.service</code>","text":"<p>Module for search_api.service.</p>"},{"location":"api/search_api/service/#search_api.service.apply_kg_boosts","title":"<code>apply_kg_boosts(fused, query)</code>","text":"<p>Apply kg boosts.</p> <p>Parameters:</p> Name Type Description Default <code>fused</code> <code>List[Tuple[str, float]]</code> <p>TODO.</p> required <code>query</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, float]]</code> <p>List[Tuple[str, float]]: TODO.</p> Source code in <code>src/search_api/service.py</code> <pre><code>def apply_kg_boosts(fused: list[tuple[str, float]], query: str) -&gt; list[tuple[str, float]]:\n    \"\"\"Apply kg boosts.\n\n    Args:\n        fused (List[Tuple[str, float]]): TODO.\n        query (str): TODO.\n\n    Returns:\n        List[Tuple[str, float]]: TODO.\n    \"\"\"\n    # TODO: apply boosts for direct &amp; one-hop concept matches\n    return fused\n</code></pre>"},{"location":"api/search_api/service/#search_api.service.mmr_deduplicate","title":"<code>mmr_deduplicate(results, lambda_=0.7)</code>","text":"<p>Mmr deduplicate.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Tuple[str, float]]</code> <p>TODO.</p> required <code>lambda_</code> <code>float</code> <p>TODO.</p> <code>0.7</code> <p>Returns:</p> Type Description <code>list[tuple[str, float]]</code> <p>List[Tuple[str, float]]: TODO.</p> Source code in <code>src/search_api/service.py</code> <pre><code>def mmr_deduplicate(\n    results: list[tuple[str, float]], lambda_: float = 0.7\n) -&gt; list[tuple[str, float]]:\n    \"\"\"Mmr deduplicate.\n\n    Args:\n        results (List[Tuple[str, float]]): TODO.\n        lambda_ (float): TODO.\n\n    Returns:\n        List[Tuple[str, float]]: TODO.\n    \"\"\"\n    # TODO: diversity via MMR at doc-level\n    return results\n</code></pre>"},{"location":"api/search_api/service/#search_api.service.rrf_fuse","title":"<code>rrf_fuse(dense, sparse, k=60)</code>","text":"<p>Reciprocal Rank Fusion skeleton.</p> Source code in <code>src/search_api/service.py</code> <pre><code>def rrf_fuse(\n    dense: list[tuple[str, float]], sparse: list[tuple[str, float]], k: int = 60\n) -&gt; list[tuple[str, float]]:\n    \"\"\"Reciprocal Rank Fusion skeleton.\"\"\"\n    # TODO: implement stable RRF across rankers\n    return []\n</code></pre>"},{"location":"api/search_api/splade_index/","title":"<code>search_api.splade_index</code>","text":"<p>Module for search_api.splade_index.</p>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeDoc","title":"<code>SpladeDoc</code>  <code>dataclass</code>","text":"<p>Spladedoc.</p> Source code in <code>src/search_api/splade_index.py</code> <pre><code>@dataclass\nclass SpladeDoc:\n    \"\"\"Spladedoc.\"\"\"\n\n    chunk_id: str\n    doc_id: str\n    section: str\n    text: str\n</code></pre>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex","title":"<code>SpladeIndex</code>","text":"<p>Spladeindex.</p> Source code in <code>src/search_api/splade_index.py</code> <pre><code>class SpladeIndex:\n    \"\"\"Spladeindex.\"\"\"\n\n    def __init__(\n        self,\n        db_path: str,\n        chunks_dataset_root: str | None = None,\n        sparse_root: str | None = None,\n    ):\n        \"\"\"Init.\n\n        Args:\n            db_path (str): TODO.\n            chunks_dataset_root (Optional[str]): TODO.\n            sparse_root (Optional[str]): TODO.\n        \"\"\"\n        self.db_path = db_path\n        self.docs: list[SpladeDoc] = []\n        self.df: dict[str, int] = {}\n        self.N = 0\n        self._load(chunks_dataset_root)\n\n    def _load(self, chunks_root: str | None) -&gt; None:\n        \"\"\"Load.\n\n        Args:\n            chunks_root (Optional[str]): TODO.\n        \"\"\"\n        if not Path(self.db_path).exists():\n            return\n        con = duckdb.connect(self.db_path)\n        try:\n            ds = con.execute(\n                \"SELECT parquet_root FROM datasets \"\n                \"WHERE kind='chunks' ORDER BY created_at DESC LIMIT 1\"\n            ).fetchone()\n            if ds:\n                rows = con.execute(\n                    \"SELECT c.chunk_id, c.doc_id, coalesce(c.section,''), c.text \"\n                    f\"FROM read_parquet('{ds[0]}/*/*.parquet', union_by_name=true) AS c\"\n                ).fetchall()\n                for r in rows:\n                    self.docs.append(\n                        SpladeDoc(\n                            chunk_id=r[0], doc_id=r[1] or \"urn:doc:fixture\", section=r[2], text=r[3]\n                        )\n                    )\n        finally:\n            con.close()\n        self.N = len(self.docs)\n        for d in self.docs:\n            for t in set(tok(d.text)):\n                self.df[t] = self.df.get(t, 0) + 1\n\n    def search(self, query: str, k: int = 10) -&gt; list[tuple[int, float]]:\n        \"\"\"Search.\n\n        Args:\n            query (str): TODO.\n            k (int): TODO.\n\n        Returns:\n            List[Tuple[int, float]]: TODO.\n        \"\"\"\n        if self.N == 0:\n            return []\n        q = tok(query)\n        if not q:\n            return []\n        scores = [0.0] * self.N\n        for i, d in enumerate(self.docs):\n            tf: dict[str, int] = {}\n            for t in tok(d.text):\n                tf[t] = tf.get(t, 0) + 1\n            s = 0.0\n            for t in q:\n                if t in self.df:\n                    idf = (self.N + 1) / (self.df[t] + 0.5)\n                    s += (tf.get(t, 0)) * idf\n            scores[i] = s\n        ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n        return [(i, s) for i, s in ranked[:k] if s &gt; 0.0]\n\n    def doc(self, i: int) -&gt; SpladeDoc:\n        \"\"\"Doc.\n\n        Args:\n            i (int): TODO.\n\n        Returns:\n            SpladeDoc: TODO.\n        \"\"\"\n        return self.docs[i]\n</code></pre>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex.__init__","title":"<code>__init__(db_path, chunks_dataset_root=None, sparse_root=None)</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>TODO.</p> required <code>chunks_dataset_root</code> <code>Optional[str]</code> <p>TODO.</p> <code>None</code> <code>sparse_root</code> <code>Optional[str]</code> <p>TODO.</p> <code>None</code> Source code in <code>src/search_api/splade_index.py</code> <pre><code>def __init__(\n    self,\n    db_path: str,\n    chunks_dataset_root: str | None = None,\n    sparse_root: str | None = None,\n):\n    \"\"\"Init.\n\n    Args:\n        db_path (str): TODO.\n        chunks_dataset_root (Optional[str]): TODO.\n        sparse_root (Optional[str]): TODO.\n    \"\"\"\n    self.db_path = db_path\n    self.docs: list[SpladeDoc] = []\n    self.df: dict[str, int] = {}\n    self.N = 0\n    self._load(chunks_dataset_root)\n</code></pre>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex.doc","title":"<code>doc(i)</code>","text":"<p>Doc.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>SpladeDoc</code> <code>SpladeDoc</code> <p>TODO.</p> Source code in <code>src/search_api/splade_index.py</code> <pre><code>def doc(self, i: int) -&gt; SpladeDoc:\n    \"\"\"Doc.\n\n    Args:\n        i (int): TODO.\n\n    Returns:\n        SpladeDoc: TODO.\n    \"\"\"\n    return self.docs[i]\n</code></pre>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex.search","title":"<code>search(query, k=10)</code>","text":"<p>Search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>TODO.</p> required <code>k</code> <code>int</code> <p>TODO.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[tuple[int, float]]</code> <p>List[Tuple[int, float]]: TODO.</p> Source code in <code>src/search_api/splade_index.py</code> <pre><code>def search(self, query: str, k: int = 10) -&gt; list[tuple[int, float]]:\n    \"\"\"Search.\n\n    Args:\n        query (str): TODO.\n        k (int): TODO.\n\n    Returns:\n        List[Tuple[int, float]]: TODO.\n    \"\"\"\n    if self.N == 0:\n        return []\n    q = tok(query)\n    if not q:\n        return []\n    scores = [0.0] * self.N\n    for i, d in enumerate(self.docs):\n        tf: dict[str, int] = {}\n        for t in tok(d.text):\n            tf[t] = tf.get(t, 0) + 1\n        s = 0.0\n        for t in q:\n            if t in self.df:\n                idf = (self.N + 1) / (self.df[t] + 0.5)\n                s += (tf.get(t, 0)) * idf\n        scores[i] = s\n    ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n    return [(i, s) for i, s in ranked[:k] if s &gt; 0.0]\n</code></pre>"},{"location":"api/search_api/splade_index/#search_api.splade_index.tok","title":"<code>tok(s)</code>","text":"<p>Tok.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: TODO.</p> Source code in <code>src/search_api/splade_index.py</code> <pre><code>def tok(s: str) -&gt; list[str]:\n    \"\"\"Tok.\n\n    Args:\n        s (str): TODO.\n\n    Returns:\n        List[str]: TODO.\n    \"\"\"\n    return [t.lower() for t in TOKEN.findall(s or \"\")]\n</code></pre>"},{"location":"api/search_client/","title":"<code>search_client</code>","text":"<p>Module for search_client.init.</p>"},{"location":"api/search_client/#search_client.KGForgeClient","title":"<code>KGForgeClient</code>","text":"<p>Kgforgeclient.</p> Source code in <code>src/search_client/client.py</code> <pre><code>class KGForgeClient:\n    \"\"\"Kgforgeclient.\"\"\"\n\n    def __init__(\n        self,\n        base_url: str = \"http://localhost:8080\",\n        api_key: str | None = None,\n        timeout: float = 30.0,\n    ):\n        \"\"\"Init.\n\n        Args:\n            base_url (str): TODO.\n            api_key (Optional[str]): TODO.\n            timeout (float): TODO.\n        \"\"\"\n        self.base_url = base_url.rstrip(\"/\")\n        self.api_key = api_key\n        self.timeout = timeout\n\n    def _headers(self) -&gt; dict[str, str]:\n        \"\"\"Headers.\n\n        Returns:\n            Dict[str, str]: TODO.\n        \"\"\"\n        h = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            h[\"Authorization\"] = f\"Bearer {self.api_key}\"\n        return h\n\n    def healthz(self) -&gt; dict[str, Any]:\n        \"\"\"Healthz.\n\n        Returns:\n            Dict[str, Any]: TODO.\n        \"\"\"\n        r = requests.get(f\"{self.base_url}/healthz\", timeout=self.timeout)\n        r.raise_for_status()\n        return r.json()\n\n    def search(\n        self,\n        query: str,\n        k: int = 10,\n        filters: dict[str, Any] | None = None,\n        explain: bool = False,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Search.\n\n        Args:\n            query (str): TODO.\n            k (int): TODO.\n            filters (Optional[Dict[str, Any]]): TODO.\n            explain (bool): TODO.\n\n        Returns:\n            Dict[str, Any]: TODO.\n        \"\"\"\n        payload = {\"query\": query, \"k\": k, \"filters\": filters or {}, \"explain\": explain}\n        r = requests.post(\n            f\"{self.base_url}/search\", json=payload, headers=self._headers(), timeout=self.timeout\n        )\n        r.raise_for_status()\n        return r.json()\n\n    def concepts(self, q: str, limit: int = 50) -&gt; dict[str, Any]:\n        \"\"\"Concepts.\n\n        Args:\n            q (str): TODO.\n            limit (int): TODO.\n\n        Returns:\n            Dict[str, Any]: TODO.\n        \"\"\"\n        r = requests.post(\n            f\"{self.base_url}/graph/concepts\",\n            json={\"q\": q, \"limit\": limit},\n            headers=self._headers(),\n            timeout=self.timeout,\n        )\n        r.raise_for_status()\n        return r.json()\n</code></pre>"},{"location":"api/search_client/#search_client.KGForgeClient.__init__","title":"<code>__init__(base_url='http://localhost:8080', api_key=None, timeout=30.0)</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>TODO.</p> <code>'http://localhost:8080'</code> <code>api_key</code> <code>Optional[str]</code> <p>TODO.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>TODO.</p> <code>30.0</code> Source code in <code>src/search_client/client.py</code> <pre><code>def __init__(\n    self,\n    base_url: str = \"http://localhost:8080\",\n    api_key: str | None = None,\n    timeout: float = 30.0,\n):\n    \"\"\"Init.\n\n    Args:\n        base_url (str): TODO.\n        api_key (Optional[str]): TODO.\n        timeout (float): TODO.\n    \"\"\"\n    self.base_url = base_url.rstrip(\"/\")\n    self.api_key = api_key\n    self.timeout = timeout\n</code></pre>"},{"location":"api/search_client/#search_client.KGForgeClient.concepts","title":"<code>concepts(q, limit=50)</code>","text":"<p>Concepts.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>str</code> <p>TODO.</p> required <code>limit</code> <code>int</code> <p>TODO.</p> <code>50</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: TODO.</p> Source code in <code>src/search_client/client.py</code> <pre><code>def concepts(self, q: str, limit: int = 50) -&gt; dict[str, Any]:\n    \"\"\"Concepts.\n\n    Args:\n        q (str): TODO.\n        limit (int): TODO.\n\n    Returns:\n        Dict[str, Any]: TODO.\n    \"\"\"\n    r = requests.post(\n        f\"{self.base_url}/graph/concepts\",\n        json={\"q\": q, \"limit\": limit},\n        headers=self._headers(),\n        timeout=self.timeout,\n    )\n    r.raise_for_status()\n    return r.json()\n</code></pre>"},{"location":"api/search_client/#search_client.KGForgeClient.healthz","title":"<code>healthz()</code>","text":"<p>Healthz.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: TODO.</p> Source code in <code>src/search_client/client.py</code> <pre><code>def healthz(self) -&gt; dict[str, Any]:\n    \"\"\"Healthz.\n\n    Returns:\n        Dict[str, Any]: TODO.\n    \"\"\"\n    r = requests.get(f\"{self.base_url}/healthz\", timeout=self.timeout)\n    r.raise_for_status()\n    return r.json()\n</code></pre>"},{"location":"api/search_client/#search_client.KGForgeClient.search","title":"<code>search(query, k=10, filters=None, explain=False)</code>","text":"<p>Search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>TODO.</p> required <code>k</code> <code>int</code> <p>TODO.</p> <code>10</code> <code>filters</code> <code>Optional[Dict[str, Any]]</code> <p>TODO.</p> <code>None</code> <code>explain</code> <code>bool</code> <p>TODO.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: TODO.</p> Source code in <code>src/search_client/client.py</code> <pre><code>def search(\n    self,\n    query: str,\n    k: int = 10,\n    filters: dict[str, Any] | None = None,\n    explain: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Search.\n\n    Args:\n        query (str): TODO.\n        k (int): TODO.\n        filters (Optional[Dict[str, Any]]): TODO.\n        explain (bool): TODO.\n\n    Returns:\n        Dict[str, Any]: TODO.\n    \"\"\"\n    payload = {\"query\": query, \"k\": k, \"filters\": filters or {}, \"explain\": explain}\n    r = requests.post(\n        f\"{self.base_url}/search\", json=payload, headers=self._headers(), timeout=self.timeout\n    )\n    r.raise_for_status()\n    return r.json()\n</code></pre>"},{"location":"api/search_client/client/","title":"<code>search_client.client</code>","text":"<p>Module for search_client.client.</p>"},{"location":"api/search_client/client/#search_client.client.KGForgeClient","title":"<code>KGForgeClient</code>","text":"<p>Kgforgeclient.</p> Source code in <code>src/search_client/client.py</code> <pre><code>class KGForgeClient:\n    \"\"\"Kgforgeclient.\"\"\"\n\n    def __init__(\n        self,\n        base_url: str = \"http://localhost:8080\",\n        api_key: str | None = None,\n        timeout: float = 30.0,\n    ):\n        \"\"\"Init.\n\n        Args:\n            base_url (str): TODO.\n            api_key (Optional[str]): TODO.\n            timeout (float): TODO.\n        \"\"\"\n        self.base_url = base_url.rstrip(\"/\")\n        self.api_key = api_key\n        self.timeout = timeout\n\n    def _headers(self) -&gt; dict[str, str]:\n        \"\"\"Headers.\n\n        Returns:\n            Dict[str, str]: TODO.\n        \"\"\"\n        h = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            h[\"Authorization\"] = f\"Bearer {self.api_key}\"\n        return h\n\n    def healthz(self) -&gt; dict[str, Any]:\n        \"\"\"Healthz.\n\n        Returns:\n            Dict[str, Any]: TODO.\n        \"\"\"\n        r = requests.get(f\"{self.base_url}/healthz\", timeout=self.timeout)\n        r.raise_for_status()\n        return r.json()\n\n    def search(\n        self,\n        query: str,\n        k: int = 10,\n        filters: dict[str, Any] | None = None,\n        explain: bool = False,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Search.\n\n        Args:\n            query (str): TODO.\n            k (int): TODO.\n            filters (Optional[Dict[str, Any]]): TODO.\n            explain (bool): TODO.\n\n        Returns:\n            Dict[str, Any]: TODO.\n        \"\"\"\n        payload = {\"query\": query, \"k\": k, \"filters\": filters or {}, \"explain\": explain}\n        r = requests.post(\n            f\"{self.base_url}/search\", json=payload, headers=self._headers(), timeout=self.timeout\n        )\n        r.raise_for_status()\n        return r.json()\n\n    def concepts(self, q: str, limit: int = 50) -&gt; dict[str, Any]:\n        \"\"\"Concepts.\n\n        Args:\n            q (str): TODO.\n            limit (int): TODO.\n\n        Returns:\n            Dict[str, Any]: TODO.\n        \"\"\"\n        r = requests.post(\n            f\"{self.base_url}/graph/concepts\",\n            json={\"q\": q, \"limit\": limit},\n            headers=self._headers(),\n            timeout=self.timeout,\n        )\n        r.raise_for_status()\n        return r.json()\n</code></pre>"},{"location":"api/search_client/client/#search_client.client.KGForgeClient.__init__","title":"<code>__init__(base_url='http://localhost:8080', api_key=None, timeout=30.0)</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>TODO.</p> <code>'http://localhost:8080'</code> <code>api_key</code> <code>Optional[str]</code> <p>TODO.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>TODO.</p> <code>30.0</code> Source code in <code>src/search_client/client.py</code> <pre><code>def __init__(\n    self,\n    base_url: str = \"http://localhost:8080\",\n    api_key: str | None = None,\n    timeout: float = 30.0,\n):\n    \"\"\"Init.\n\n    Args:\n        base_url (str): TODO.\n        api_key (Optional[str]): TODO.\n        timeout (float): TODO.\n    \"\"\"\n    self.base_url = base_url.rstrip(\"/\")\n    self.api_key = api_key\n    self.timeout = timeout\n</code></pre>"},{"location":"api/search_client/client/#search_client.client.KGForgeClient.concepts","title":"<code>concepts(q, limit=50)</code>","text":"<p>Concepts.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>str</code> <p>TODO.</p> required <code>limit</code> <code>int</code> <p>TODO.</p> <code>50</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: TODO.</p> Source code in <code>src/search_client/client.py</code> <pre><code>def concepts(self, q: str, limit: int = 50) -&gt; dict[str, Any]:\n    \"\"\"Concepts.\n\n    Args:\n        q (str): TODO.\n        limit (int): TODO.\n\n    Returns:\n        Dict[str, Any]: TODO.\n    \"\"\"\n    r = requests.post(\n        f\"{self.base_url}/graph/concepts\",\n        json={\"q\": q, \"limit\": limit},\n        headers=self._headers(),\n        timeout=self.timeout,\n    )\n    r.raise_for_status()\n    return r.json()\n</code></pre>"},{"location":"api/search_client/client/#search_client.client.KGForgeClient.healthz","title":"<code>healthz()</code>","text":"<p>Healthz.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: TODO.</p> Source code in <code>src/search_client/client.py</code> <pre><code>def healthz(self) -&gt; dict[str, Any]:\n    \"\"\"Healthz.\n\n    Returns:\n        Dict[str, Any]: TODO.\n    \"\"\"\n    r = requests.get(f\"{self.base_url}/healthz\", timeout=self.timeout)\n    r.raise_for_status()\n    return r.json()\n</code></pre>"},{"location":"api/search_client/client/#search_client.client.KGForgeClient.search","title":"<code>search(query, k=10, filters=None, explain=False)</code>","text":"<p>Search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>TODO.</p> required <code>k</code> <code>int</code> <p>TODO.</p> <code>10</code> <code>filters</code> <code>Optional[Dict[str, Any]]</code> <p>TODO.</p> <code>None</code> <code>explain</code> <code>bool</code> <p>TODO.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: TODO.</p> Source code in <code>src/search_client/client.py</code> <pre><code>def search(\n    self,\n    query: str,\n    k: int = 10,\n    filters: dict[str, Any] | None = None,\n    explain: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Search.\n\n    Args:\n        query (str): TODO.\n        k (int): TODO.\n        filters (Optional[Dict[str, Any]]): TODO.\n        explain (bool): TODO.\n\n    Returns:\n        Dict[str, Any]: TODO.\n    \"\"\"\n    payload = {\"query\": query, \"k\": k, \"filters\": filters or {}, \"explain\": explain}\n    r = requests.post(\n        f\"{self.base_url}/search\", json=payload, headers=self._headers(), timeout=self.timeout\n    )\n    r.raise_for_status()\n    return r.json()\n</code></pre>"},{"location":"api/vectorstore_faiss/","title":"<code>vectorstore_faiss</code>","text":"<p>Module for vectorstore_faiss.init.</p>"},{"location":"api/vectorstore_faiss/gpu/","title":"<code>vectorstore_faiss.gpu</code>","text":"<p>Module for vectorstore_faiss.gpu.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex","title":"<code>FaissGpuIndex</code>","text":"<p>FAISS GPU/cuVS wrapper.</p> <p>Provides a brute-force cosine search fallback when FAISS is unavailable at runtime.</p> Source code in <code>src/vectorstore_faiss/gpu.py</code> <pre><code>class FaissGpuIndex:\n    \"\"\"FAISS GPU/cuVS wrapper.\n\n    Provides a brute-force cosine search fallback when FAISS is unavailable at runtime.\n    \"\"\"\n\n    def __init__(\n        self,\n        factory: str = \"OPQ64,IVF8192,PQ64\",\n        nprobe: int = 64,\n        gpu: bool = True,\n        cuvs: bool = True,\n    ):\n        \"\"\"Init.\n\n        Args:\n            factory (str): TODO.\n            nprobe (int): TODO.\n            gpu (bool): TODO.\n            cuvs (bool): TODO.\n        \"\"\"\n        self.factory = factory\n        self.nprobe = nprobe\n        self.gpu = gpu\n        self.cuvs = cuvs\n        self._faiss: Any | None = None\n        self._res: Any | None = None\n        self._index: Any | None = None\n        self._idmap: np.ndarray | None = None\n        self._xb: np.ndarray | None = None  # fallback matrix for brute force\n        # try import faiss\n        try:\n            import faiss\n\n            self._faiss = faiss\n        except Exception:\n            self._faiss = None\n\n    def _ensure_resources(self) -&gt; None:\n        \"\"\"Ensure resources.\"\"\"\n        if not self._faiss or not self.gpu:\n            return\n        if self._res is None:\n            faiss = self._faiss\n            self._res = faiss.StandardGpuResources()\n            # memory knobs can be tuned by caller later\n\n    def train(self, train_vectors: np.ndarray, *, seed: int = 42) -&gt; None:\n        \"\"\"Train.\n\n        Args:\n            train_vectors (np.ndarray): TODO.\n            seed (int): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        if self._faiss is None:\n            # no-op in fallback; brute force doesn't need training\n            return\n        faiss = self._faiss\n        d = train_vectors.shape[1]\n        cpu_index = faiss.index_factory(d, self.factory, faiss.METRIC_INNER_PRODUCT)\n        faiss.normalize_L2(train_vectors)\n        cpu_index.train(train_vectors)\n        self._ensure_resources()\n        if self.gpu:\n            co = faiss.GpuClonerOptions()\n            if hasattr(co, \"use_cuvs\"):\n                co.use_cuvs = bool(self.cuvs)\n            try:\n                self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index, co)\n            except Exception:\n                # fallback without cuVS\n                self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index)\n        else:\n            self._index = cpu_index\n        # set nprobe\n        try:\n            ps = faiss.GpuParameterSpace() if self.gpu else faiss.ParameterSpace()\n            ps.set_index_parameter(self._index, \"nprobe\", self.nprobe)\n        except Exception:\n            pass\n\n    def add(self, keys: list[str], vectors: np.ndarray) -&gt; None:\n        \"\"\"Add.\n\n        Args:\n            keys (List[str]): TODO.\n            vectors (np.ndarray): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        if self._faiss is None:\n            # fallback: keep matrix and id map for brute-force\n            self._xb = vectors.astype(\"float32\", copy=True)\n            self._idmap = np.array(keys)\n            # normalize for cosine/IP\n            norms = np.linalg.norm(self._xb, axis=1, keepdims=True) + 1e-12\n            self._xb /= norms\n            return\n        faiss = self._faiss\n        if self._index is None:\n            raise RuntimeError(\"FAISS index not initialized; call train() before add().\")\n        faiss.normalize_L2(vectors)\n        if isinstance(self._index, faiss.IndexIDMap2):\n            self._index.add_with_ids(vectors, np.array(keys, dtype=\"int64\"))\n        elif hasattr(faiss, \"IndexIDMap2\"):\n            # wrap once\n            idmap = faiss.IndexIDMap2(self._index)\n            idmap.add_with_ids(vectors, np.array(keys, dtype=\"int64\"))\n            self._index = idmap\n        else:\n            self._index.add(vectors)\n\n    def search(self, query: np.ndarray, k: int) -&gt; list[tuple[str, float]]:\n        \"\"\"Search.\n\n        Args:\n            query (np.ndarray): TODO.\n            k (int): TODO.\n\n        Returns:\n            List[Tuple[str, float]]: TODO.\n        \"\"\"\n        q = query.astype(\"float32\", copy=True)\n        # normalize for cosine/IP\n        q /= np.linalg.norm(q, axis=-1, keepdims=True) + 1e-12\n        if self._faiss is None or self._index is None:\n            # brute-force cosine over self._xb\n            if self._xb is None or self._idmap is None:\n                return []\n            sims = (self._xb @ q.T).squeeze()\n            idx = np.argsort(-sims)[:k]\n            return [(str(self._idmap[i]), float(sims[i])) for i in idx.tolist()]\n        if self._idmap is None:\n            raise RuntimeError(\"ID map not loaded; cannot resolve FAISS results.\")\n        distances, indices = self._index.search(q.reshape(1, -1), k)\n        # map IDs to strings if using IDMap; else cast ints\n        ids = indices[0]\n        scores = distances[0]\n        return [(str(ids[i]), float(scores[i])) for i in range(len(ids)) if ids[i] != -1]\n\n    def save(self, index_uri: str, idmap_uri: str) -&gt; None:\n        \"\"\"Save.\n\n        Args:\n            index_uri (str): TODO.\n            idmap_uri (str): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        if self._faiss is None or self._index is None:\n            # save fallback matrix\n            if self._xb is not None and self._idmap is not None:\n                np.savez(index_uri, xb=self._xb, ids=self._idmap)\n            return\n        faiss = self._faiss\n        if faiss is None:\n            raise RuntimeError(\"FAISS not available\")\n        target_index = faiss.index_gpu_to_cpu(self._index) if self.gpu else self._index\n        faiss.write_index(target_index, index_uri)\n\n    def load(self, index_uri: str, idmap_uri: str | None = None) -&gt; None:\n        \"\"\"Load.\n\n        Args:\n            index_uri (str): TODO.\n            idmap_uri (str | None): TODO.\n\n        Returns:\n            None: TODO.\n        \"\"\"\n        if self._faiss is None:\n            # load fallback matrix\n            if os.path.exists(index_uri + \".npz\"):\n                data = np.load(index_uri + \".npz\", allow_pickle=True)\n                self._xb = data[\"xb\"]\n                self._idmap = data[\"ids\"]\n            return\n        faiss = self._faiss\n        if faiss is None:\n            raise RuntimeError(\"FAISS not available\")\n        cpu_index = faiss.read_index(index_uri)\n        self._ensure_resources()\n        if self.gpu:\n            co = faiss.GpuClonerOptions()\n            if hasattr(co, \"use_cuvs\"):\n                co.use_cuvs = bool(self.cuvs)\n            try:\n                self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index, co)\n            except Exception:\n                self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index)\n        else:\n            self._index = cpu_index\n</code></pre>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.__init__","title":"<code>__init__(factory='OPQ64,IVF8192,PQ64', nprobe=64, gpu=True, cuvs=True)</code>","text":"<p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>factory</code> <code>str</code> <p>TODO.</p> <code>'OPQ64,IVF8192,PQ64'</code> <code>nprobe</code> <code>int</code> <p>TODO.</p> <code>64</code> <code>gpu</code> <code>bool</code> <p>TODO.</p> <code>True</code> <code>cuvs</code> <code>bool</code> <p>TODO.</p> <code>True</code> Source code in <code>src/vectorstore_faiss/gpu.py</code> <pre><code>def __init__(\n    self,\n    factory: str = \"OPQ64,IVF8192,PQ64\",\n    nprobe: int = 64,\n    gpu: bool = True,\n    cuvs: bool = True,\n):\n    \"\"\"Init.\n\n    Args:\n        factory (str): TODO.\n        nprobe (int): TODO.\n        gpu (bool): TODO.\n        cuvs (bool): TODO.\n    \"\"\"\n    self.factory = factory\n    self.nprobe = nprobe\n    self.gpu = gpu\n    self.cuvs = cuvs\n    self._faiss: Any | None = None\n    self._res: Any | None = None\n    self._index: Any | None = None\n    self._idmap: np.ndarray | None = None\n    self._xb: np.ndarray | None = None  # fallback matrix for brute force\n    # try import faiss\n    try:\n        import faiss\n\n        self._faiss = faiss\n    except Exception:\n        self._faiss = None\n</code></pre>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.add","title":"<code>add(keys, vectors)</code>","text":"<p>Add.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>List[str]</code> <p>TODO.</p> required <code>vectors</code> <code>ndarray</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/vectorstore_faiss/gpu.py</code> <pre><code>def add(self, keys: list[str], vectors: np.ndarray) -&gt; None:\n    \"\"\"Add.\n\n    Args:\n        keys (List[str]): TODO.\n        vectors (np.ndarray): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    if self._faiss is None:\n        # fallback: keep matrix and id map for brute-force\n        self._xb = vectors.astype(\"float32\", copy=True)\n        self._idmap = np.array(keys)\n        # normalize for cosine/IP\n        norms = np.linalg.norm(self._xb, axis=1, keepdims=True) + 1e-12\n        self._xb /= norms\n        return\n    faiss = self._faiss\n    if self._index is None:\n        raise RuntimeError(\"FAISS index not initialized; call train() before add().\")\n    faiss.normalize_L2(vectors)\n    if isinstance(self._index, faiss.IndexIDMap2):\n        self._index.add_with_ids(vectors, np.array(keys, dtype=\"int64\"))\n    elif hasattr(faiss, \"IndexIDMap2\"):\n        # wrap once\n        idmap = faiss.IndexIDMap2(self._index)\n        idmap.add_with_ids(vectors, np.array(keys, dtype=\"int64\"))\n        self._index = idmap\n    else:\n        self._index.add(vectors)\n</code></pre>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.load","title":"<code>load(index_uri, idmap_uri=None)</code>","text":"<p>Load.</p> <p>Parameters:</p> Name Type Description Default <code>index_uri</code> <code>str</code> <p>TODO.</p> required <code>idmap_uri</code> <code>str | None</code> <p>TODO.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/vectorstore_faiss/gpu.py</code> <pre><code>def load(self, index_uri: str, idmap_uri: str | None = None) -&gt; None:\n    \"\"\"Load.\n\n    Args:\n        index_uri (str): TODO.\n        idmap_uri (str | None): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    if self._faiss is None:\n        # load fallback matrix\n        if os.path.exists(index_uri + \".npz\"):\n            data = np.load(index_uri + \".npz\", allow_pickle=True)\n            self._xb = data[\"xb\"]\n            self._idmap = data[\"ids\"]\n        return\n    faiss = self._faiss\n    if faiss is None:\n        raise RuntimeError(\"FAISS not available\")\n    cpu_index = faiss.read_index(index_uri)\n    self._ensure_resources()\n    if self.gpu:\n        co = faiss.GpuClonerOptions()\n        if hasattr(co, \"use_cuvs\"):\n            co.use_cuvs = bool(self.cuvs)\n        try:\n            self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index, co)\n        except Exception:\n            self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index)\n    else:\n        self._index = cpu_index\n</code></pre>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.save","title":"<code>save(index_uri, idmap_uri)</code>","text":"<p>Save.</p> <p>Parameters:</p> Name Type Description Default <code>index_uri</code> <code>str</code> <p>TODO.</p> required <code>idmap_uri</code> <code>str</code> <p>TODO.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/vectorstore_faiss/gpu.py</code> <pre><code>def save(self, index_uri: str, idmap_uri: str) -&gt; None:\n    \"\"\"Save.\n\n    Args:\n        index_uri (str): TODO.\n        idmap_uri (str): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    if self._faiss is None or self._index is None:\n        # save fallback matrix\n        if self._xb is not None and self._idmap is not None:\n            np.savez(index_uri, xb=self._xb, ids=self._idmap)\n        return\n    faiss = self._faiss\n    if faiss is None:\n        raise RuntimeError(\"FAISS not available\")\n    target_index = faiss.index_gpu_to_cpu(self._index) if self.gpu else self._index\n    faiss.write_index(target_index, index_uri)\n</code></pre>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.search","title":"<code>search(query, k)</code>","text":"<p>Search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>ndarray</code> <p>TODO.</p> required <code>k</code> <code>int</code> <p>TODO.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, float]]</code> <p>List[Tuple[str, float]]: TODO.</p> Source code in <code>src/vectorstore_faiss/gpu.py</code> <pre><code>def search(self, query: np.ndarray, k: int) -&gt; list[tuple[str, float]]:\n    \"\"\"Search.\n\n    Args:\n        query (np.ndarray): TODO.\n        k (int): TODO.\n\n    Returns:\n        List[Tuple[str, float]]: TODO.\n    \"\"\"\n    q = query.astype(\"float32\", copy=True)\n    # normalize for cosine/IP\n    q /= np.linalg.norm(q, axis=-1, keepdims=True) + 1e-12\n    if self._faiss is None or self._index is None:\n        # brute-force cosine over self._xb\n        if self._xb is None or self._idmap is None:\n            return []\n        sims = (self._xb @ q.T).squeeze()\n        idx = np.argsort(-sims)[:k]\n        return [(str(self._idmap[i]), float(sims[i])) for i in idx.tolist()]\n    if self._idmap is None:\n        raise RuntimeError(\"ID map not loaded; cannot resolve FAISS results.\")\n    distances, indices = self._index.search(q.reshape(1, -1), k)\n    # map IDs to strings if using IDMap; else cast ints\n    ids = indices[0]\n    scores = distances[0]\n    return [(str(ids[i]), float(scores[i])) for i in range(len(ids)) if ids[i] != -1]\n</code></pre>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.train","title":"<code>train(train_vectors, *, seed=42)</code>","text":"<p>Train.</p> <p>Parameters:</p> Name Type Description Default <code>train_vectors</code> <code>ndarray</code> <p>TODO.</p> required <code>seed</code> <code>int</code> <p>TODO.</p> <code>42</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>TODO.</p> Source code in <code>src/vectorstore_faiss/gpu.py</code> <pre><code>def train(self, train_vectors: np.ndarray, *, seed: int = 42) -&gt; None:\n    \"\"\"Train.\n\n    Args:\n        train_vectors (np.ndarray): TODO.\n        seed (int): TODO.\n\n    Returns:\n        None: TODO.\n    \"\"\"\n    if self._faiss is None:\n        # no-op in fallback; brute force doesn't need training\n        return\n    faiss = self._faiss\n    d = train_vectors.shape[1]\n    cpu_index = faiss.index_factory(d, self.factory, faiss.METRIC_INNER_PRODUCT)\n    faiss.normalize_L2(train_vectors)\n    cpu_index.train(train_vectors)\n    self._ensure_resources()\n    if self.gpu:\n        co = faiss.GpuClonerOptions()\n        if hasattr(co, \"use_cuvs\"):\n            co.use_cuvs = bool(self.cuvs)\n        try:\n            self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index, co)\n        except Exception:\n            # fallback without cuVS\n            self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index)\n    else:\n        self._index = cpu_index\n    # set nprobe\n    try:\n        ps = faiss.GpuParameterSpace() if self.gpu else faiss.ParameterSpace()\n        ps.set_index_parameter(self._index, \"nprobe\", self.nprobe)\n    except Exception:\n        pass\n</code></pre>"},{"location":"architecture/","title":"Architecture","text":"<p>Architecture decision records and diagrams that govern KGForge.</p> <pre><code>:maxdepth: 1\nadr/0001-record-architecture-decisions\nadr/0002-hexagonal-architecture\n</code></pre> <p>Add diagram markdown files under <code>docs/architecture/diagrams/</code> to surface them here.</p>"},{"location":"architecture/adr/0001-record-architecture-decisions/","title":"0001 \u2013 Record architecture decisions","text":"<p>Date: 2025-10-26</p>"},{"location":"architecture/adr/0001-record-architecture-decisions/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/adr/0001-record-architecture-decisions/#context","title":"Context","text":"<p>We need persistent, reviewable decisions for the architecture (layers, boundaries, invariants).</p>"},{"location":"architecture/adr/0001-record-architecture-decisions/#decision","title":"Decision","text":"<p>Use ADRs (Markdown files in <code>docs/architecture/adr/</code>) to capture decisions. Use Import Linter to encode layering rules; fail CI on violations. Use C4 diagrams (PlantUML/Mermaid) to visualize context and containers.</p>"},{"location":"architecture/adr/0001-record-architecture-decisions/#consequences","title":"Consequences","text":"<ul> <li>New contributors and AI agents understand why choices were made.</li> <li>Changes across layers require updating rules and ADRs to stay aligned.</li> </ul>"},{"location":"architecture/adr/0002-hexagonal-architecture/","title":"0002 \u2013 Adopt a Hexagonal Architecture","text":"<p>Date: 2025-10-25</p>"},{"location":"architecture/adr/0002-hexagonal-architecture/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/adr/0002-hexagonal-architecture/#context","title":"Context","text":"<p>We are delivering a single-machine, high-performance hybrid search system that stitches together DocTags, Qwen3 embeddings, SPLADE, FAISS GPU/cuVS, and DuckDB as the registry. We must keep interfaces stable while enabling rapid iteration.</p>"},{"location":"architecture/adr/0002-hexagonal-architecture/#decision","title":"Decision","text":"<p>Adopt a hexagonal architecture with strict inbound/outbound ports. Persist vectors in Parquet to keep storage self-describing and tooling-friendly.</p>"},{"location":"architecture/adr/0002-hexagonal-architecture/#consequences","title":"Consequences","text":"<ul> <li>Independent workstreams can progress without coordination friction.</li> <li>Ports/adapters isolate infrastructure churn from core domain logic.</li> <li>Testing and reproducibility improve because IO remains deterministic.</li> </ul>"},{"location":"explanations/","title":"Explanations","text":"<p>Deep dives into KGForge architecture and design decisions.</p> <pre><code>:maxdepth: 1\n251025_HighLevelArchitecture\n251025_FAISS_whl_overview\n</code></pre>"},{"location":"explanations/251025_FAISS_whl_overview/","title":"Updated cuVS Addendum","text":"<p>Below is an updated, implementation\u2011grade addendum that reflects your new FAISS wheel with cuVS integration compiled for: Flat, IVF\u2011Flat, IVF\u2011PQ, Graph CAGRA, and Binary\u2011CAGRA, as indicated by the build artifacts:</p> <pre><code>CuvsFlatIndex.cu.o\nCuvsIVFFlat.cu.o\nCuvsIVFPQ.cu.o\nCuvsCagra.cu.o\nBinaryCuvsCagra.cu.o\nutils/CuvsFilterConvert.cu.o\nutils/CuvsUtils.cu.o\n</code></pre> <p>Key consequence: In this wheel, FAISS GPU indexes can dispatch to cuVS kernels for the families above when you enable cuVS at construction/cloning time and the cuVS/RAPIDS libraries are present in the process. The runtime loader &amp; resource rules for cuVS remain the same (preload <code>libcuvs</code>, <code>librmm</code>, logging; use a shared <code>Resources</code> handle if calling cuVS directly).  </p>"},{"location":"explanations/251025_FAISS_whl_overview/#1-revised-capability-matrix-this-wheel","title":"1) Revised capability matrix (this wheel)","text":"Family / class CPU GPU (FAISS kernels) cuVS through FAISS (this wheel) cuVS direct (Python) Flat (exact) \u2705 <code>IndexFlat{L2,IP}</code> \u2705 <code>GpuIndexFlat{L2,IP}</code> \u2705 via index\u2011level cuVS (<code>GpuIndexFlat{L2,IP}</code> with <code>use_cuvs</code>) and via bfKNN (<code>knn_gpu/bfKnn</code> with <code>use_cuvs</code>) \u2705 <code>cuvs.neighbors.brute_force</code> IVF\u2011Flat \u2705 \u2705 <code>GpuIndexIVFFlat</code> \u2705 index\u2011level cuVS (<code>GpuIndexIVFFlat</code> with <code>use_cuvs</code>) \u2705 <code>cuvs.neighbors.ivf_flat</code> IVF\u2011PQ \u2705 \u2705 <code>GpuIndexIVFPQ</code> \u2705 index\u2011level cuVS (<code>GpuIndexIVFPQ</code> with <code>use_cuvs</code>) \u2705 <code>cuvs.neighbors.ivf_pq</code> IVF\u2011SQ \u2705 \u2705 <code>GpuIndexIVFScalarQuantizer</code> \u2013 (no cuVS kernel here) \u2013 Graph (HNSW) \u2705 \u2013 \u2013 \u2705 <code>cuvs.neighbors.hnsw</code> Graph (CAGRA) \u2705 <code>IndexHNSWCagra</code> \u2705 <code>GpuIndexCagra</code> \u2705 index\u2011level cuVS (<code>GpuIndexCagra</code> with <code>use_cuvs</code>) \u2705 <code>cuvs.neighbors.cagra</code> (+ multi\u2011GPU) Binary \u2705 \u2705 <code>GpuIndexBinaryFlat</code>, <code>GpuIndexBinaryCagra</code> \u2705 Binary\u2011CAGRA via index\u2011level cuVS (<code>GpuIndexBinaryCagra</code>) \u2013 Direct bfKNN \u2013 \u2705 <code>bfKnn(...)</code> / <code>knn_gpu(...)</code> \u2705 set <code>use_cuvs=True</code> (guard with <code>should_use_cuvs(...)</code>) \u2705 <code>cuvs.distance</code> / brute_force <p>What changed: In earlier guidance, IVF\u2011Flat/IVF\u2011PQ/CAGRA/Binary\u2011CAGRA were marked \u201cnot via FAISS route\u201d. In this wheel, those families are cuVS\u2011enabled inside FAISS when you opt in via the index configs or GPU cloner options described below (and cuVS libs are present). The bfKNN cuVS path remains available. The cuVS loader &amp; environment remain as documented. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#2-how-to-enable-cuvs-reliable-patterns","title":"2) How to enable cuVS (reliable patterns)","text":""},{"location":"explanations/251025_FAISS_whl_overview/#21-oneline-preload-cuvsrapids-libs","title":"2.1. One\u2011line preload (cuVS/RAPIDS libs)","text":"<p>Call this before importing/initializing FAISS to ensure the shared libraries are resident:</p> <pre><code>from libcuvs import load_library\nload_library()  # loads libcuvs, librmm, rapids_logger; idempotent\n</code></pre> <p>This follows the integration guidance in your loader reference and prevents <code>OSError: libcuvs_c.so not found</code> surprises. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#22-enabling-cuvs-via-gpu-cloner-options-recommended","title":"2.2. Enabling cuVS via GPU Cloner Options (recommended)","text":"<p>The simplest and most portable way to engage cuVS for Flat / IVF\u2011Flat / IVF\u2011PQ / CAGRA / Binary\u2011CAGRA is to build the index on CPU, then clone to GPU with a cuVS\u2011aware cloner:</p> <pre><code>import faiss\n\n# 1) Build a CPU index (examples below)\ncpu_index = faiss.index_factory(d, \"OPQ64,IVF8192,PQ64\", faiss.METRIC_INNER_PRODUCT)\ncpu_index.train(train_vectors)\n\n# 2) Prepare GPU resources\nres = faiss.StandardGpuResources()\n\n# 3) Clone with cuVS enabled (guarded)\nco = faiss.GpuClonerOptions()\nco.use_cuvs = True  # ask for cuVS kernels if available\n\ntry:\n    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)\nexcept RuntimeError:\n    # Fallback if cuVS cannot be used for this combo\n    co.use_cuvs = False\n    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)\n</code></pre> <ul> <li><code>GpuClonerOptions.use_cuvs</code> is exposed in this wheel and directs FAISS to build the cuVS\u2011backed GPU index when possible. </li> <li>This pattern works uniformly across Flat / IVF\u2011Flat / IVF\u2011PQ and also covers CAGRA if cloning from <code>IndexHNSWCagra</code> to <code>GpuIndexCagra</code> is desired. (You can also build <code>GpuIndexCagra</code> directly; see 2.3.)</li> <li>Keep your existing memory tuning (<code>setTempMemory</code>, <code>setPinnedMemory</code>) for throughput.</li> </ul> <p>You can also probe <code>faiss.should_use_cuvs(...)</code> before setting the flag; however, the try/except is the most robust guard when combining drivers, cards, and dims. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#23-enabling-cuvs-via-index-configs-direct-gpu-constructors","title":"2.3. Enabling cuVS via index configs (direct GPU constructors)","text":"<p>When you construct GPU indexes directly, pass a config whose base type is <code>GpuIndexConfig</code> and set <code>use_cuvs</code> there (the property is inherited by specific configs):</p> <pre><code># Flat\nflat_cfg = faiss.GpuIndexFlatConfig()\nflat_cfg.use_cuvs = True\ngpu_flat = faiss.GpuIndexFlatIP(res, d, flat_cfg)  # or GpuIndexFlatL2\n\n# IVF-FLAT / IVF-PQ\nivf_cfg = faiss.GpuIndexIVFConfig()\nivf_cfg.use_cuvs = True\n# ... pass ivf_cfg to the appropriate GpuIndexIVF* constructor\n\n# CAGRA\ncagra_cfg = faiss.GpuIndexCagraConfig()\ncagra_cfg.use_cuvs = True\ngpu_cagra = faiss.GpuIndexCagra(res, d, cagra_cfg)\n\n# Binary-CAGRA (if you build it directly)\n# (config class may not be separate in the API; prefer the cloner path if unsure)\n</code></pre> <p>These configs inherit <code>use_cuvs</code> through <code>GpuIndexConfig</code> in this wheel. Use the cloner option (2.2) when you want one code path that works across all index types. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#24-enabling-cuvs-for-direct-gpu-bruteforce-knn","title":"2.4. Enabling cuVS for direct GPU brute\u2011force kNN","text":"<p>The bfKNN path can independently dispatch to cuVS kernels:</p> <pre><code>from faiss import StandardGpuResources, METRIC_INNER_PRODUCT, GpuDistanceParams, knn_gpu\n\nres = StandardGpuResources()\nparams = GpuDistanceParams(); params.metric = METRIC_INNER_PRODUCT; params.k = 10; params.dims = d\n\n# guard with should_use_cuvs; fallback is automatic inside knn_gpu if you pass False\nuse_cuvs = True  # or: bool(faiss.should_use_cuvs(params))\nD, I = knn_gpu(res, xq.astype('float32'), xb.astype('float32'),\n               k=10, metric=METRIC_INNER_PRODUCT, use_cuvs=use_cuvs)\n</code></pre> <p>This remains a great baseline even when your main serving index is IVF\u2011PQ/CAGRA. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#3-minimal-correct-by-construction-recipes-per-family","title":"3) Minimal \u201ccorrect by construction\u201d recipes per family","text":"<p>These patterns train on CPU (deterministic), then clone to GPU with cuVS. You can switch to direct GPU construction later; the recall/latency behavior is identical once cuVS is engaged.</p>"},{"location":"explanations/251025_FAISS_whl_overview/#31-ivfpq-opq64ivf8192pq64-with-cuvs","title":"3.1. IVF\u2011PQ (OPQ64,IVF8192,PQ64) with cuVS","text":"<pre><code>import faiss, numpy as np\n\nd = 2560\ncpu = faiss.index_factory(d, \"OPQ64,IVF8192,PQ64\", faiss.METRIC_INNER_PRODUCT)\n\ntrain = np.random.randn(10_000_000, d).astype('float32')\nfaiss.normalize_L2(train)\ncpu.train(train)\n\nres = faiss.StandardGpuResources()\nco = faiss.GpuClonerOptions(); co.use_cuvs = True\ngpu = faiss.index_cpu_to_gpu(res, 0, cpu, co)  # cuVS-backed IVFPQ\n\n# add/search as usual\n</code></pre> <ul> <li>Keeps your default \u201cbest\u2011in\u2011class\u201d factory and determinism.</li> <li>The cloner chooses the cuVS implementation for the data path compiled in this wheel. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#32-ivfflat-with-cuvs","title":"3.2. IVF\u2011Flat with cuVS","text":"<pre><code>cpu = faiss.index_factory(d, \"IVF8192,Flat\", faiss.METRIC_INNER_PRODUCT)\ncpu.train(train)\n\nco = faiss.GpuClonerOptions(); co.use_cuvs = True\ngpu = faiss.index_cpu_to_gpu(res, 0, cpu, co)  # cuVS-backed IVFFlat\n</code></pre>"},{"location":"explanations/251025_FAISS_whl_overview/#33-flat-exact-with-cuvs","title":"3.3. Flat (exact) with cuVS","text":"<ul> <li>Index route: <code>GpuIndexFlat{L2,IP}</code> with <code>GpuIndexFlatConfig.use_cuvs = True</code>.</li> <li>Direct KNN: <code>knn_gpu(..., use_cuvs=True)</code> (guarded). </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#34-cagra-with-cuvs","title":"3.4. CAGRA with cuVS","text":"<ul> <li>Direct GPU build (graph lives on GPU):</li> </ul> <p><code>python   cfg = faiss.GpuIndexCagraConfig(); cfg.use_cuvs = True   gpu_cagra = faiss.GpuIndexCagra(res, d, cfg)   # gpu_cagra.add(n, xb) builds the graph; search as usual</code></p> <ul> <li>CPU\u2194GPU interop: <code>IndexHNSWCagra</code> exists on CPU for moving graph structure if needed (e.g., copy base level). Keep primary serving on GPU to avoid host &lt;-&gt; device hops.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#35-binarycagra-with-cuvs","title":"3.5. Binary\u2011CAGRA with cuVS","text":"<ul> <li>Prefer the cloner path when coming from CPU binary indexes; if you instantiate <code>GpuIndexBinaryCagra</code> directly, keep the same principle: construct with a config that enables cuVS (or clone with <code>GpuClonerOptions.use_cuvs=True</code>) and fall back on error.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#4-filters-selectors-what-those-cuvsfilter-objects-mean-for-you","title":"4) Filters &amp; selectors (what those <code>CuvsFilter*</code> objects mean for you)","text":"<p>The presence of <code>utils/CuvsFilterConvert.cu.o</code> indicates FAISS performs selector/bitset conversion into cuVS\u2011compatible filter representations when a filter is passed at search time (e.g., via <code>SearchParametersIVF.sel</code> / ID selectors). You do not need to change your Python calls:</p> <ul> <li>Keep using FAISS\u2019 search parameters (e.g., <code>SearchParametersIVF</code>\u2019s selector).</li> <li>When the index was constructed/cloned with <code>use_cuvs=True</code>, the FAISS runtime converts the filter to cuVS\u2019 internal format and applies it during list scans.</li> </ul> <p>This is transparent but worth noting for filtered search scenarios. (The general cuVS run\u2011time and loader expectations remain exactly as previously documented.) </p>"},{"location":"explanations/251025_FAISS_whl_overview/#5-operational-reminders-unchanged-but-critical","title":"5) Operational reminders (unchanged but critical)","text":"<ul> <li>Preload cuVS once per process (<code>libcuvs.load_library()</code>), so RMM and logging are initialized and all <code>.so</code> dependencies are resident. </li> <li>Stick to row\u2011major <code>float32</code> contiguous arrays for FAISS; normalize vectors for cosine (IP).</li> <li>Keep GPU working set \u2264 ~80% VRAM; pre\u2011allocate temp and pinned memory on <code>StandardGpuResources</code>.</li> <li>For cuVS direct experimentation (IVF\u2011PQ, IVF\u2011Flat, CAGRA, HNSW, multi\u2011GPU), continue to use the Python surfaces under <code>cuvs.neighbors.*</code> with a shared <code>Resources</code> handle and device arrays, as documented in your cuVS reference. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#6-quick-am-i-using-cuvs-right-now-probes","title":"6) Quick \u201cam I using cuVS right now?\u201d probes","text":"<pre><code># After calling libcuvs.load_library() and constructing your GPU index:\n\nimport faiss\n\n# 1) For bfKNN:\np = faiss.GpuDistanceParams(); p.k=10; p.dims=d; p.metric=faiss.METRIC_INNER_PRODUCT\nprint(\"bfKNN should_use_cuvs:\", faiss.should_use_cuvs(p))  # True \u2192 bfKNN will route to cuVS\n\n# 2) For an index you plan to build:\ncfg = faiss.GpuIndexIVFConfig()\nprint(\"Index should_use_cuvs:\", faiss.should_use_cuvs(cfg))  # True \u2192 set cfg/co.use_cuvs = True\n</code></pre> <ul> <li>If you skip <code>should_use_cuvs</code>, simply set <code>use_cuvs=True</code> and catch a <code>RuntimeError</code> on construction; re\u2011try with <code>False</code>. This keeps your code robust across environments while preferring cuVS whenever available. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#pointers-unchanged","title":"Pointers (unchanged)","text":"<ul> <li>cuVS API &amp; usage (neighbors, distance, resources, device arrays) \u2014 your cuVS reference. </li> <li>cuVS loader &amp; environment (preload order, env vars, system vs wheel libs) \u2014 libcuvs loader reference. </li> <li>System architecture &amp; defaults (2560\u2011d, OPQ64/IVF8192/PQ64, persistence, observability) \u2014 high\u2011level architecture. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#7-bottom-line-what-to-change-in-your-code","title":"7) Bottom line (what to change in your code)","text":"<ol> <li>Call <code>load_library()</code> once at startup. </li> <li>When cloning CPU\u2192GPU, set <code>co.use_cuvs = True</code> and fall back on error. This now enables cuVS for Flat, IVF\u2011Flat, IVF\u2011PQ, CAGRA, and Binary\u2011CAGRA in this build.</li> <li>When constructing GPU indexes directly, set <code>cfg.use_cuvs = True</code> on the appropriate <code>GpuIndex*Config</code> (or use the cloner).</li> <li>For bfKNN, pass <code>use_cuvs=True</code> (or guard with <code>should_use_cuvs</code>). </li> <li>Keep the rest of the pipeline (factory string, normalization, memory planning, multi\u2011GPU strategy, persistence) unchanged. </li> </ol>"},{"location":"explanations/251025_FAISS_whl_overview/#addendum-base-content-on-faiss-gpu-library","title":"addendum, base content on faiss gpu library","text":""},{"location":"explanations/251025_FAISS_whl_overview/#0-what-changed-vs-the-prior-wheel-and-how-to-think-about-it","title":"0) What changed vs. the prior wheel (and how to think about it)","text":"<p>Hardware/ABI</p> <ul> <li>CUDA runtime linked: <code>libcudart.so.13</code> \u2192 the wheel targets CUDA\u202f13 (Blackwell\u2011ready).</li> <li>GPU arch: <code>sm_120</code> SASS + PTX (\u201c<code>sm_120\u2011virtual</code>\u201d) for forward compatibility\u2014native kernels when they match, PTX JIT otherwise.</li> </ul> <p>cuVS inside FAISS (this wheel)</p> <ul> <li>The GPU SWIG layer exports <code>use_cuvs</code> gates on index configs and cloner options and dynamically links against cuVS/RAPIDS libs. With cuVS libraries preloaded, FAISS GPU indexes can route to cuVS kernels for Flat, IVF\u2011Flat, IVF\u2011PQ, CAGRA, Binary\u2011CAGRA; otherwise FAISS falls back to its own kernels. </li> </ul> <p>Reminder: the kernels themselves live in the cuVS shared libraries; the FAISS wheel holds dispatch hooks and dynamic links. Ensure the cuVS loader is invoked at process start. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#1-cpu-vs-gpu-vs-cuvs-updated-capability-map-this-wheel","title":"1) CPU vs GPU vs cuVS \u2014 updated capability map (this wheel)","text":"Family / class CPU GPU (FAISS kernels) cuVS through FAISS (this wheel) cuVS direct (Python) Flat (exact) \u2705 <code>IndexFlat{L2,IP}</code> \u2705 <code>GpuIndexFlat{L2,IP}</code> \u2705 via index\u2011level <code>use_cuvs</code> on <code>GpuIndexFlat*</code> and via bfKNN (<code>knn_gpu/bfKnn</code> <code>use_cuvs</code>) \u2705 <code>cuvs.neighbors.brute_force</code> IVF\u2011Flat \u2705 \u2705 <code>GpuIndexIVFFlat</code> \u2705 via index\u2011level <code>use_cuvs</code> / GPU cloner \u2705 <code>cuvs.neighbors.ivf_flat</code> IVF\u2011PQ \u2705 \u2705 <code>GpuIndexIVFPQ</code> \u2705 via index\u2011level <code>use_cuvs</code> / GPU cloner \u2705 <code>cuvs.neighbors.ivf_pq</code> IVF\u2011SQ \u2705 \u2705 <code>GpuIndexIVFScalarQuantizer</code> \u2013 (no cuVS path) \u2013 Graph (HNSW) \u2705 \u2013 \u2013 \u2705 <code>cuvs.neighbors.hnsw</code> Graph (CAGRA) \u2705 <code>IndexHNSWCagra</code> \u2705 <code>GpuIndexCagra</code> \u2705 via index\u2011level <code>use_cuvs</code> \u2705 <code>cuvs.neighbors.cagra</code> (+ multi\u2011GPU) Binary \u2705 \u2705 <code>GpuIndexBinaryFlat</code>, <code>GpuIndexBinaryCagra</code> \u2705 Binary\u2011CAGRA via index\u2011level <code>use_cuvs</code> \u2013 Direct bfKNN \u2013 \u2705 <code>bfKnn(...)</code> / <code>knn_gpu(...)</code> \u2705 <code>use_cuvs=True</code> (guard with <code>should_use_cuvs</code>) \u2705 <code>cuvs.distance</code> / brute_force <p>Filters &amp; selectors: your build includes <code>CuvsFilterConvert.cu.o</code>; FAISS will convert FAISS selectors/bitsets into cuVS\u2019 filter format automatically when the index was created with <code>use_cuvs=True</code>. You keep using standard FAISS search params at the Python layer. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#2-blackwell-rtx-5090-support-sass-sm_120-ptx-sm_120virtual","title":"2) Blackwell (RTX\u202f5090) support: SASS <code>sm_120</code> + PTX <code>sm_120\u2011virtual</code>","text":"<ul> <li>Why it matters: perfect performance when SASS matches; PTX fallback when driver/toolkit moves ahead (no \u201cno kernel image\u201d errors).</li> <li>Ops tips: leave <code>CUDA_CACHE_MAXSIZE</code> roomy to avoid first\u2011run JIT thrash; never force JIT in production.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#3-cuvs-enablement-model-final-patterns","title":"3) cuVS enablement model (final patterns)","text":"<p>Always preload cuVS/RAPIDS libs before FAISS:</p> <pre><code>from libcuvs import load_library\nload_library()  # loads libcuvs, librmm, rapids_logger; idempotent. :contentReference[oaicite:12]{index=12}\n</code></pre> <p>A) Index\u2011level enablement (recommended) Set <code>use_cuvs=True</code> on the GPU cloner or GPU index config; catch &amp; fallback:</p> <pre><code>import faiss\n\n# Clone CPU \u2192 GPU (works uniformly for Flat / IVF-Flat / IVF-PQ / CAGRA / Binary-CAGRA)\nco = faiss.GpuClonerOptions(); co.use_cuvs = True\ntry:\n    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)\nexcept RuntimeError:\n    co.use_cuvs = False\n    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)\n</code></pre> <p>Or construct GPU indexes directly with a config inheriting <code>GpuIndexConfig.use_cuvs</code>:</p> <pre><code>flat_cfg = faiss.GpuIndexFlatConfig(); flat_cfg.use_cuvs = True\ngpu_flat = faiss.GpuIndexFlatIP(res, d, flat_cfg)\n\nivf_cfg = faiss.GpuIndexIVFConfig(); ivf_cfg.use_cuvs = True\n# pass ivf_cfg into the appropriate GpuIndexIVF* constructor\n\ncagra_cfg = faiss.GpuIndexCagraConfig(); cagra_cfg.use_cuvs = True\ngpu_cagra = faiss.GpuIndexCagra(res, d, cagra_cfg)\n</code></pre> <p>B) Brute\u2011force KNN path Guard <code>use_cuvs</code> with the central probe (falls back if not viable):</p> <pre><code>params = faiss.GpuDistanceParams(); params.dims=d; params.k=10; params.metric=faiss.METRIC_INNER_PRODUCT\nuse_cuvs = bool(faiss.should_use_cuvs(params))  # detects viability at runtime. :contentReference[oaicite:13]{index=13}\nD, I = faiss.knn_gpu(res, xq.astype('float32'), xb.astype('float32'), 10,\n                     metric=faiss.METRIC_INNER_PRODUCT, use_cuvs=use_cuvs)\n</code></pre> <p>C) Direct cuVS ANN (optional) Use cuVS Python APIs for IVF\u2011Flat/IVF\u2011PQ/CAGRA/HNSW (especially when you want multi\u2011GPU <code>mg.*</code> variants): <code>cuvs.neighbors.{ivf_flat, ivf_pq, cagra, hnsw}</code> with <code>Resources</code> handle and device arrays. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#4-bestinclass-faiss-build-on-rtx-5090-cosine-2560d-now-with-cuvs-toggles","title":"4) \u201cBest\u2011in\u2011class\u201d FAISS build on RTX\u202f5090 (cosine, 2560\u2011d) \u2014 now with cuVS toggles","text":"<p>Index choice: <code>OPQ64,IVF8192,PQ64</code> (cosine via L2 normalization + IP). Train on up to 10\u202fM samples (seed=42). Search with <code>nprobe=64</code>. </p> <p>GPU clone with cuVS (preferred path):</p> <pre><code>import faiss, numpy as np\n\nd = 2560\ncpu = faiss.index_factory(d, \"OPQ64,IVF8192,PQ64\", faiss.METRIC_INNER_PRODUCT)\ntrain = np.random.randn(10_000_000, d).astype('float32'); faiss.normalize_L2(train)\ncpu.train(train)\n\nres = faiss.StandardGpuResources()\nres.setTempMemory(1_200_000_000); res.setPinnedMemory(512_000_000)\n\nco = faiss.GpuClonerOptions(); co.use_cuvs = True\ntry:\n    gpu = faiss.index_cpu_to_gpu(res, 0, cpu, co)  # cuVS-backed IVFPQ in this build\nexcept RuntimeError:\n    co.use_cuvs = False\n    gpu = faiss.index_cpu_to_gpu(res, 0, cpu, co)\n\n# add / search as usual\n</code></pre> <p>Notes &amp; knobs</p> <ul> <li>Precompute lookup tables on GPU if your PQ config benefits (via cloner/options); keep working set \u2264 80% VRAM; reuse pinned host buffers for larger transfers. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#5-when-to-call-cuvs-directly-and-when-faisscuvs-is-enough","title":"5) When to call cuVS directly (and when FAISS+cuVS is enough)","text":"<ul> <li>Stay inside FAISS (with <code>use_cuvs=True</code>) for: Flat, IVF\u2011Flat, IVF\u2011PQ, CAGRA, Binary\u2011CAGRA; you get FAISS\u2019 ergonomics and cuVS performance.</li> <li>Call cuVS directly when you need: multi\u2011GPU <code>mg.*</code> pipelines, algorithm\u2011specific parameters not exposed by FAISS\u2019 wrappers, or you want to co\u2011locate with other RAFT pipelines with explicit stream choreography. </li> </ul> <p>Direct IVF\u2011PQ example (unchanged) \u2014 device arrays + <code>Resources</code>:</p> <pre><code>from cuvs.common import Resources\nfrom cuvs.neighbors import ivf_pq\nfrom pylibraft.common import device_ndarray\n# build/search as in your prior example \u2026 :contentReference[oaicite:18]{index=18}\n</code></pre>"},{"location":"explanations/251025_FAISS_whl_overview/#6-quick-api-map-what-youll-call-most-often","title":"6) Quick API map \u2014 what you\u2019ll call most often","text":"<p>CPU <code>IndexFlat{L2,IP}</code>, <code>IndexIVFFlat</code>, <code>IndexIVFPQ</code>, <code>IndexIVFScalarQuantizer</code>, <code>IndexHNSW*</code>, <code>IndexHNSWCagra</code>, <code>IndexBinary*</code>, <code>IndexPreTransform</code>, <code>OPQMatrix</code>, <code>PCAMatrix</code>, <code>IndexRefineFlat</code>, <code>IndexShards</code>, <code>IndexReplicas</code>.</p> <p>GPU (FAISS) <code>StandardGpuResources</code>, <code>GpuIndexFlat{L2,IP}</code>, <code>GpuIndexIVFFlat</code>, <code>GpuIndexIVFPQ</code>, <code>GpuIndexIVFScalarQuantizer</code>, <code>GpuIndexCagra</code>, <code>GpuIndexBinaryFlat</code>, <code>GpuIndexBinaryCagra</code>, <code>index_cpu_to_gpu[_multiple]</code>, <code>index_gpu_to_cpu</code>, <code>GpuClonerOptions</code>, <code>GpuParameterSpace</code>, <code>knn_gpu(...)</code>, <code>bfKnn(...)</code>.</p> <p>cuVS hooks inside FAISS <code>GpuClonerOptions.use_cuvs</code>, <code>GpuIndex*Config.use_cuvs</code>, <code>should_use_cuvs(...)</code> (for bfKNN probe), <code>knn_gpu(..., use_cuvs=...)</code>. </p> <p>cuVS (direct) <code>cuvs.neighbors.{ivf_flat, ivf_pq, cagra, hnsw, brute_force, mg.*}</code>, <code>cuvs.cluster.kmeans</code>, <code>cuvs.distance.pairwise_distance</code>, <code>cuvs.common.Resources</code>. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#7-validation-checklist-runtime","title":"7) Validation checklist (runtime)","text":"<ol> <li>Preload check (did we load the libraries?):</li> </ol> <pre><code>from libcuvs import load_library\nprint([h._name for h in load_library()])  # expect libcuvs*, librmm, rapids_logger. :contentReference[oaicite:21]{index=21}\n</code></pre> <ol> <li> <p>Index\u2011level cuVS (construction guard):</p> </li> <li> <p>Clone/construct with <code>use_cuvs=True</code>. If it raises, retry with <code>False</code>. Log the final <code>use_cuvs</code> state per index.</p> </li> <li> <p>bfKNN cuVS probe (fast sanity):</p> </li> </ol> <pre><code>from faiss import GpuDistanceParams, should_use_cuvs, METRIC_INNER_PRODUCT\np = GpuDistanceParams(); p.metric = METRIC_INNER_PRODUCT; p.dims = 2560; p.k = 10\nprint(\"bfKNN should_use_cuvs:\", should_use_cuvs(p))  # True =&gt; bfKNN will route to cuVS. :contentReference[oaicite:22]{index=22}\n</code></pre> <ol> <li>Direct cuVS smoke (IVF\u2011PQ or brute\u2011force) with device arrays + <code>Resources</code>. </li> </ol>"},{"location":"explanations/251025_FAISS_whl_overview/#8-operational-guidance-unchanged-principles-cuvsaware","title":"8) Operational guidance (unchanged principles, cuVS\u2011aware)","text":"<ul> <li>Loader order: preload RAPIDS/cuVS first; then import FAISS. </li> <li>Memory: size <code>setTempMemory</code>/<code>setPinnedMemory</code> once; keep VRAM headroom ~20%; batch adds to avoid OOM; precompute tables where helpful. </li> <li>Streams: cuVS direct calls accept a <code>Resources</code> handle (shared stream) and only sync when you call <code>handle.sync()</code>. FAISS tends to sync at call boundaries; overlap I/O with compute using pinned buffers. </li> <li>Indexes at rest: save CPU index (<code>.faiss</code>) + <code>.ids</code>; rebuild GPU clones at startup (set <code>use_cuvs</code> again when cloning). </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#9-readytopaste-helpers-with-cuvs-autoengage","title":"9) Ready\u2011to\u2011paste helpers (with cuVS auto\u2011engage)","text":"<p>A. GPU clone (cuVS\u2011first, safe fallback)</p> <pre><code>def to_gpu_cuvs_first(cpu_index, res, device=0):\n    co = faiss.GpuClonerOptions(); co.use_cuvs = True\n    try:\n        return faiss.index_cpu_to_gpu(res, device, cpu_index, co), True\n    except RuntimeError:\n        co.use_cuvs = False\n        return faiss.index_cpu_to_gpu(res, device, cpu_index, co), False\n</code></pre> <p>B. bfKNN with automatic cuVS</p> <pre><code>def knn_gpu_auto(res, xq_f32, xb_f32, k=10, metric=faiss.METRIC_INNER_PRODUCT):\n    params = faiss.GpuDistanceParams()\n    params.metric = metric; params.k = k; params.dims = xq_f32.shape[1]\n    use = faiss.should_use_cuvs(params)  # True when viable. :contentReference[oaicite:28]{index=28}\n    return faiss.knn_gpu(res, xq_f32.astype('float32', copy=False),\n                         xb_f32.astype('float32', copy=False),\n                         k, metric=metric, use_cuvs=use)\n</code></pre> <p>C. Direct cuVS IVF\u2011PQ (device arrays) \u2014 unchanged. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#10-bottom-line","title":"10) Bottom line","text":"<ul> <li>This wheel: Blackwell\u2011ready (<code>sm_120</code> + PTX), CUDA\u201113, and cuVS\u2011enabled inside FAISS for Flat, IVF\u2011Flat, IVF\u2011PQ, CAGRA, Binary\u2011CAGRA.</li> <li>Your code: preload cuVS; set <code>use_cuvs=True</code> on GPU clones/constructors; guard with try/except; use <code>should_use_cuvs(...)</code> for bfKNN.</li> <li>Architecture &amp; defaults (factory, training, persistence, multi\u2011GPU) remain per plan; you simply get cuVS speed paths where available without changing the higher\u2011level design. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#addendum-agentoriented-details-updated-to-match-the-new-cuvs-coverage","title":"Addendum \u2014 Agent\u2011oriented details (updated to match the new cuVS coverage)","text":"<p>These sections expand the \u201chow\u201d with concrete interfaces, schemas, and runbooks across the stack and already assume FAISS can route to cuVS for the families above.</p>"},{"location":"explanations/251025_FAISS_whl_overview/#a1-process-bootstrap-deterministic-cuvsready","title":"A1. Process bootstrap (deterministic &amp; cuVS\u2011ready)","text":"<pre><code>from libcuvs import load_library; load_library()   # ensures libcuvs/librmm/logging are loaded. :contentReference[oaicite:31]{index=31}\nimport faiss\nres = faiss.StandardGpuResources()\nres.setTempMemory(1_200_000_000); res.setPinnedMemory(512_000_000)\n</code></pre> <p>Keep a single <code>StandardGpuResources</code> per device for the process lifetime and configure it once. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#a2-zerocopy-interop-faiss-cuvs","title":"A2. Zero\u2011copy interop (FAISS \u2194 cuVS)","text":"<p>Use RAFT <code>Resources</code> and device arrays when calling cuVS directly; prefer pinned host arenas + contiguous <code>float32</code> for FAISS inputs to minimize hidden copies. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#a3-multigpu-replicas-and-shards-unchanged","title":"A3. Multi\u2011GPU: replicas and shards (unchanged)","text":"<p>Use <code>IndexReplicas</code> (QPS) or <code>IndexShards</code> (capacity). When cloning to multiple GPUs, set <code>use_cuvs=True</code> on the cloner options once; FAISS will apply it per device (fallback per GPU on error). </p>"},{"location":"explanations/251025_FAISS_whl_overview/#a4-memory-planning-batching","title":"A4. Memory planning &amp; batching","text":"<p>Budget scratch + PQ tables + codes + ids + batch \u2264 80% VRAM. Tune batch add/search sizes empirically; precompute lookup tables where it helps latency. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#a5-persistence-registry","title":"A5. Persistence &amp; registry","text":"<p>Persist CPU index (<code>.faiss</code>) + <code>.ids</code>; rebuild GPU clones at service start using <code>use_cuvs=True</code>. Record the final applied state (<code>cuvs=true/false</code>) in your registry\u2019s <code>faiss_indexes</code> rows for observability. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#a6-observability-probes","title":"A6. Observability &amp; probes","text":"<ul> <li>Log <code>{gpu_id, nlist, m, nprobe, k, batch, temp_mem, pinned_mem, use_cuvs}</code> for each search; expose p50/p95/p99 histograms. </li> <li>Quick \u201care the libs loaded?\u201d probe: <code>list(h._name for h in load_library())</code>. </li> <li>Quick \u201cwill bfKNN use cuVS?\u201d probe: <code>should_use_cuvs(GpuDistanceParams(...))</code>. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a7-failure-taxonomy-fallbacks-gpu-paths","title":"A7. Failure taxonomy &amp; fallbacks (GPU paths)","text":"<ul> <li>Missing libs \u2192 run <code>load_library()</code> first (or fix <code>LD_LIBRARY_PATH</code>); resume with FAISS kernels if cuVS fails to initialize. </li> <li>OOM \u2192 halve batch; lower <code>setTempMemory</code>; retry twice; then quarantine batch. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a8-direct-cuvs-catalog-unchanged-surfaces","title":"A8. Direct cuVS catalog (unchanged surfaces)","text":"<p><code>cuvs.neighbors.{ivf_flat, ivf_pq, cagra, hnsw, brute_force, mg.*}</code>, <code>cuvs.cluster.kmeans</code>, <code>cuvs.distance.pairwise_distance</code>, <code>cuvs.common.Resources</code> \u2014 consistent <code>IndexParams</code>/<code>SearchParams</code>/<code>build</code>/<code>extend</code>/<code>save</code>/<code>load</code> patterns. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#a9-architecture-defaults-for-the-broader-system","title":"A9. Architecture defaults (for the broader system)","text":"<p>Keep the 2560\u2011d embedder, chunking settings, Parquet schemas, DuckDB catalog/migrations, and hybrid retrieval pipeline exactly as specified in your architecture doc; nothing about the cuVS enablement changes those contracts. </p> <p>If you\u2019d like, I can also output a drop\u2011in <code>vectorstore_faiss</code> module (Python package) that wraps the train/add/search/persist flow with cuVS\u2011first cloning and automatic bfKNN fallback, plus a tiny validator script that prints library load locations, shows <code>should_use_cuvs(...)</code>, and runs a 10k\u2011vector bfKNN smoke test on the RTX\u202f5090.</p> <p>Below is a comprehensive, example\u2011driven reference that fills in the remaining FAISS and cuVS Python surfaces we hadn\u2019t previously shown with code. It\u2019s organized so an agent can copy/paste snippets and understand shapes, params, edge\u2011cases, and performance notes. Where relevant, I point to the cuVS and loader docs you provided.   </p>"},{"location":"explanations/251025_FAISS_whl_overview/#a-faiss-cpuside-classes-utilities-with-examples","title":"A) FAISS \u2014 CPU\u2011side classes &amp; utilities (with examples)","text":""},{"location":"explanations/251025_FAISS_whl_overview/#a1-indexidmap-indexidmap2-stable-ids-around-any-index","title":"A1) <code>IndexIDMap</code> / <code>IndexIDMap2</code> \u2014 stable IDs around any index","text":"<p>Why: FAISS stores integer IDs; these wrappers let you control them.</p> <pre><code>import faiss, numpy as np\n\n# Base ANN index (any type works)\nd = 2560\nbase = faiss.index_factory(d, \"IVF8192,PQ64\", faiss.METRIC_INNER_PRODUCT)\n\n# Wrap with an ID map so we add our own 64-bit IDs\nidmap = faiss.IndexIDMap2(base)\n\n# Train as usual (on base)\ntrain = np.random.randn(1_000_000, d).astype('float32'); faiss.normalize_L2(train)\nidmap.train(train)\n\n# Add vectors with application-specific IDs\nxb = np.random.randn(500_000, d).astype('float32'); faiss.normalize_L2(xb)\nids = np.arange(100_000, 100_000 + xb.shape[0], dtype='int64')\nidmap.add_with_ids(xb, ids)\n\n# Search returns your IDs\nxq = np.random.randn(1000, d).astype('float32'); faiss.normalize_L2(xq)\nD, I = idmap.search(xq, k=10)\n</code></pre> <p>Notes</p> <ul> <li>Use <code>IndexIDMap2</code> (64\u2011bit IDs) for large corpora.</li> <li>Persist CPU form (see A6) and keep a parallel <code>.ids</code> copy if you ever unwrap the map. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a2-indexrefineflat-refine-ann-hits-with-exact-scoring","title":"A2) <code>IndexRefineFlat</code> \u2014 refine ANN hits with exact scoring","text":"<p>Why: Get IVF/graph latency with exact cosine/IP re\u2011ranking.</p> <pre><code># Build a fast coarse index (e.g., IVF,PQ) then refine with Flat\ncoarse = faiss.index_factory(d, \"IVF8192,PQ64\", faiss.METRIC_INNER_PRODUCT)\ncoarse.train(train); coarse.add(xb)\n\nrefined = faiss.IndexRefineFlat(coarse)   # wraps the coarse index with an exact refiner\nrefined.kfactor = 2.0                     # search 2\u00d7k in coarse stage, re-rank down to k\n\nD, I = refined.search(xq, k=10)\n</code></pre> <p>Notes</p> <ul> <li>On GPU you\u2019ll typically run the coarse stage on device, then refine on device or CPU depending on memory; the wrapper manages the sequence.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a3-indexpretransform-transforms-opqmatrix-pcamatrix","title":"A3) <code>IndexPreTransform</code> + transforms (<code>OPQMatrix</code>, <code>PCAMatrix</code>)","text":"<p>Why: Pre\u2011rotate or reduce dimension inside the index for better PQ fit.</p> <pre><code># 1) Build transforms: PCA (optional) then OPQ\npca = faiss.PCAMatrix(d_in=2560, d_out=1024, eigen_power=-0.5)  # example PCA step\nopq = faiss.OPQMatrix(d_out=1024, m=64)  # OPQ with 64 subquantizers\n\n# 2) Chain transforms in a vector\nvt = faiss.VectorTransformChain()\nvt.append(pca)\nvt.append(opq)\n\n# 3) Base index AFTER transforms (note d=1024 now)\nbase = faiss.index_factory(1024, \"IVF8192,PQ64\", faiss.METRIC_INNER_PRODUCT)\n\n# 4) Wrap into IndexPreTransform\npre = faiss.IndexPreTransform(vt, base)\n\n# 5) Train on original-dim data; transform is applied internally\npre.train(train)       # 'train' runs PCA/OPQ fitting + IVF/PQ training\npre.add(xb)\nD, I = pre.search(xq, 10)\n</code></pre> <p>Notes</p> <ul> <li>For our default blueprint we already use OPQ64; <code>IndexPreTransform</code> is the explicit form of that pipeline. Keep cosine/IP consistency: normalize before train/add/query. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a4-clustering-kmeans-build-coarse-quantizers-explicitly","title":"A4) <code>Clustering</code> (K\u2011means) \u2014 build coarse quantizers explicitly","text":"<p>Why: Manual control when you don\u2019t use <code>index_factory()</code>.</p> <pre><code># Train IVF coarse quantizer centroids explicitly with K-means\nnlist = 8192\nkmeans = faiss.Clustering(d, nlist)\nkmeans.niter = 25; kmeans.verbose = True\n\nquantizer = faiss.IndexFlatIP(d)        # cosine via normalization + IP\nkmeans.train(train, quantizer)\n\n# Use trained quantizer to build IVF, then attach PQ\nivf = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\nivf.train(train)\nivf.add(xb)\n</code></pre> <p>Notes</p> <ul> <li><code>index_factory()</code> automates this, but direct <code>Clustering</code> is useful for specialized training schedules.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a5-parameterspace-gpuparameterspace-named-tuning-knobs","title":"A5) <code>ParameterSpace</code> / <code>GpuParameterSpace</code> \u2014 named tuning knobs","text":"<p>Why: Set parameters (e.g., <code>nprobe</code>) by name, works across families.</p> <pre><code># CPU\nps = faiss.ParameterSpace()\nps.set_index_parameter(base, \"nprobe\", 64)\n\n# GPU\ngps = faiss.GpuParameterSpace(); gps.initialize(gpu_index)\ngps.set_index_parameter(gpu_index, \"nprobe\", 64)   # same name on GPU\n</code></pre> <p>Notes</p> <ul> <li>Prefer these over ad\u2011hoc setters; helps keep code index\u2011agnostic.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a6-persistence-write_index-read_index","title":"A6) Persistence \u2014 <code>write_index</code> / <code>read_index</code>","text":"<p>Why: Save CPU index for fast reload; re\u2011clone to GPU at runtime.</p> <pre><code>faiss.write_index(idmap, \"/data/faiss/shard_000.idx\")          # save CPU index (any type)\nidmap2 = faiss.read_index(\"/data/faiss/shard_000.idx\")         # load CPU index\n</code></pre> <p>Notes</p> <ul> <li>Don\u2019t save GPU indexes; reconstruct by cloning the CPU index on startup. Track paths and config in DuckDB (<code>faiss_indexes</code>). </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a7-range-search-range_search-aka-radius-search","title":"A7) Range search \u2014 <code>range_search</code> (a.k.a. radius search)","text":"<p>Why: Get all neighbors within a distance threshold instead of top\u2011K.</p> <pre><code>radius = 0.1  # for cosine/IP, pick meaningful threshold on normalized vectors\nrs = faiss.RangeSearchResult(xq.shape[0])\nidmap.range_search(xq, radius, rs)\n\n# Extract result per query\nlims, D, I = rs.lims, rs.distances, rs.labels\nfor qi in range(xq.shape[0]):\n    begin, end = lims[qi], lims[qi+1]\n    neigh_ids = I[begin:end]\n    neigh_dist = D[begin:end]\n</code></pre> <p>Notes</p> <ul> <li>Range search is less common for large\u2011scale serving, but handy for thresholded neighbor graphs.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a8-filtered-search-idselector-searchparametersivf","title":"A8) Filtered search \u2014 <code>IDSelector*</code> + <code>SearchParametersIVF</code>","text":"<p>Why: Exclude/allow subsets of IDs at query time.</p> <pre><code># Build a selector (ids in [lo, hi))\nsel = faiss.IDSelectorRange(lo=100_000, hi=150_000)\n\n# Tie selector to IVF search params\nsp = faiss.SearchParametersIVF()\nsp.nprobe = 64\nsp.sel = sel\n\n# Call the 3-arg search to pass params\nD, I = idmap.search(xq, 10, params=sp)\n</code></pre> <p>Notes</p> <ul> <li>With cuVS\u2011backed GPU indexes enabled (<code>use_cuvs=True</code> when cloning/constructing), FAISS converts these selectors into cuVS filters under the hood (your build includes the converter). </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a9-deletions-remove_ids","title":"A9) Deletions \u2014 <code>remove_ids</code>","text":"<p>Why: Remove a set of IDs (e.g., retractions, updates).</p> <pre><code>to_remove = faiss.IDSelectorRange(200_000, 210_000)  # OR IDSelectorBatch([...])\nn_removed = idmap.remove_ids(to_remove)\n</code></pre> <p>Notes</p> <ul> <li>IVF/PQ structures become \u201choley\u201d over time; periodic rebuilds (or re\u2011add to a fresh shard) keep performance consistent.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a10-reconstruction-reconstruct-reconstruct_n","title":"A10) Reconstruction \u2014 <code>reconstruct</code>, <code>reconstruct_n</code>","text":"<p>Why: Retrieve vector approximations (exact for Flat/HNSW; approximate for PQ).</p> <pre><code>vec = idmap.reconstruct(int(I[0,0]))\nmat = idmap.reconstruct_n(0, 100)   # first 100 vectors\n</code></pre>"},{"location":"explanations/251025_FAISS_whl_overview/#a11-cpu-parallelism-omp_set_num_threads","title":"A11) CPU parallelism \u2014 <code>omp_set_num_threads</code>","text":"<p>Why: Pin thread count for CPU add/search/training.</p> <pre><code>faiss.omp_set_num_threads(14)  # match your server threads\n</code></pre>"},{"location":"explanations/251025_FAISS_whl_overview/#b-faiss-gpu-multigpu-functions-deeper-with-cuvs-enablement","title":"B) FAISS \u2014 GPU &amp; multi\u2011GPU functions (deeper, with cuVS enablement)","text":"<p>Reminder: In your wheel, FAISS GPU indexes can dispatch to cuVS for Flat, IVF\u2011Flat, IVF\u2011PQ, CAGRA, Binary\u2011CAGRA when you set <code>use_cuvs=True</code> on the GPU cloner or the GPU index config, and the cuVS libraries are preloaded. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#b1-cpugpu-cloning-single-multigpu","title":"B1) CPU\u2192GPU cloning (single &amp; multi\u2011GPU)","text":"<pre><code>import faiss\n\n# 1) Single GPU (cuVS-first, safe fallback)\nco = faiss.GpuClonerOptions(); co.use_cuvs = True\ntry:\n    gpu_index = faiss.index_cpu_to_gpu(res, 0, idmap, co)\nexcept RuntimeError:\n    co.use_cuvs = False\n    gpu_index = faiss.index_cpu_to_gpu(res, 0, idmap, co)\n\n# 2) Replicated (QPS-scale)\nngpu = faiss.get_num_gpus()\nresources = [faiss.StandardGpuResources() for _ in range(ngpu)]\nreplicas = faiss.IndexReplicas()\nfor dev in range(ngpu):\n    replicas.addIndex(faiss.index_cpu_to_gpu(resources[dev], dev, idmap, co))\n\n# 3) Sharded (capacity-scale)\nshards = faiss.IndexShards(d, threaded=True, successive_ids=False)\n# Build per-shard CPU indexes first, then clone each with co.use_cuvs=True and add to 'shards'\n</code></pre> <p>Notes</p> <ul> <li>Replicas improve QPS; shards expand capacity; wrap both under one logical index in your registry and fan\u2011out search. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#b2-direct-gpu-constructors-with-gpuindexconfiguse_cuvs","title":"B2) Direct GPU constructors with <code>GpuIndex*Config.use_cuvs</code>","text":"<p>Flat</p> <pre><code>cfg = faiss.GpuIndexFlatConfig(); cfg.use_cuvs = True\ngpu_flat = faiss.GpuIndexFlatIP(res, d, cfg)\n</code></pre> <p>IVF\u2011Flat / IVF\u2011PQ</p> <pre><code>ivf_cfg = faiss.GpuIndexIVFConfig(); ivf_cfg.use_cuvs = True\n# Pass ivf_cfg to the appropriate GpuIndexIVF* constructor for your metric/quantizer\n</code></pre> <p>CAGRA (graph ANN)</p> <pre><code># Prefer CPU\u2192GPU cloning unless you need specific GPU-only build options.\n# If constructing directly, set the config that inherits GpuIndexConfig:\n# cagra_cfg = faiss.GpuIndexCagraConfig(); cagra_cfg.use_cuvs = True\n# gpu_cagra = faiss.GpuIndexCagra(res, d, cagra_cfg)\n</code></pre> <p>Binary\u2011CAGRA</p> <ul> <li>Use the cloner (<code>GpuClonerOptions.use_cuvs=True</code>) when moving from CPU binary graph indexes to GPU.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#b3-gpu-distance-primitives-bfknn-knn_gpubfknn","title":"B3) GPU distance primitives \u2014 bfKNN (<code>knn_gpu</code>/<code>bfKnn</code>)","text":"<p>Exact GPU kNN (cuVS\u2011aware):</p> <pre><code>params = faiss.GpuDistanceParams(); params.metric = faiss.METRIC_INNER_PRODUCT\nparams.k = 10; params.dims = d\nuse_cuvs = bool(faiss.should_use_cuvs(params))     # central probe, then:\n\nD, I = faiss.knn_gpu(res, xq.astype('float32'), xb.astype('float32'),\n                     k=10, metric=faiss.METRIC_INNER_PRODUCT, use_cuvs=use_cuvs)\n</code></pre> <p>Notes</p> <ul> <li>Your build routes bfKNN to cuVS when possible; otherwise FAISS kernels are used. Preload cuVS so the probe can return True when viable. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#b4-gpu-tuning-knobs-gpuparameterspace-configs","title":"B4) GPU tuning knobs \u2014 <code>GpuParameterSpace</code> &amp; configs","text":"<ul> <li><code>GpuParameterSpace.set_index_parameter(gpu_index, \"nprobe\", 64)</code> \u2014 uniform way to set IVF probes.</li> <li> <p>IVFPQ config highlights:</p> </li> <li> <p><code>usePrecomputedTables</code> \u2014 faster scans at higher memory.</p> </li> <li><code>useFloat16LookupTables</code> \u2014 lower memory, slight precision trade\u2011off; useful for large <code>m</code> or tight VRAM budgets.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#c-faiss-contrib-helpers-youll-likely-use","title":"C) FAISS contrib helpers you\u2019ll likely use","text":"<p>(These ship in the python package and are handy for pipelines.)</p> <ul> <li><code>contrib.factory_tools</code> \u2014 convenience builders &amp; validators for factory strings.</li> <li><code>contrib.ivf_tools</code> \u2014 IVF diagnostics, centroid utilities.</li> <li><code>contrib.big_batch_search</code> \u2014 large\u2011batch search helpers to pipeline host\u2194device copies.</li> <li><code>contrib.ondisk</code> \u2014 on\u2011disk inverted lists for huge CPU\u2011side corpora.</li> </ul> <p>Use them to simplify scripts; your production path should still keep CPU index persisted + GPU clone at runtime as the source of truth. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#d-cuvs-functions-modules-we-hadnt-exemplified-now-with-code","title":"D) cuVS \u2014 functions &amp; modules we hadn\u2019t exemplified (now with code)","text":"<p>cuVS runs entirely on GPU with a consistent <code>build / extend / search / save / load</code> shape per algorithm. Provide a <code>Resources</code> handle to share a stream and avoid implicit syncs; pass device arrays for best performance. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#d1-neighborsbrute_force-exact-knn-baseline-qa","title":"D1) <code>neighbors.brute_force</code> \u2014 exact kNN (baseline &amp; QA)","text":"<pre><code>import numpy as np\nfrom cuvs.common import Resources\nfrom cuvs.neighbors import brute_force\nfrom pylibraft.common import device_ndarray\n\nh = Resources()\nxb = device_ndarray(np.random.rand(1_000_000, 2560).astype(np.float32))\nxq = device_ndarray(np.random.rand(10_000,    2560).astype(np.float32))\n\nindex = brute_force.build(brute_force.IndexParams(metric=\"sqeuclidean\"), xb, resources=h)\n\nD = device_ndarray(np.empty((xq.shape[0], 10), np.float32))\nI = device_ndarray(np.empty((xq.shape[0], 10), np.int64))\nbrute_force.search(brute_force.SearchParams(metric=\"sqeuclidean\"),\n                   index, xq, k=10, distances=D, neighbors=I, resources=h)\n\n# Incremental ingest\nxb2 = device_ndarray(np.random.rand(200_000, 2560).astype(np.float32))\nbrute_force.extend(index, xb2, resources=h)\n\n# Persistence\nbrute_force.save(index, \"/tmp/cuvs_bruteforce.idx\")\nindex = brute_force.load(\"/tmp/cuvs_bruteforce.idx\", resources=h)\nh.sync()\n</code></pre> <p>Notes</p> <ul> <li>Good for ground truth during tuning and for small datasets. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#d2-neighborsivf_flat-faisslike-ivf-on-gpu","title":"D2) <code>neighbors.ivf_flat</code> \u2014 FAISS\u2011like IVF on GPU","text":"<pre><code>from cuvs.neighbors import ivf_flat\n\nh = Resources()\nxb = device_ndarray(np.random.rand(10_000, 2560).astype(np.float32))\nxq = device_ndarray(np.random.rand(1_000,  2560).astype(np.float32))\n\nidx = ivf_flat.build(ivf_flat.IndexParams(n_lists=8192, metric=\"sqeuclidean\"), xb, resources=h)\n\nD = device_ndarray(np.empty((xq.shape[0], 10), np.float32))\nI = device_ndarray(np.empty((xq.shape[0], 10), np.int64))\nivf_flat.search(ivf_flat.SearchParams(n_probes=64), idx, xq, k=10,\n                distances=D, neighbors=I, resources=h)\n\n# Add more\nivf_flat.extend(idx, device_ndarray(np.random.rand(5_000, 2560).astype(np.float32)), resources=h)\n\n# Save / load\nivf_flat.save(idx, \"/tmp/cuvs_ivfflat.idx\")\nidx2 = ivf_flat.load(\"/tmp/cuvs_ivfflat.idx\", resources=h)\nh.sync()\n</code></pre> <p>Notes</p> <ul> <li>Parameter names mirror FAISS concepts: <code>n_lists</code>, <code>n_probes</code>, metric. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#d3-neighborsivf_pq-ivf-with-product-quantization","title":"D3) <code>neighbors.ivf_pq</code> \u2014 IVF with Product Quantization","text":"<p>(We\u2019d shown build/search before; here\u2019s <code>extend/save/load</code> and host output conversion.) </p> <pre><code>from cuvs.neighbors import ivf_pq\n\nh = Resources()\nidx = ivf_pq.build(ivf_pq.IndexParams(n_lists=8192, metric=\"sqeuclidean\", pq_bits=8),\n                   xb, resources=h)\n\n# Extend incrementally\nivf_pq.extend(idx, device_ndarray(np.random.rand(20_000, 2560).astype(np.float32)), resources=h)\n\n# Search to host arrays (optional)\nD_dev = device_ndarray(np.empty((xq.shape[0], 10), np.float32))\nI_dev = device_ndarray(np.empty((xq.shape[0], 10), np.int64))\nivf_pq.search(ivf_pq.SearchParams(n_probes=64), idx, xq, k=10, distances=D_dev, neighbors=I_dev, resources=h)\nD_host, I_host = D_dev.copy_to_host(), I_dev.copy_to_host()\n\n# Save / load\nivf_pq.save(idx, \"/tmp/cuvs_ivfpq.idx\")\nidx = ivf_pq.load(\"/tmp/cuvs_ivfpq.idx\", resources=h)\nh.sync()\n</code></pre>"},{"location":"explanations/251025_FAISS_whl_overview/#d4-neighborscagra-highrecall-graph-ann","title":"D4) <code>neighbors.cagra</code> \u2014 high\u2011recall graph ANN","text":"<pre><code>from cuvs.neighbors import cagra\n\nh = Resources()\nxb = device_ndarray(np.random.rand(2_000_000, 2560).astype(np.float32))\nxq = device_ndarray(np.random.rand(20_000,    2560).astype(np.float32))\n\nidx = cagra.build(cagra.IndexParams(metric=\"sqeuclidean\"), xb, resources=h)\n\nD = device_ndarray(np.empty((xq.shape[0], 10), np.float32))\nI = device_ndarray(np.empty((xq.shape[0], 10), np.int64))\ncagra.search(cagra.SearchParams(), idx, xq, k=10, distances=D, neighbors=I, resources=h)\n\n# Extend / persist\ncagra.extend(idx, device_ndarray(np.random.rand(100_000, 2560).astype(np.float32)), resources=h)\ncagra.save(idx, \"/tmp/cuvs_cagra.idx\")\nidx = cagra.load(\"/tmp/cuvs_cagra.idx\", resources=h)\nh.sync()\n</code></pre> <p>Notes</p> <ul> <li>CAGRA is excellent for tight latency at high recall; memory\u2011heavier than PQ. Use it for premium tiers. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#d5-neighborshnsw","title":"D5) <code>neighbors.hnsw</code>","text":"<pre><code>from cuvs.neighbors import hnsw\n\nh = Resources()\nidx = hnsw.build(hnsw.IndexParams(metric=\"sqeuclidean\"), xb, resources=h)\nhnsw.search(hnsw.SearchParams(), idx, xq, k=10,\n            distances=D, neighbors=I, resources=h)\nhnsw.save(idx, \"/tmp/cuvs_hnsw.idx\")\n</code></pre> <p>Notes</p> <ul> <li>A familiar graph ANN baseline on GPU. Pick either CAGRA (higher perf) or HNSW depending on constraints. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#d6-filters-refine-tiered-indexes-highlevel-patterns","title":"D6) Filters, refine, tiered indexes (high\u2011level patterns)","text":"<ul> <li>Filters \u2014 apply post\u2011search restrictions or pre\u2011filter candidates with <code>neighbors.filters</code> utilities (e.g., by ID set). Use them for policy constraints or tenancy. </li> <li>Refine \u2014 re\u2011score ANN candidates with an exact step (<code>neighbors.refine</code>) for accuracy boosts. Good when you need Flat re\u2011ranking on a subset. </li> <li>Tiered index \u2014 orchestrate multi\u2011stage pipelines (e.g., IVF\u2011PQ \u2192 refine) using <code>neighbors.tiered_index</code> helpers. </li> </ul> <p>(Function names follow the <code>build/search/extend/save/load</code> theme; wire them like the examples above.)</p>"},{"location":"explanations/251025_FAISS_whl_overview/#d7-multigpu-neighborsmg-distributed-ivfpq-ivfflat-cagra","title":"D7) Multi\u2011GPU (<code>neighbors.mg</code>) \u2014 distributed IVF\u2011PQ / IVF\u2011Flat / CAGRA","text":"<p>Conceptual usage (keep the same <code>Resources</code> concept per device and use NCCL):</p> <pre><code>from cuvs.neighbors.mg import ivf_pq as mg_ivfpq\n# Build: mg_ivfpq.build(mg_ivfpq.IndexParams(...), dataset_per_gpu, resources=handles_per_gpu)\n# Search: mg_ivfpq.search(..., k=..., resources=handles_per_gpu)\n</code></pre> <p>Notes</p> <ul> <li>Requires NCCL. Use this path when one GPU\u2019s memory is insufficient or you need cluster\u2011like throughput while staying on a single multi\u2011GPU box. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#d8-distancepairwise_distance-gpu-distance-matrix","title":"D8) <code>distance.pairwise_distance</code> \u2014 GPU distance matrix","text":"<pre><code>from cuvs import distance\nDx = distance.pairwise_distance(xq, xb, metric=\"sqeuclidean\", resources=h)  # Dx on device by default\n</code></pre> <p>Notes</p> <ul> <li>Handy for diagnostics, unit tests, or custom scoring layers. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#d9-clusterkmeans-gpu-kmeans-centers-predict-cost","title":"D9) <code>cluster.kmeans</code> \u2014 GPU KMeans (centers, predict, cost)","text":"<pre><code>from cuvs.cluster import kmeans\n\ncenters, inertia, n_iters = kmeans.fit(kmeans.KMeansParams(n_clusters=8192), xb, resources=h)\nlabels = kmeans.predict(xb, centers, resources=h)\ncost = kmeans.cluster_cost(xb, centers, resources=h)\n</code></pre> <p>Notes</p> <ul> <li>Use for custom IVF training or other clustering tasks; interoperates with device arrays. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#e-libcuvs-loader-functions-env-youll-use-in-code","title":"E) libcuvs loader \u2014 functions &amp; env you\u2019ll use in code","text":"<p>Preload cuVS and RAPIDS libs once per process before any FAISS/cuVS calls to guarantee symbol resolution and allocator setup. </p> <pre><code>from libcuvs import load_library\nhandles = load_library()\nprint(\"Loaded:\", [h._name for h in handles])  # libcuvs.so, libcuvs_c.so, librmm.so, rapids_logger.so\n</code></pre> <p>Environment knobs (optional):</p> <ul> <li><code>RAPIDS_LIBCUVS_PREFER_SYSTEM_LIBRARY=true</code> \u2192 prefer system libs over wheel\u2011bundled copies.</li> <li>HybridSearch\u2019s <code>_ensure_cuvs_loader_path()</code> adds all <code>lib64/</code> dirs to <code>LD_LIBRARY_PATH</code> for child processes (inheritance is useful for workers). </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#f-endtoend-patterns-that-combine-the-above","title":"F) End\u2011to\u2011end patterns that combine the above","text":""},{"location":"explanations/251025_FAISS_whl_overview/#f1-filtered-refined-ivfpq-on-gpu-with-cuvs","title":"F1) Filtered, refined IVF\u2011PQ on GPU with cuVS","text":"<pre><code># 0) Preload cuVS (once)\nfrom libcuvs import load_library; load_library()                               # loader. :contentReference[oaicite:25]{index=25}\nimport faiss, numpy as np\n\nd=2560\ncpu = faiss.index_factory(d, \"OPQ64,IVF8192,PQ64\", faiss.METRIC_INNER_PRODUCT)\ntrain = np.random.randn(10_000_000, d).astype('float32'); faiss.normalize_L2(train)\ncpu.train(train)\n\n# ID map to carry your IDs\nidmap = faiss.IndexIDMap2(cpu)\nxb  = np.random.randn(20_000_000, d).astype('float32'); faiss.normalize_L2(xb)\nids = np.arange(1, 1+xb.shape[0], dtype='int64')\nidmap.add_with_ids(xb, ids)\n\n# Clone to GPU with cuVS enabled (fallback safe)\nres = faiss.StandardGpuResources(); res.setTempMemory(1_200_000_000); res.setPinnedMemory(512_000_000)\nco  = faiss.GpuClonerOptions(); co.use_cuvs=True\ntry: gpu = faiss.index_cpu_to_gpu(res, 0, idmap, co)\nexcept RuntimeError:\n    co.use_cuvs=False; gpu = faiss.index_cpu_to_gpu(res, 0, idmap, co)\n\n# Filter: only IDs in a range, then search &amp; refine\nsp = faiss.SearchParametersIVF(); sp.nprobe = 64; sp.sel = faiss.IDSelectorRange(100_000, 500_000)\nrefiner = faiss.IndexRefineFlat(gpu); refiner.kfactor = 2.0\n\nxq = np.random.randn(1000, d).astype('float32'); faiss.normalize_L2(xq)\nD, I = refiner.search(xq, 10, params=sp)\n</code></pre> <p>Why this works well for you</p> <ul> <li>Cosine via normalization + IP (2560\u2011d per architecture). </li> <li>cuVS path engaged for IVF\u2011PQ when available; otherwise FAISS kernels. </li> <li>Filter converted to cuVS filter internally in your build. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#f2-direct-cuvs-multigpu-ivfpq-outline","title":"F2) Direct cuVS multi\u2011GPU IVF\u2011PQ (outline)","text":"<pre><code># Pseudocode outline (patterns match single-GPU examples; use mg.ivf_pq)\nfrom cuvs.neighbors.mg import ivf_pq as mg_ivfpq\n# handles = [Resources() per GPU]; datasets split per GPU as device arrays\n# idx = mg_ivfpq.build(mg_ivfpq.IndexParams(n_lists=8192, metric=\"sqeuclidean\", pq_bits=8),\n#                      datasets, resources=handles)\n# mg_ivfpq.search(mg_ivfpq.SearchParams(n_probes=64), idx, queries, k=10,\n#                 distances=..., neighbors=..., resources=handles)\n</code></pre> <p>Notes</p> <ul> <li>Reach for multi\u2011GPU cuVS when a single GPU\u2019s VRAM is the bottleneck. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#g-quick-checklist-for-agents-so-you-dont-miss-a-function-again","title":"G) Quick checklist for agents (so you don\u2019t miss a function again)","text":"<ul> <li>Always normalize embeddings for cosine/IP.</li> <li>Use <code>IndexIDMap2</code> to own IDs; <code>IndexRefineFlat</code> when you need exact re\u2011ranking.</li> <li>Set knobs with <code>ParameterSpace</code> / <code>GpuParameterSpace</code>.</li> <li>For filters, use <code>IDSelector*</code> + <code>SearchParametersIVF</code>; your build converts to cuVS automatically when cuVS is enabled. </li> <li>Persist CPU index with <code>write_index</code>; re\u2011clone to GPU on startup.</li> <li>Preload cuVS (<code>libcuvs.load_library()</code>), then set <code>use_cuvs=True</code> on cloner/config for Flat, IVF\u2011Flat, IVF\u2011PQ, CAGRA, Binary\u2011CAGRA; guard with try/except or <code>should_use_cuvs</code> for bfKNN. </li> <li>For direct cuVS: use <code>build/extend/search/save/load</code> in <code>neighbors.{ivf_flat, ivf_pq, cagra, hnsw, brute_force}</code> with device arrays and a shared <code>Resources</code> handle. </li> <li>Keep defaults from the architecture doc (2560\u2011d, <code>OPQ64,IVF8192,PQ64</code>, <code>nprobe=64</code>, shards \u226410\u202fM vectors). </li> </ul> <p>If you want, I can also package these into a reference Jupyter notebook that runs each snippet end\u2011to\u2011end (with small synthetic data) and prints sanity checks (dimensions, top\u2011K shapes, cuVS enabled flags).</p>"},{"location":"explanations/251025_HighLevelArchitecture/","title":"Implementation-Grade Architecture Overview","text":"<p>Below is the fully elaborated, implementation\u2011grade architecture for your end\u2011to\u2011end, single\u2011machine system. It updates every topic from the original plan and adds all new constraints you set:</p> <ul> <li>OS/Host: Ubuntu 24.04 (single box, no external servers).</li> <li>GPU: NVIDIA RTX 5090 (CUDA 13.0 toolchain).</li> <li>CPU/RAM: AMD 9950X (16 cores), 192\u202fGB RAM.</li> <li>Python: 3.13.</li> <li>PyTorch: 2.9 (CUDA 13 build).</li> <li>vLLM: latest pre\u2011release that supports CUDA 13.</li> <li>DuckDB: \u2265\u202f1.4.1 (interpreting your \u201c1.41\u201d as 1.4.1).</li> <li>Dense embeddings: Qwen3\u2011Embedding\u20114B at 2560\u2011dimensional output.</li> <li>Sparse embeddings: BM25 + SPLADE\u2011v3 (GPU).</li> <li>PDF\u2192DocTags &amp; chunking: Docling VLM (Granite\u2011Docling) + Docling HybridChunker.</li> <li>All embeddings in Parquet (no JSONL at rest).</li> <li>Vector indexing &amp; ops: FAISS GPU with cuVS enabled.</li> <li>Model serving: a single logical endpoint fronting two local vLLM processes (Granite\u2011Docling VLM + Qwen3\u2011Embedding\u20114B), routed by Nginx (necessary because one vLLM process \u2259 one base model).</li> <li>Registry: all artifacts (PDFs, DocTags, chunks, embeddings, indices, ontologies, links) registered in local DuckDB.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#0-big-picture-endtoend-singlebox","title":"0) Big picture (end\u2011to\u2011end, single\u2011box)","text":"<pre><code>Topic \u2192 (PyAlex) Harvest &amp; OA PDF Download (with fallbacks)\n     \u2192 Docling VLM (Granite\u2011Docling) \u2192 DocTags\n     \u2192 Docling HybridChunker \u2192 Chunk Parquet\n     \u2192 Dense (Qwen3\u2011Embedding\u20114B, 2560\u2011d via vLLM) \u2192 Parquet \u2192 FAISS (GPU, cuVS)\n     \u2192 Sparse (SPLADE\u2011v3, GPU \u2192 Parquet \u2192 Lucene impact) + BM25 (Lucene)\n     \u2192 Ontology ingest \u2192 Concept catalog + embeddings (dense 2560\u2011d + SPLADE)\n     \u2192 Chunk\u2013Concept linker \u2192 Assertions (Parquet) \u2192 KG (Neo4j local)\n     \u2192 Hybrid search API (FAISS + BM25 + SPLADE + KG\u2011aware rerank)\n     \u2192 Everything registered in DuckDB (\u22651.4.1)\n</code></pre> <p>Architectural style: Ports &amp; Adapters (Hexagonal) + Domain contracts (Pydantic v2) + Plugin registry (entry points) + Immutable, content\u2011addressed artifacts. All heavy compute runs locally (no remote compute/storage).</p>"},{"location":"explanations/251025_HighLevelArchitecture/#1-design-principles-concretized","title":"1) Design principles (concretized)","text":"<ul> <li>Interface\u2011first: every subsystem exposes an ABC (Abstract Base Class). Impl classes are pluggable via entry\u2011points.</li> <li>Encapsulation &amp; cohesion: one public fa\u00e7ade per package; internals hidden. Each module owns a single concern.</li> <li>Idempotency: outputs keyed by content hashes; re\u2011runs never duplicate artifacts.</li> <li>Determinism: global <code>seed=42</code>, fixed training samples for FAISS, fixed chunking parameters.</li> <li>All embeddings in Parquet: columnar, compressed with ZSTD=6, row_group=4096; no JSONL persisted.</li> <li>Observability from day one: OpenTelemetry traces; structured logs with artifact IDs; Prometheus counters/histograms.</li> <li>Local\u2011only: no remote indices or vector DBs; FAISS, Pyserini, Neo4j, DuckDB all on the box.</li> <li>Security (local): vLLM bound to localhost; Nginx fronts a single endpoint; API\u2011key auth for the search API.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#2-repository-layout-monorepo-independently-ownable-packages","title":"2) Repository layout (monorepo; independently ownable packages)","text":"<pre><code>/src\n  /kgforge_common            # contracts, IDs, hashing, config, utils\n  /download                  # PyAlex harvester + OA PDF downloader + fallbacks\n  /docling                   # VLM convert-to-DocTags + HybridChunker wrappers\n  /embeddings_dense          # vLLM client, Qwen3-Embedding-4B (2560-d)\n  /embeddings_sparse         # SPLADE-v3 GPU encoder (Parquet) + BM25/SPLADE indices (Pyserini)\n  /vectorstore_faiss         # FAISS GPU/cuVS index build/search adapters\n  /ontology                  # OWL/OBO/SKOS loaders, normalization, concept embeddings\n  /linking                   # candidate gen, scoring, calibration, assertions\n  /kg_builder                # Neo4j adapter; nodes/edges upsert\n  /search_api                # FastAPI; hybrid retrieval + KG-aware rerank; OpenAPI\n  /orchestration             # Prefect flows &amp; CLI commands (local)\n  /registry                  # DuckDB schema, migrations, dataset registration\n  /observability             # OTEL + Prometheus exporters\n/tests\n/config                      # YAML config(s), ngnix.conf, systemd units\n/scripts                     # bootstrap &amp; build scripts (CUDA 13, FAISS+cuVS, vLLM)\n</code></pre> <p>Packaging: <code>pyproject.toml</code> (Poetry or uv). Entry\u2011points register providers:</p> <pre><code>[project.entry-points.kgforge.plugins]\ndense.qwen3 = \"embeddings_dense.qwen3:Qwen3Embedder\"\nsparse.splade_v3 = \"embeddings_sparse.splade:SPLADEv3Encoder\"\nsparse.bm25 = \"embeddings_sparse.bm25:BM25Index\"\ndocling.vlm = \"docling.vlm:GraniteDoclingVLM\"\nchunker.docling_hybrid = \"docling.hybrid:HybridChunker\"\nvector.faiss_gpu = \"vectorstore_faiss.gpu:FaissGpuIndex\"\ngraph.neo4j = \"kg_builder.neo4j:Neo4jStore\"\nontology.loader = \"ontology.loader:OntologyLoader\"\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#3-core-data-contracts-pydantic-v2-contentaddressed-ids","title":"3) Core data contracts (Pydantic v2; content\u2011addressed IDs)","text":"<p>Keep models lean. Payloads (large text, vectors) live in Parquet files; rows refer to them via IDs.</p> <pre><code># kgforge_common/models.py (Python 3.13, Pydantic v2)\nfrom pydantic import BaseModel, Field, AwareDatetime\nfrom typing import Optional, List, Dict, Literal\nfrom dataclasses import dataclass\n\nId = str  # URN-like opaque IDs, stable and content-addressed\n\nclass Doc(BaseModel):\n    id: Id                           # urn:doc:sha256:&lt;16b&gt;\n    openalex_id: Optional[str] = None\n    doi: Optional[str] = None\n    arxiv_id: Optional[str] = None\n    pmcid: Optional[str] = None\n    title: str\n    authors: List[str] = []\n    pub_date: Optional[str] = None   # ISO8601\n    license: Optional[str] = None\n    language: Optional[str] = \"en\"\n    pdf_uri: str                     # local path (e.g., /data/pdfs/&lt;id&gt;.pdf)\n    source: str                      # 'openalex', 'arxiv', 'pmc', ...\n    content_hash: str                # sha256 of canonical text (after DocTags-&gt;text)\n    created_at: AwareDatetime\n\nclass DoctagsAsset(BaseModel):\n    doc_id: Id\n    doctags_uri: str                 # /data/doctags/&lt;doc_id&gt;.dt.json.zst\n    pages: int\n    vlm_model: str                   # granite-docling-258M\n    vlm_revision: str                # 'untied' if used\n    avg_logprob: Optional[float] = None\n    created_at: AwareDatetime\n\nclass Chunk(BaseModel):\n    id: Id                           # urn:chunk:&lt;doc_hash&gt;:&lt;start&gt;-&lt;end&gt;\n    doc_id: Id\n    section: Optional[str]\n    start_char: int\n    end_char: int\n    tokens: int\n    doctags_span: Dict[str, int]     # {node_id, start, end}\n    created_at: AwareDatetime\n    dataset_id: str                  # backpointer to Parquet dataset\n\nclass DenseVectorMeta(BaseModel):\n    chunk_id: Id\n    model: str                       # 'Qwen3-Embedding-4B'\n    run_id: str\n    dim: int                         # 2560\n    l2_norm: float\n    created_at: AwareDatetime\n\nclass SparseVectorMeta(BaseModel):\n    chunk_id: Id\n    model: str                       # 'SPLADE-v3-distilbert'\n    run_id: str\n    nnz: int\n    created_at: AwareDatetime\n\nclass Concept(BaseModel):\n    id: Id                           # urn:concept:&lt;ontology&gt;:&lt;curie&gt;\n    ontology: str\n    pref_label: str\n    alt_labels: List[str] = []\n    definition: Optional[str] = None\n    parents: List[Id] = []\n    meta: Dict[str, str] = {}\n\nclass LinkAssertion(BaseModel):\n    id: Id                           # urn:assert:&lt;chunk_id&gt;:&lt;concept_id&gt;@&lt;run_id&gt;\n    chunk_id: Id\n    concept_id: Id\n    score: float\n    decision: Literal['link','reject','uncertain']\n    evidence_span: Optional[str] = None\n    features: Dict[str, float] = {}  # dense_sim, sparse_sim, lexical_overlap, depth_bonus\n    run_id: str\n    created_at: AwareDatetime\n</code></pre> <p>ID scheme (deterministic)</p> <ul> <li><code>doc_id = urn:doc:sha256:&lt;first16 bytes of sha256 canonical_text, base32&gt;</code></li> <li><code>chunk_id = urn:chunk:&lt;doc_hash&gt;:&lt;start&gt;-&lt;end&gt;</code></li> <li><code>dense vec id = urn:vec:&lt;chunk_id&gt;:qwen3@&lt;run_id&gt;</code></li> <li><code>sparse id = urn:sparse:&lt;chunk_id&gt;:splade_v3@&lt;run_id&gt;</code></li> <li><code>concept id = urn:concept:&lt;ontology&gt;:&lt;curie&gt;</code></li> <li><code>assertion id = urn:assert:&lt;chunk_id&gt;:&lt;concept_id&gt;@&lt;run_id&gt;</code></li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#4-storage-layers-adapters-ports","title":"4) Storage layers &amp; adapters (ports)","text":"<p>VectorStore (FAISS GPU + cuVS)</p> <pre><code>class VectorStore(Protocol):\n    def train(self, train_vectors: \"np.ndarray\", **params) -&gt; None: ...\n    def add(self, keys: list[str], vectors: \"np.ndarray\") -&gt; None: ...\n    def search(self, query: \"np.ndarray\", k: int) -&gt; list[tuple[str, float]]: ...\n    def save(self, index_uri: str, idmap_uri: str) -&gt; None: ...\n    def load(self, index_uri: str, idmap_uri: str) -&gt; None: ...\n</code></pre> <p>SparseIndex</p> <pre><code>class SparseIndex(Protocol):\n    def build(self, docs_iterable: \"Iterable[SparseDoc]\") -&gt; None: ...\n    def search(self, query: str, k: int, fields: dict|None=None) -&gt; list[tuple[str, float]]: ...\n    def stats(self) -&gt; dict: ...\n</code></pre> <p>Implementations: BM25Index (Pyserini) and SpladeImpactIndex (Pyserini).</p> <p>GraphStore</p> <pre><code>class GraphStore(Protocol):\n    def upsert_nodes(self, docs: list[Doc], concepts: list[Concept], chunks: list[Chunk]) -&gt; None: ...\n    def upsert_mentions(self, assertions: list[LinkAssertion]) -&gt; None: ...\n    def neighbors(self, concept_id: str, depth:int=1) -&gt; list[str]: ...\n    def linked_concepts(self, chunk_id: str) -&gt; list[str]: ...\n</code></pre> <p>Registry (DuckDB \u22651.4.1)</p> <ul> <li>Provides DDL/migrations, safe registration (two\u2011phase commit: write Parquet \u2192 atomically register).</li> <li>Exposes views across Parquet datasets (union_by_name).</li> <li>Threading: default PRAGMA threads=14.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#5-orchestration-prefect-2x-all-local","title":"5) Orchestration (Prefect 2.x; all local)","text":"<p>Flows (idempotent, content\u2011hash keyed):</p> <ol> <li><code>harvest_and_download(topic, years, max_works)</code></li> <li><code>convert_to_doctags(doc_ids[])</code></li> <li><code>chunk_with_docling(doc_ids[])</code></li> <li><code>embed_dense_qwen3(chunk_dataset_id)</code></li> <li><code>encode_splade_v3(chunk_dataset_id)</code></li> <li><code>build_bm25_index(chunk_dataset_id)</code></li> <li><code>build_faiss_index(dense_run_id)</code></li> <li><code>ingest_ontologies(ontology_specs[])</code></li> <li><code>embed_concepts(ontology_id)</code></li> <li><code>link_chunks_to_concepts(chunk_dataset_id, ontology_id)</code></li> <li><code>upsert_kg(link_run_id)</code></li> <li><code>serve_search_api()</code> (starts uvicorn if not already)</li> </ol> <p>Events recorded in <code>registry.pipeline_events</code>: <code>DocumentIngested</code>, <code>DoctagsReady</code>, <code>ChunksCreated</code>, <code>DenseEmbedded</code>, <code>SpladeEncoded</code>, <code>BM25Built</code>, <code>FAISSBuilt</code>, <code>OntologyLoaded</code>, <code>ConceptEmbeddingsReady</code>, <code>LinkerRun</code>, <code>KGUpdated</code>.</p> <p>Concurrency defaults:</p> <ul> <li>Downloader 8 parallel fetches;</li> <li>VLM 2 docs in flight;</li> <li>Dense/SPLADE batchers tuned to ~80% VRAM;</li> <li>FAISS build: 2 shards concurrently;</li> <li>All others CPU\u2011bound threads = 14.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#6-harvesters-pdf-download-pyalex-first-resilient-fallbacks","title":"6) Harvesters &amp; PDF download (PyAlex first; resilient fallbacks)","text":"<p>Workflow</p> <ul> <li>Search: PyAlex (OpenAlex) by <code>topic</code> across title/abstract/fulltext; filter OA flags; optionally by year range.</li> <li>Extract candidate OA locations: <code>best_oa_location</code>, <code>primary_location</code>, <code>locations[]</code>, DOI, arXiv id, pmcid.</li> <li> <p>Download resolution (in order):</p> </li> <li> <p>OpenAlex <code>best_oa_location.pdf_url</code>.</p> </li> <li>Any <code>locations[].pdf_url</code> with <code>is_oa=true</code> favoring <code>publishedVersion</code> or <code>acceptedVersion</code>.</li> <li>Unpaywall by DOI (<code>url_for_pdf</code>) \u2014 requires configured contact email and rate limits.</li> <li> <p>Source\u2011specific:</p> <ul> <li>arXiv \u2192 <code>https://arxiv.org/pdf/&lt;id&gt;.pdf</code></li> <li>PubMed Central \u2192 <code>https://www.ncbi.nlm.nih.gov/pmc/articles/&lt;pmcid&gt;/pdf</code></li> <li>License guard: accept only OA\u2011compatible licenses; persist license string on <code>Doc</code>.</li> <li>Download: 8 concurrency; 60s timeout; 3 retries w/ exp backoff; <code>User-Agent</code> includes contact/email.</li> <li>Storage: <code>/data/pdfs/&lt;doc_id&gt;.pdf</code> where <code>doc_id</code> derived later from canonical text (post\u2011DocTags). Temporarily name by OpenAlex ID then rename after canonicalization.</li> <li>Registration: each successful PDF produces a <code>documents</code> row (openalex id, doi, license, pdf_uri, source).</li> </ul> </li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#7-pdf-doctags-docling-vlm-granitedocling","title":"7) PDF \u2192 DocTags (Docling VLM Granite\u2011Docling)","text":"<ul> <li>Serving: vLLM pre\u2011release (CUDA 13) process #1 on localhost:8001.</li> <li>Router: Nginx exposes <code>/vlm/* \u2192 8001</code>.</li> <li>Defaults: DPI=220, page_batch=8, bf16 if supported, fallback fp16.</li> <li>Quality gate: if avg token logprob &lt; 0.5 on a page \u2192 fallback OCR for that page; provenance notes retained.</li> <li>Timeouts: 120s per 100 pages; max 2000 pages.</li> <li>Outputs: <code>/data/doctags/&lt;doc_id&gt;.dt.json.zst</code>.</li> <li>Registration: DuckDB <code>doctags</code> with pages, model, revision (<code>untied</code> if applicable), avg_logprob.</li> </ul> <p>We intentionally present one logical model endpoint via Nginx, but use two local vLLM processes (one model each) due to vLLM\u2019s one\u2011model\u2011per\u2011process design.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#8-chunking-docling-hybridchunker","title":"8) Chunking (Docling HybridChunker)","text":"<ul> <li>Parameters: <code>target_tokens=400</code>, <code>overlap=80</code>, <code>min_tokens=120</code>, <code>max_tokens=480</code>.</li> <li>Structure\u2011aware segmentation first; spillover via sliding windows.</li> <li>Captures <code>doctags_span</code> and <code>start_char/end_char</code> for explainable highlights.</li> <li>Parquet dataset: <code>parquet/chunks/model=docling_hybrid/run=&lt;run_id&gt;/part-*.parquet</code> Schema:</li> </ul> <p><code>chunk_id: string   doc_id: string   section: string   start_char: int32   end_char: int32   doctags_span: struct&lt;node_id:string,start:int32,end:int32&gt;   text: string   tokens: int32   created_at: timestamp</code> * Register resulting dataset in <code>registry.datasets</code> (kind='chunks') and each row in <code>chunks</code>.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#9-dense-embeddings-qwen3embedding4b-2560dim-vllm","title":"9) Dense embeddings (Qwen3\u2011Embedding\u20114B @ 2560\u2011dim, vLLM)","text":"<ul> <li>Serving: vLLM pre\u2011release process #2 on localhost:8002; router maps <code>/v1/embeddings \u2192 8002</code>.</li> <li>Embedding length: 2560 (model supports 32\u20132560; set explicitly).</li> <li>Batching: automatic; target \u2264 80% of available VRAM.</li> <li>Normalization: L2; store norm (float32).</li> <li>Parquet dataset: <code>parquet/dense/model=Qwen3-Embedding-4B/run=&lt;run_id&gt;/part-*.parquet</code> Schema:</li> </ul> <p><code>chunk_id: string   model: string              -- 'Qwen3-Embedding-4B'   run_id: string   dim: int16                 -- 2560   vector: list&lt;float&gt;        -- length==2560 (float32 on disk)   l2_norm: float   created_at: timestamp</code> * Compression: ZSTD=6; row_group=4096; dictionary encoding on categorical cols.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#10-sparse-embeddings-indices-spladev3-gpu-bm25","title":"10) Sparse embeddings &amp; indices (SPLADE\u2011v3 GPU + BM25)","text":"<ul> <li> <p>SPLADE\u2011v3 encoder: <code>naver/splade-v3-distilbert</code></p> </li> <li> <p>CUDA (Torch 2.9, cu130), AMP fp16, <code>max_seq_len=512</code>, <code>topK=256</code> nnz per chunk.</p> </li> <li>Tokenizer: distilbert WordPiece, <code>vocab_size=30522</code>.</li> <li> <p>Parquet dataset: <code>parquet/sparse/model=SPLADE-v3-distilbert/run=&lt;run_id&gt;/part-*.parquet</code> Schema:</p> <p><code>chunk_id: string model: string run_id: string vocab_ids: list&lt;int32&gt;    -- sorted, unique weights:   list&lt;float&gt;    -- same length as vocab_ids nnz: int16 created_at: timestamp</code> * Indexing: Lucene impact index (Pyserini).</p> </li> <li> <p>No JSONL at rest: a streaming adapter reads Parquet rows and writes directly to Lucene via Pyserini builders (any temporary files are ephemeral and excluded from registry).</p> </li> <li>BM25: Pyserini (Lucene). Params: k1=0.9, b=0.4; field boosts <code>title^2.0, section^1.2, body^1.0</code>.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#11-faiss-gpu-with-cuvs-cuda-13","title":"11) FAISS (GPU with cuVS), CUDA 13","text":"<ul> <li>Install: Prefer GPU binary (wheel) built for CUDA 13 with cuVS enabled. If unavailable, build from source with <code>-DFAISS_ENABLE_GPU=ON -DFAISS_ENABLE_CUVS=ON</code>.</li> <li>Index factory (default): <code>OPQ64,IVF8192,PQ64</code> (good for d=2560; OPQ pre\u2011rotation at 64; PQ m=64).</li> <li>Training: sample 10M vectors or all (if fewer), seed=42.</li> <li>Search: <code>nprobe=64</code>. Codes 8\u2011bit per subvector.</li> <li>Memory: PQ codes ~64\u202fB/vector (+ID/overhead \u21d2 ~80\u2013100\u202fB/vector typical).</li> <li>Persistence: <code>.faiss</code> index + <code>.ids</code> idmap; shards \u226410M vectors each.</li> <li>Registration: rows in DuckDB <code>faiss_indexes</code> (logical index id groups shards).</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#12-ontology-ingestion-concept-encoders","title":"12) Ontology ingestion &amp; concept encoders","text":"<ul> <li>Formats: OWL/RDF (TTL/RDF\u2011XML), OBO, SKOS.</li> <li>Normalization: lowercase, NFC, strip most punctuation, lemmatize EN, stopword\u2011smart; build surface forms from <code>pref_label</code>, <code>alt_labels</code>, <code>synonyms</code>, and <code>definition</code>.</li> <li>Dense concept embeddings: Qwen3\u2011Embedding\u20114B with dim=2560 (concat text: <code>pref_label | definition | 5 best synonyms</code>).</li> <li>Sparse concept embeddings: SPLADE\u2011v3 with topK=128.</li> <li>Parquet datasets mirroring chunk schemas; registered in <code>concept_embeddings</code>.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#13-linker-chunkconcept-calibrated","title":"13) Linker (chunk\u2192concept), calibrated","text":"<p>Candidate generation</p> <ul> <li>SPLADE concept index @100 using chunk text;</li> <li>Lexical dictionary match @50 (max phrase len=6 tokens, case\u2011folded).</li> </ul> <p>Feature scoring</p> <ul> <li><code>dense_sim</code> = cosine(qwen_chunk, qwen_concept)</li> <li><code>sparse_sim</code> = normalized SPLADE (min\u2013max per query)</li> <li><code>lexical_overlap</code> = matched_chars / chunk_chars (clamp [0, 0.2])</li> <li><code>depth_bonus</code> = +0.02 \u00d7 levels from root (cap +0.10)</li> </ul> <p>Fusion &amp; decision</p> <ul> <li><code>score = 0.55*dense + 0.35*sparse + 0.10*lexical + depth_bonus</code></li> <li>Thresholds: link \u2265 0.62, reject \u2264 0.35, else uncertain.</li> <li>Calibration: isotonic regression stored per linker run; output <code>LinkAssertion</code> Parquet.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#14-knowledge-graph-neo4j-local","title":"14) Knowledge Graph (Neo4j, local)","text":"<p>Nodes</p> <ul> <li><code>(:Doc {doc_id, title, year, license, source, source_id})</code></li> <li><code>(:Chunk {chunk_id, section, start_char, end_char})</code></li> <li><code>(:Concept {concept_id, ontology, pref_label})</code></li> </ul> <p>Relationships</p> <ul> <li><code>(:Doc)-[:HAS_CHUNK]-&gt;(:Chunk)</code></li> <li><code>(:Chunk)-[:MENTIONS {score, run_id, evidence_span, created_at}]-&gt;(:Concept)</code></li> <li><code>(:Concept)-[:IS_A]-&gt;(:Concept)</code> and <code>[:RELATED_TO]</code> (from ontology).</li> </ul> <p>Constraints &amp; indexes</p> <ul> <li>Uniqueness on <code>doc_id</code>, <code>chunk_id</code>, <code>concept_id</code>; B\u2011tree index on <code>MENTIONS.score</code> and <code>MENTIONS.run_id</code>.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#15-hybrid-search-api-fastapi-local","title":"15) Hybrid search API (FastAPI, local)","text":"<p>Endpoints</p> <ul> <li><code>POST /search</code>   Body: <code>{query: str, k?: int=10, filters?: {...}, explain?: bool=false}</code>   Returns top chunks &amp; doc rollups with dense/sparse scores, KG boosts, spans, linked concepts.</li> <li><code>POST /graph/concepts</code> (browse ontology)</li> <li><code>GET /healthz</code></li> </ul> <p>Algorithm (fixed)</p> <ol> <li>Query embedding via Qwen3 (2560\u2011d) + parse lexical concept mentions.</li> <li>Retrieve dense@200 (FAISS) + sparse@200 (BM25 and SPLADE).</li> <li>Fuse via RRF(k=60).</li> <li>KG boosts: +0.08 for direct concept match; +0.04 for one\u2011hop neighbor.</li> <li>MMR (\u03bb=0.7) at doc level.</li> <li>Return top\u2011k.</li> </ol>"},{"location":"explanations/251025_HighLevelArchitecture/#16-configuration-yaml-singlebox-defaults","title":"16) Configuration (YAML; single\u2011box defaults)","text":"<pre><code>system:\n  os: ubuntu-24.04\n  threads: 14\n  seed: 42\n  parquet_root: /data/parquet\n  artifacts_root: /data/artifacts\n  duckdb_path: /data/catalog/catalog.duckdb\n\nruntime:\n  python: \"3.13\"\n  cuda: \"13.0\"\n  torch: \"2.9\"\n  duckdb_min: \"1.4.1\"\n  vllm_channel: \"pre-release-cuda13\"\n\nnetwork:\n  nginx_port: 80\n  vlm_port: 8001\n  emb_port: 8002\n  api_port: 8080\n\nharvest:\n  provider: pyalex\n  per_page: 200\n  max_works: 20000\n  years: \"&gt;=2018\"\n  filters:\n    is_oa: true\n    has_oa_published_version: true\n  fallbacks:\n    unpaywall: true\n    arxiv: true\n    pmc: true\n  concurrency: 8\n  timeout_sec: 60\n  retries: 3\n\ndoc_conversion:\n  vlm_model: ibm-granite/granite-docling-258M\n  vlm_revision: untied\n  endpoint: http://localhost/vlm/\n  dpi: 220\n  page_batch: 8\n  ocr_fallback: true\n  max_pages: 2000\n  timeout_sec: 120\n\nchunking:\n  engine: docling_hybrid\n  target_tokens: 400\n  overlap_tokens: 80\n  min_tokens: 120\n  max_tokens: 480\n\ndense_embedding:\n  model: Qwen/Qwen3-Embedding-4B\n  endpoint: http://localhost/v1/embeddings\n  output_dim: 2560\n  parquet_out: ${system.parquet_root}/dense/model=Qwen3-Embedding-4B/run=${run_id}\n\nsparse_embedding:\n  splade:\n    model: naver/splade-v3-distilbert\n    device: cuda\n    amp: fp16\n    max_seq_len: 512\n    topk: 256\n    parquet_out: ${system.parquet_root}/sparse/model=SPLADE-v3-distilbert/run=${run_id}\n  bm25:\n    k1: 0.9\n    b: 0.4\n    field_boosts: { title: 2.0, section: 1.2, body: 1.0 }\n    index_dir: /data/lucene/bm25\n\nfaiss:\n  index_factory: OPQ64,IVF8192,PQ64\n  nprobe: 64\n  train_samples: 10000000\n  shards:\n    max_vectors_per_shard: 10000000\n  gpu: true\n  cuvs: true\n  output_dir: /data/faiss/qwen3_ivfpq\n\nontology:\n  inputs:\n    - { ontology_id: mesh, format: obo, uri: /data/ontologies/mesh.obo }\n    - { ontology_id: go,   format: obo, uri: /data/ontologies/go.obo }\n  concept_embed:\n    dense_model: Qwen3-Embedding-4B\n    dense_dim: 2560\n    splade_model: SPLADE-v3-distilbert\n    splade_topk: 128\n\nlinker:\n  candidates: { splade_topk: 100, lexicon_topk: 50 }\n  fusion_weights: { dense: 0.55, sparse: 0.35, lexical: 0.10, depth_bonus_per_level: 0.02, depth_cap: 0.10 }\n  thresholds: { high: 0.62, low: 0.35 }\n  calibration: isotonic\n\ngraph:\n  backend: neo4j\n  uri: bolt://localhost:7687\n  user: neo4j\n  password_env: NEO4J_PASSWORD\n\nsearch:\n  k: 10\n  dense_candidates: 200\n  sparse_candidates: 200\n  rrf_k: 60\n  mmr_lambda: 0.7\n  kg_boosts: { direct: 0.08, one_hop: 0.04 }\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#17-duckdb-141-schema-views-full-fidelity","title":"17) DuckDB (\u2265\u202f1.4.1) schema &amp; views (full fidelity)","text":"<p>Tables (key ones)</p> <pre><code>CREATE TABLE model_registry (\n  model_id TEXT,\n  repo TEXT,\n  revision TEXT,\n  tokenizer TEXT,\n  embedding_dim INT,      -- 2560 for Qwen3 embeddings\n  vocab_size INT,         -- 30522 for SPLADE v3 tokenizer\n  framework TEXT,         -- 'vllm'|'hf'\n  framework_version TEXT,\n  build_info JSON,        -- FAISS flags, CUDA, cuVS info\n  PRIMARY KEY (model_id, revision)\n);\n\nCREATE TABLE runs (\n  run_id TEXT PRIMARY KEY,\n  purpose TEXT,           -- 'dense_embed'|'splade_encode'|'bm25_build'|'faiss_build'|...\n  model_id TEXT,\n  revision TEXT,\n  started_at TIMESTAMP,\n  finished_at TIMESTAMP,\n  config JSON\n);\n\nCREATE TABLE documents (\n  doc_id TEXT PRIMARY KEY,\n  openalex_id TEXT, doi TEXT, arxiv_id TEXT, pmcid TEXT,\n  title TEXT, authors JSON, pub_date TIMESTAMP,\n  license TEXT, language TEXT,\n  pdf_uri TEXT, source TEXT,\n  content_hash TEXT,         -- canonical text hash (after DocTags\u2192text)\n  created_at TIMESTAMP\n);\n\nCREATE TABLE doctags (\n  doc_id TEXT PRIMARY KEY REFERENCES documents(doc_id),\n  doctags_uri TEXT, pages INT,\n  vlm_model TEXT, vlm_revision TEXT,\n  avg_logprob DOUBLE,\n  created_at TIMESTAMP\n);\n\nCREATE TABLE datasets (\n  dataset_id TEXT PRIMARY KEY,\n  kind TEXT,                 -- 'chunks'|'dense'|'sparse'|'concepts'\n  parquet_root TEXT,\n  run_id TEXT REFERENCES runs(run_id),\n  created_at TIMESTAMP\n);\n\nCREATE TABLE chunks (\n  chunk_id TEXT PRIMARY KEY,\n  doc_id TEXT REFERENCES documents(doc_id),\n  section TEXT, start_char INT, end_char INT,\n  doctags_span JSON, tokens INT,\n  dataset_id TEXT REFERENCES datasets(dataset_id),\n  created_at TIMESTAMP\n);\n\nCREATE TABLE dense_runs (\n  run_id TEXT PRIMARY KEY REFERENCES runs(run_id),\n  model TEXT, dim INT,       -- 2560\n  parquet_root TEXT,\n  created_at TIMESTAMP\n);\n\nCREATE TABLE sparse_runs (\n  run_id TEXT PRIMARY KEY REFERENCES runs(run_id),\n  model TEXT, vocab_size INT,\n  parquet_root TEXT,\n  created_at TIMESTAMP,\n  backend TEXT               -- 'lucene-impact'|'lucene-bm25'\n);\n\nCREATE TABLE faiss_indexes (\n  logical_index_id TEXT,     -- groups shards into a single logical index\n  run_id TEXT REFERENCES dense_runs(run_id),\n  shard_id INT,\n  index_type TEXT, nlist INT, m INT, opq INT, nprobe INT,\n  gpu BOOLEAN, cuvs BOOLEAN,\n  index_uri TEXT, idmap_uri TEXT,\n  created_at TIMESTAMP,\n  PRIMARY KEY (logical_index_id, shard_id)\n);\n\nCREATE TABLE ontologies (\n  ontology_id TEXT PRIMARY KEY,\n  format TEXT, src_uri TEXT,\n  loaded_at TIMESTAMP, concept_count INT\n);\n\nCREATE TABLE concept_embeddings (\n  ontology_id TEXT REFERENCES ontologies(ontology_id),\n  model TEXT, dim INT,\n  parquet_root TEXT,\n  created_at TIMESTAMP,\n  PRIMARY KEY (ontology_id, model)\n);\n\nCREATE TABLE link_assertions (\n  id TEXT PRIMARY KEY,\n  chunk_id TEXT REFERENCES chunks(chunk_id),\n  concept_id TEXT,\n  score DOUBLE, decision TEXT,\n  evidence_span TEXT,\n  features JSON,\n  run_id TEXT REFERENCES runs(run_id),\n  created_at TIMESTAMP\n);\n\nCREATE TABLE pipeline_events (\n  event_id TEXT PRIMARY KEY,\n  event_name TEXT, subject_id TEXT, payload JSON,\n  created_at TIMESTAMP\n);\n\n-- Helpful indexes\nCREATE INDEX idx_chunks_doc ON chunks(doc_id);\nCREATE INDEX idx_link_chunk ON link_assertions(chunk_id);\nCREATE INDEX idx_link_concept ON link_assertions(concept_id);\n</code></pre> <p>Views (Parquet unification)</p> <pre><code>PRAGMA threads=14;\n\nCREATE OR REPLACE VIEW dense_vectors_view AS\nSELECT * FROM read_parquet(\n  (SELECT parquet_root FROM dense_runs), union_by_name=true);\n\nCREATE OR REPLACE VIEW splade_vectors_view AS\nSELECT * FROM read_parquet(\n  (SELECT parquet_root FROM sparse_runs WHERE backend='lucene-impact'), union_by_name=true);\n\nCREATE OR REPLACE VIEW chunk_texts AS\nSELECT * FROM read_parquet(\n  (SELECT parquet_root FROM datasets WHERE kind='chunks'), union_by_name=true);\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#18-vllm-nginx-single-logical-endpoint","title":"18) vLLM &amp; Nginx (single logical endpoint)","text":"<p>Nginx (local):</p> <pre><code>server {\n  listen 80;\n  server_name localhost;\n\n  location /vlm/ {              # Docling VLM\n    proxy_pass http://127.0.0.1:8001/;\n  }\n  location /v1/embeddings {     # Qwen3 embeddings (OpenAI-compatible)\n    proxy_pass http://127.0.0.1:8002/v1/embeddings;\n  }\n  location /healthz {\n    return 200 'ok\\n';\n  }\n}\n</code></pre> <p>vLLM processes (systemd units suggested)</p> <ul> <li>Granite\u2011Docling: <code>vllm serve ibm-granite/granite-docling-258M --revision untied --host 127.0.0.1 --port 8001 --dtype bfloat16</code></li> <li>Qwen3\u2011Embedding: <code>vllm serve Qwen/Qwen3-Embedding-4B --host 127.0.0.1 --port 8002 --dtype float16 --max-num-seqs 1024 --trust-remote-code</code></li> </ul> <p>(One model per vLLM process; router gives you a single host/endpoint.)</p>"},{"location":"explanations/251025_HighLevelArchitecture/#19-implementation-skeletons-selected","title":"19) Implementation skeletons (selected)","text":""},{"location":"explanations/251025_HighLevelArchitecture/#191-downloader-pyalex-fallbacks","title":"19.1 Downloader (PyAlex + fallbacks)","text":"<pre><code># download/harvester.py\nclass OpenAccessHarvester:\n    def __init__(self, cfg, http, unpaywall_client):\n        ...\n\n    def search_openalex(self, topic: str, years: str, max_works: int) -&gt; list[dict]:\n        # paginate PyAlex queries; collect candidate OA PDF URLs + metadata\n        ...\n\n    def resolve_pdf(self, work: dict) -&gt; str | None:\n        # try best_oa_location.pdf_url \u2192 locations[].pdf_url \u2192 unpaywall \u2192 arxiv/pmc\n        ...\n\n    def download_pdf(self, url: str, dest_path: str) -&gt; bool:\n        # 60s timeout, 3 retries, 8 concurrent workers\n        ...\n\n    def run(self, topic: str, years: str, max_works: int) -&gt; list[Doc]:\n        # returns registered documents (DuckDB insert)\n        ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#192-doctags-conversion","title":"19.2 DocTags conversion","text":"<pre><code># docling/vlm.py\nclass GraniteDoclingVLM:\n    def __init__(self, endpoint: str, dpi: int, page_batch: int, timeout: int, ocr_fallback: bool):\n        ...\n\n    def to_doctags(self, pdf_path: str) -&gt; dict:\n        # call vLLM endpoint; assemble DocTags JSON; OCR fallback per page if needed\n        ...\n\n    def persist(self, doc_id: str, doctags: dict) -&gt; str:\n        # write to /data/doctags/&lt;doc_id&gt;.dt.json.zst, return URI\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#193-chunking","title":"19.3 Chunking","text":"<pre><code># docling/hybrid.py\nclass HybridChunker:\n    def __init__(self, target: int=400, overlap: int=80, min_tokens: int=120, max_tokens: int=480):\n        ...\n    def chunk(self, doctags_uri: str) -&gt; list[Chunk]:\n        # parse, section-aware splitting, sliding windows, record spans/offsets\n        ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#194-dense-embeddings-qwen3-2560d-via-vllm","title":"19.4 Dense embeddings (Qwen3 2560\u2011d via vLLM)","text":"<pre><code># embeddings_dense/qwen3.py\nclass Qwen3Embedder:\n    name = \"Qwen3-Embedding-4B\"\n    dim = 2560\n    def __init__(self, endpoint: str):\n        ...\n    def embed_texts(self, texts: list[str]) -&gt; \"np.ndarray\":\n        # call /v1/embeddings; enforce dim=2560; L2-normalize\n        ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#195-spladev3-encoder-gpu","title":"19.5 SPLADE\u2011v3 encoder (GPU)","text":"<pre><code># embeddings_sparse/splade.py\nclass SPLADEv3Encoder:\n    def __init__(self, model_id: str, device=\"cuda\", topk=256, max_seq_len=512):\n        ...\n    def encode(self, texts: list[str]) -&gt; list[tuple[list[int], list[float]]]:\n        # torch.no_grad + autocast; prune to topK\n        ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#196-faiss-index-gpu-cuvs","title":"19.6 FAISS index (GPU + cuVS)","text":"<pre><code># vectorstore_faiss/gpu.py\nclass FaissGpuIndex:\n    def __init__(self, factory: str, nprobe: int, gpu: bool=True, cuvs: bool=True):\n        ...\n    def train(self, train_vectors: \"np.ndarray\") -&gt; None:\n        ...\n    def add(self, keys: list[str], vectors: \"np.ndarray\") -&gt; None:\n        ...\n    def search(self, query: \"np.ndarray\", k: int) -&gt; list[tuple[str, float]]:\n        ...\n    def save(self, index_uri: str, idmap_uri: str) -&gt; None:\n        ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#197-hybrid-retrieval-rerank-search_api","title":"19.7 Hybrid retrieval &amp; rerank (search_api)","text":"<pre><code># search_api/service.py\ndef hybrid_search(query: str, k: int) -&gt; list[dict]:\n    q_vec = dense_embedder.embed_texts([query])[0]\n    dense_hits = faiss.search(q_vec, k=200)\n    sparse_hits = bm25.search(query, k=200) + splade.search(query, k=200)\n    fused = rrf_fuse(dense_hits, sparse_hits, k=60)\n    boosted = apply_kg_boosts(fused, query)\n    results = mmr_deduplicate(boosted, lambda_=0.7)\n    return explain(results, k)\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#20-quality-gates-evaluation-ci","title":"20) Quality gates, evaluation &amp; CI","text":"<ul> <li>Retrieval: nDCG@10 \u2265 0.50 (baseline), Recall@1K \u2265 0.85.</li> <li>Linker: F1 \u2265 0.70 on a labeled set; ECE \u2264 0.08 after isotonic calibration.</li> <li>Latency: p95 search &lt; 300\u202fms (top\u201110); p50 chunk\u2192embed throughput tracked.</li> <li>CI: Unit tests (mypy, ruff), golden tests, nightly 1k\u2011doc mini\u2011pipeline; fail on regressions.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#21-security-licensing-governance-local","title":"21) Security, licensing, governance (local)","text":"<ul> <li>License guard at harvest; store license string in <code>documents.license</code>.</li> <li>vLLM binds to localhost only; Nginx is the only front door; API\u2011key auth on search API.</li> <li>Secrets (if any) from environment; never persisted.</li> <li>Provenance: every artifact row includes <code>{run_id, model_id, revision, created_at}</code>.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#22-capacity-sizing-with-2560dim","title":"22) Capacity &amp; sizing with 2560\u2011dim","text":"<ul> <li>Dense Parquet: \u2248 10\u202fKB/chunk pre\u2011compression (2560 \u00d7 4\u202fB).</li> <li>IVF\u2011PQ (PQ64) codes: \u2248 64\u202fB/vector (+ID/overhead ~80\u2013100\u202fB).</li> <li>SPLADE postings: ~2\u202fKB/chunk before Lucene compression (256 nnz).</li> <li>Scale by chunk count (e.g., 5M chunks \u2192 ~50\u202fGB dense Parquet uncompressed; index is small enough for GPU).</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#23-operational-runbooks","title":"23) Operational runbooks","text":"<ul> <li>CUDA 13 + Torch 2.9: install CUDA 13, confirm <code>nvcc --version</code>, install cu130 wheels.</li> <li>vLLM pre\u2011release: install channel with CUDA\u201113 support; start two processes; verify health on <code>:8001</code>, <code>:8002</code>; Nginx routes.</li> <li>FAISS GPU + cuVS: install GPU binary (preferred); if needed, build from source with CMake flags enabling cuVS.</li> <li> <p>Directories:</p> </li> <li> <p>PDFs: <code>/data/pdfs</code></p> </li> <li>DocTags: <code>/data/doctags</code></li> <li>Parquet: <code>/data/parquet</code></li> <li>Lucene: <code>/data/lucene/{bm25,splade}</code></li> <li>FAISS: <code>/data/faiss</code></li> <li>DuckDB: <code>/data/catalog/catalog.duckdb</code></li> <li>Resource limits: <code>ulimit -n 65535</code>; I/O scheduler <code>mq-deadline</code>; set <code>PRAGMA threads=14</code> in DuckDB.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#24-workstreams-implementation-plans-deliverables","title":"24) Workstreams (implementation plans &amp; deliverables)","text":"<ol> <li> <p>Downloader &amp; Harvester (PyAlex + fallbacks)</p> </li> <li> <p>PyAlex client; OA filters; DOI/ID extract; Unpaywall integration; robust downloader; license guard; DuckDB registration.</p> </li> <li> <p>Deliverables: module, config, tests (mock HTTP), metrics (<code>pdf_download_success_total</code>, <code>oa_resolution_latency</code>).</p> </li> <li> <p>Docling VLM &amp; DocTags</p> </li> <li> <p>vLLM integration; page batching; OCR fallback; DocTags writer; quality metrics (avg logprob/page).</p> </li> <li> <p>Deliverables: module, Nginx+vLLM configs, DDL updates, golden DocTags samples.</p> </li> <li> <p>Chunking (Docling Hybrid)</p> </li> <li> <p>Hybrid configuration; token accounting; offsets/spans; Parquet writer; golden chunk fixtures.</p> </li> <li> <p>Dense embeddings (Qwen3 2560\u2011d)</p> </li> <li> <p>vLLM client; batching &amp; normalization; Parquet writer; memory instrumentation; retries.</p> </li> <li> <p>SPLADE\u2011v3 GPU encoder + Pyserini indices</p> </li> <li> <p>Torch 2.9 encoder; Parquet encoder; streaming into Lucene (impact); BM25 build; query APIs.</p> </li> <li> <p>FAISS GPU/cuVS</p> </li> <li> <p>Binary installation or source build; index builder (OPQ64,IVF8192,PQ64); training sampler; shard manager; search adapter.</p> </li> <li> <p>Ontology &amp; concept embeddings</p> </li> <li> <p>Loaders (OWL/OBO/SKOS); normalization; concept Parquet; Qwen3(2560) + SPLADE encoders.</p> </li> <li> <p>Linker</p> </li> <li> <p>Candidate gen; fusion; calibration (isotonic); assertions Parquet; explainability; ablations.</p> </li> <li> <p>KG Builder (Neo4j)</p> </li> <li> <p>Node/edge upserts; constraints; indexes; maintenance; graph query helpers.</p> </li> <li> <p>Search API</p> <ul> <li>FastAPI; hybrid retrieval; RRF; KG boosts; MMR; explanations; OpenAPI schema; auth.</li> </ul> </li> <li> <p>Registry (DuckDB)</p> <ul> <li>Migrations; registration helpers; views across Parquet; integrity checks; provenance guards.</li> </ul> </li> <li> <p>Orchestration &amp; Observability</p> <ul> <li>Prefect flows; retries; dashboards; tracing; alerting; local startup scripts.</li> </ul> </li> </ol>"},{"location":"explanations/251025_HighLevelArchitecture/#25-coding-standards-bestinclass-python-practices","title":"25) Coding standards &amp; best\u2011in\u2011class Python practices","text":"<ul> <li>Static typing everywhere (<code>mypy --strict</code>).</li> <li>Pydantic v2 for all externalized contracts; validate at boundaries only.</li> <li>Dataclasses/NamedTuples for internal\u2011only small records on hot paths.</li> <li>Pure functions for transformations; side effects behind adapters.</li> <li>Dependency inversion: constructors accept interfaces, not concretes.</li> <li>Factories resolve plugins from entry\u2011points; no <code>if/elif</code> on provider names in business logic.</li> <li>Error handling: wrap external calls in <code>Result[T, E]</code>\u2011like helpers (or <code>try/except</code> mapping to domain errors).</li> <li>Logging: structured (<code>json</code>); include <code>run_id</code>, <code>doc_id</code>, <code>chunk_id</code>.</li> <li>Testing: unit + contract tests per port; \u201cfake\u201d in\u2011memory providers; golden tests for DocTags/chunks; end\u2011to\u2011end smoke.</li> <li>Performance: vector ops on GPU; streaming I/O; Parquet row groups sized for cache locality; avoid pandas on hot paths.</li> <li>Docs: every public class/method has docstrings; README per package; ADRs for significant choices.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#addendum-additional-architecture-information","title":"Addendum - additional architecture information","text":""},{"location":"explanations/251025_HighLevelArchitecture/#part-a-gap-analysis-whats-unspecified-resolutions-in-addendum","title":"PART A \u2014 GAP ANALYSIS (WHAT\u2019S UNSPECIFIED) \u2192 RESOLUTIONS IN ADDENDUM","text":"Area Gap / ambiguity Why it matters Final specification (summary) Canonical text &amp; ID hashing Exact canonicalization pipeline from DocTags to text (line breaks, whitespace, Unicode norms) not fixed; ID hashing step unclear. Reproducibility; stable <code>doc_id</code>, <code>chunk_id</code>. Define canonicalizer (NFC \u2192 collapse whitespace \u2192 normalize bullets/ligatures \u2192 Unix line endings). Hash canonical text SHA\u2011256, use first 16 bytes (base32) for URNs. (\u00a7B1) Tokenizer for chunk sizes Which tokenizer governs <code>target_tokens</code> not pinned. Chunk consistency, embedding costs. Use Qwen3\u2011Embedding tokenizer via HF <code>AutoTokenizer</code> for token counts. (\u00a7B2) DocTags fidelity Which DocTags nodes to include/exclude (headers/footers/refs), and OCR fallback merge rules unclear. Content quality; linkability back to pages. Include body, headings, tables, captions, figure text; exclude references by default. OCR per\u2011page fallback with provenance flag; page order preserved. (\u00a7B3) Downloader resolution order &amp; de\u2011dupe Ties between multiple OA locations; DOI/arXiv/PMCID overlaps; file dedup not specified. Avoid duplicate processing and wasted space. Stable priority list (OpenAlex best_oa \u2192 other locations \u2192 Unpaywall \u2192 source\u2011specific), MIME check; compute <code>pdf_sha256</code>; deduplicate by hash; symlink duplicate DoIs. (\u00a7B4) PyAlex topic query spec Which OpenAlex fields are searched; year filters and pagination; retries/timeouts not fixed. Recall and reliability. Search <code>title, abstract_inverted_index, fulltext</code> with AND semantics; filter by <code>from_year..to_year</code>; <code>per_page=200</code>; backoff retries 3\u00d7; http timeout 30\u202fs; polite <code>User-Agent</code> and mailto. (\u00a7B4) Qwen3 embeddings 2560\u2011d Request parameter to enforce 2560 output; failure behavior if unsupported. Index shape; FAISS train. Request <code>dimension=2560</code>; assert response dim; if unsupported (model mismatch), fail fast and record incident. (\u00a7B5) SPLADE\u2011v3 GPU encoder Pre\u2011processing (lowercasing, punctuation), truncation, batching, AMP, and top\u2011K pruning details not pinned. Index size and quality; repeatability. Lowercase; strip control chars; keep punctuation; truncation at 512 WP tokens; AMP fp16; batch size auto\u2011tuned; top\u2011K=256 with stable tiebreaker (token id). (\u00a7B6) BM25 analyzer Analyzer pipeline (tokenizer, stopwords, stemming, casefold) not fixed. Matching behavior and score comparability. Lucene StandardTokenizer; English minimal stemming; lowercase; English stoplist; synonyms off by default (can be added per domain). (\u00a7B6) Parquet schemas &amp; partitioning Columns, dtypes, compression, row group size, directory partitioning not fully fixed. IO performance; schema drift. Fixed schemas listed; ZSTD=6, <code>row_group=4096</code>; partition by <code>model</code>, <code>run_id</code>, <code>shard</code>; filename <code>part-&lt;nnnnn&gt;.parquet</code>. (\u00a7B7) DuckDB catalog Migrations, integrity checks, FK behaviors, thread settings unspecified. Registry reliability. Versioned migrations (<code>registry/migrations/*.sql</code>), FK constraints, indexes, <code>PRAGMA threads=14</code>, two\u2011phase registration. (\u00a7B8) FAISS details (GPU+cuVS) Factory choice confirmed, but training sample selection, OPQ params, memory budgeting, shard policy not fully pinned. Performance &amp; determinism. <code>OPQ64,IVF8192,PQ64</code>, train on 10M random (seed 42); <code>nprobe=64</code>; \u226410M vectors/shard; GPU add/search; save <code>.faiss</code> + <code>.ids</code>. (\u00a7B9) vLLM topology Routing is known, but request/response schemas, timeouts, retries, health checks not fully specified. Client stability. OpenAI\u2011compatible <code>/v1/embeddings</code>; per\u2011request timeout 30\u202fs; 3\u00d7 retries; health endpoints <code>/v1/models</code> &amp; <code>/health</code>; router (Nginx) config pinned; backpressure policy documented. (\u00a7B10) Ontology ingestion Supported predicates for labels/synonyms/defs, CURIE mapping, deprecations, and merges not fully specified. Correct concept catalog and IDs. Accept SKOS (<code>prefLabel</code>,<code>altLabel</code>), OBO (<code>hasExactSynonym</code>,<code>def</code>), RDFS labels; support <code>deprecated</code> and <code>replaced_by</code>; CURIE mapping table; normalize text; unique <code>urn:concept:&lt;ont&gt;:&lt;curie&gt;</code>. (\u00a7B11) Linker calibration Dev set size, fold policy, calibration storage, and runtime application not explicit. Score interpretability. Dev set 2,000 chunk\u2011concept pairs labeled; 5\u2011fold isotonic regression; parameters stored per <code>linker_run_id</code>; applied at runtime; ECE reported. (\u00a7B12) Hybrid fusion math RRF <code>k</code>, candidate pool sizes, KG boost exact math not completely fixed. Deterministic results. Dense@200 + Sparse@200; RRF(k=60); KG boosts: direct +0.08, 1\u2011hop +0.04; MMR \u03bb=0.7 at doc level. (\u00a7B13) API design Only endpoint sketch; errors, pagination, filters, sorting, rate limits, auth, and OpenAPI not fully specified. Independent backend/frontend work. Full OpenAPI spec, error schema, pagination, filters (year, source, license), API\u2011key auth, per\u2011key rate limits (local token bucket), JSON response formats. (\u00a7B14) End\u2011to\u2011end testing What E2E scenarios and fixtures to use; success criteria per stage unclear. Integration confidence. Provide seed corpora (10 docs), synthetic ontologies, golden outputs; E2E pipeline test that asserts all DDLs, artifact counts, index sizes, retrieval metrics over thresholds. (\u00a7B15) Observability Log schema, metric names, labels, exemplar traces, dashboards unspecified. Ops readiness. Define metrics (names, types), logs (JSON schema), traces (span names &amp; attributes); provide Grafana panels JSON. (\u00a7B16) Failure handling Error taxonomy, retry matrices, poison\u2011pill protocol, quarantine dirs not fixed. Robustness under real corpora. Standard error classes; retry/backoff tables; quarantine locations; incident log table and CLI. (\u00a7B17) Security &amp; licensing Key storage, license enforcement points, audit fields not formalized. Compliance and reproducibility. <code>.env</code> for secrets; license filters in downloader; persist license string and OA source; audit table records provenance; API keys in env; localhost binding. (\u00a7B18) Project structure &amp; code quality Pre\u2011commit, linters, typing level, docstrings, ADRs, contribution workflow not locked. Team velocity &amp; consistency. Pre\u2011commit (ruff, black, mypy\u2011strict), conventional commits, ADR template, mkdocs site, contribution guide, codeowners. (\u00a7B19) Local dev &amp; bootstrap No exact bootstrap steps and Make targets. Fast onboarding. Provide <code>scripts/bootstrap.sh</code>, <code>Makefile</code> targets, systemd units for vLLM + Nginx, folder layout creation. (\u00a7B20) Performance SLAs Hard SLAs and perf targets partially stated. Planning &amp; regression gates. SLAs/SLOs formalized per stage; CI gates with thresholds; perf budgets by stage. (\u00a7B21)"},{"location":"explanations/251025_HighLevelArchitecture/#part-b-exhaustive-specifications-resolutions","title":"PART B \u2014 EXHAUSTIVE SPECIFICATIONS (RESOLUTIONS)","text":""},{"location":"explanations/251025_HighLevelArchitecture/#b1-canonical-text-hashing-ids","title":"B1. Canonical text, hashing &amp; IDs","text":"<ul> <li> <p>Canonicalizer (DocTags \u2192 text):</p> </li> <li> <p>Decode UTF\u20118; apply Unicode NFC.</p> </li> <li>Replace Windows/old Mac line breaks with <code>\\n</code>.</li> <li>Collapse runs of whitespace to a single space except keep single <code>\\n</code> line breaks between block nodes (paragraph/heading/list/table caption).</li> <li>Normalize bullets (<code>\u2022</code>, <code>\u25e6</code>, <code>\u2013</code>) to <code>-</code>.</li> <li>Strip page headers/footers using DocTags region types; exclude References section by default (configurable).</li> <li>Retain figure/table captions, equations (inline LaTeX left as\u2011is), and footnotes (inlined at end of page).</li> <li>doc_id: <code>urn:doc:sha256:&lt;first16B base32&gt;</code> over canonical text (not PDF bytes).</li> <li>chunk_id: <code>urn:chunk:&lt;doc_hash&gt;:&lt;start&gt;-&lt;end&gt;</code> using character offsets in canonical text (inclusive start, exclusive end).</li> <li>Hashes computed with SHA\u2011256; extraction reproducible.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b2-tokenizer-chunking-quantization","title":"B2. Tokenizer &amp; chunking quantization","text":"<ul> <li>Tokenizer of record: HuggingFace tokenizer for Qwen/Qwen3\u2011Embedding\u20114B.</li> <li>Chunker parameters (fixed defaults): <code>target=400</code>, <code>overlap=80</code>, <code>min=120</code>, <code>max=480</code> Qwen tokens; stride=<code>target-overlap=320</code>.</li> <li>If a section is shorter than 120 tokens and can be merged with its successor without exceeding 480, merge; otherwise leave standalone.</li> <li>Sliding spill\u2011windows never cross major section boundaries (title/abstract/intro/methods/results/discussion).</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b3-doctags-selection-ocr-fallback","title":"B3. DocTags selection &amp; OCR fallback","text":"<ul> <li>Include: title, authors, abstract, section headings, paragraphs, lists, tables, figures\u2019 captions, equations.</li> <li>Exclude by default: acknowledgements, references; can enable via <code>config.doc_conversion.include_references=true</code>.</li> <li>OCR: page\u2011level fallback using Tesseract when Docling VLM avg logprob &lt; 0.5; OCR text anchored into DocTags page node with <code>provenance=\"ocr\"</code>.</li> <li>Provenance fields in DoctagsAsset: <code>{avg_logprob, ocr_pages:[int]}</code>.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b4-harvester-downloader-pyalex-first-with-fallbacks","title":"B4. Harvester &amp; downloader (PyAlex first, with fallbacks)","text":"<ul> <li> <p>PyAlex search:</p> </li> <li> <p>Query compiled as AND of: <code>topic</code> (applied to <code>title</code>, <code>abstract_inverted_index</code>, <code>fulltext</code>), optional <code>year &gt;= from_year</code>, <code>is_oa=true OR has_oa_published_version=true OR has_oa_accepted_or_published_version=true</code>.</p> </li> <li>Pagination: <code>per_page=200</code>, resume with <code>cursor</code>, cap at <code>max_works</code>.</li> <li>HTTP timeout=30\u202fs, retries=3 (exponential 1.0/2.0/4.0\u202fs), <code>User-Agent: kgforge/1.0 (+contact@example.com)</code>.</li> <li> <p>Resolution priority (first success wins):</p> </li> <li> <p><code>best_oa_location.pdf_url</code> (OpenAlex).</p> </li> <li>Any <code>locations[].pdf_url</code> with <code>is_oa=true</code> (prefer <code>publishedVersion</code> &gt; <code>acceptedVersion</code>).</li> <li>Unpaywall by DOI \u2192 <code>url_for_pdf</code>.</li> <li>Source\u2011specific fallback: arXiv (<code>/pdf/&lt;id&gt;.pdf</code>), PMC article PDF.</li> <li>MIME + size checks: Require <code>application/pdf</code>; max size 100\u202fMB (configurable).</li> <li>De\u2011dup: compute <code>pdf_sha256</code> of bytes; if duplicate of an existing doc, link new metadata to existing file and record alias (<code>openalex_id</code>, <code>doi</code>, etc.).</li> <li>Registration: Insert into <code>documents</code> with all source IDs, <code>pdf_uri</code>, license, OA source, and timestamp.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b5-dense-embeddings-qwen3-2560d","title":"B5. Dense embeddings (Qwen3 2560\u2011d)","text":"<ul> <li>API: OpenAI\u2011compatible <code>/v1/embeddings</code> with body:</li> </ul> <p><code>json   {\"model\":\"Qwen/Qwen3-Embedding-4B\",\"input\":[ \"...chunk text...\" ],\"dimensions\":2560}</code> * Response validation: assert vector length 2560; otherwise fail run with incident record. * Batching: dynamic batch size uses VRAM probe; keep \u226480% VRAM. * Normalization: client L2\u2011normalizes vector (float32) and stores original L2. * Errors: HTTP 429/5xx \u2192 retries (3\u00d7); timeouts \u2192 retries; after 3 failures, quarantine batch.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#b6-sparse-spladev3-gpu-bm25-details","title":"B6. Sparse (SPLADE\u2011v3 GPU) &amp; BM25 details","text":"<ul> <li> <p>SPLADE\u2011v3 encoder:</p> </li> <li> <p>Pre\u2011proc: lowercase; strip control chars (<code>\\x00</code>..<code>\\x1f</code>), keep punctuation; whitespace collapse single space; no stopword removal.</p> </li> <li>Tokenization: DistilBERT WordPiece; <code>max_seq_len=512</code> (truncate tail).</li> <li>AMP: <code>torch.autocast(device_type=\"cuda\", dtype=torch.float16)</code>.</li> <li>Batch size: auto\u2011tuned per GPU memory; cap at 2048 tokens/batch.</li> <li>Top\u2011K pruning: select top 256 weights by value; tie\u2011break by token id asc; sort ascending token ids for indexer.</li> <li> <p>BM25 (Pyserini/Lucene):</p> </li> <li> <p>Analyzer: StandardTokenizer \u2192 Lowercase \u2192 EnglishMinimalStemFilter \u2192 EnglishStopFilter.</p> </li> <li>Params: <code>k1=0.9</code>, <code>b=0.4</code>.</li> <li>Fields: <code>title</code> (boost 2.0), <code>section</code> (1.2), <code>body</code> (1.0). Title from DocTags title node; section from chunk.section; body from chunk text.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b7-parquet-datasets-all-embeddings-and-chunks","title":"B7. Parquet datasets (ALL embeddings and chunks)","text":"<ul> <li>Common options: <code>compression=\"ZSTD\"</code>, level=6; <code>row_group_size=4096</code>; <code>use_dictionary=True</code> for categorical columns (<code>model</code>, <code>run_id</code>, <code>doc_id</code>, <code>section</code>).</li> <li>Dense:</li> </ul> <p><code>chunk_id: string, model: string, run_id: string, dim: int16 (=2560),   vector: list&lt;float&gt;, l2_norm: float, created_at: timestamp</code></p> <p>Partitions: <code>model=Qwen3-Embedding-4B/run_id=&lt;run&gt;/shard=&lt;nnnn&gt;</code>. * SPLADE:</p> <p><code>chunk_id: string, model: string, run_id: string,   vocab_ids: list&lt;int32&gt;, weights: list&lt;float&gt;, nnz: int16, created_at: timestamp</code></p> <p>Partitions: <code>model=SPLADE-v3-distilbert/run_id=&lt;run&gt;/shard=&lt;nnnn&gt;</code>. * Chunks:</p> <p><code>chunk_id, doc_id, section, start_char:int32, end_char:int32,   doctags_span: struct&lt;node_id:string,start:int32,end:int32&gt;,   text:string, tokens:int32, created_at:timestamp</code></p> <p>Partition: <code>model=docling_hybrid/run_id=&lt;run&gt;/shard=&lt;nnnn&gt;</code>.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#b8-duckdb-registry-141-migrations-guards","title":"B8. DuckDB registry (\u2265\u202f1.4.1): migrations &amp; guards","text":"<ul> <li>Migrations: SQL files in <code>/registry/migrations</code>, applied by <code>registry apply-migrations</code>. Each migration increments <code>schema_version</code> table.</li> <li>Two\u2011phase registration: write Parquet to temp dir \u2192 validate row counts &amp; schema \u2192 <code>BEGIN</code> \u2192 insert into <code>datasets</code>/<code>runs</code> \u2192 <code>COMMIT</code> \u2192 rename dir into place; on failure <code>ROLLBACK</code> and delete temp.</li> <li>Threading: <code>PRAGMA threads=14</code>; all read_parquet with <code>union_by_name=true</code>.</li> <li>Integrity: FKs and unique constraints as defined.</li> <li>Views: <code>dense_vectors_view</code>, <code>splade_vectors_view</code>, <code>chunk_texts</code> pre\u2011created.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b9-faiss-gpu-cuvs-cuda-13","title":"B9. FAISS GPU + cuVS (CUDA 13)","text":"<ul> <li>Factory: <code>OPQ64,IVF8192,PQ64</code> (for d=2560).</li> <li>Train: Random sample of min(10M, 100% of corpus) dense vectors; seed=42; normalize before train.</li> <li>Add: On GPU; batch of N where memory \u2264 80% VRAM; record adds per shard in DuckDB.</li> <li>Search: <code>nprobe=64</code>; return distances as IP or L2 consistent with training (we use IP on normalized vectors \u2192 cosine).</li> <li>Persist: <code>.faiss</code> index + <code>.ids</code> idmap per shard; <code>faiss_indexes</code> table records <code>logical_index_id</code>, <code>shard_id</code>, file paths, config.</li> <li>Merge: logical index = set of shards; search fans out to shards, merges top\u2011K.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b10-vllm-servers-router","title":"B10. vLLM servers &amp; router","text":"<ul> <li>Granite\u2011Docling (VLM): vLLM process #1 on <code>127.0.0.1:8001</code>.</li> <li>Qwen3 embeddings: vLLM process #2 on <code>127.0.0.1:8002</code>.</li> <li>Router: Nginx on <code>:80</code> maps <code>/vlm/*</code> \u2192 8001 and <code>/v1/embeddings</code> \u2192 8002.</li> <li>Health checks: <code>/healthz</code> on router; <code>/v1/models</code> on each vLLM.</li> <li>Client policy: per\u2011request timeout 30\u202fs, retries \u00d73 (429/5xx), exponential backoff 1/2/4\u202fs; circuit\u2011breaker opens after 10 consecutive failures (per\u2011service) for 30\u202fs.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b11-ontology-ingestion-catalog","title":"B11. Ontology ingestion &amp; catalog","text":"<ul> <li>Formats: OBO, OWL/RDF (TTL, RDF/XML), SKOS (RDF).</li> <li> <p>Predicates:</p> </li> <li> <p>Labels: <code>rdfs:label</code>, <code>skos:prefLabel</code>.</p> </li> <li>Synonyms: <code>oboInOwl:hasExactSynonym</code>, <code>skos:altLabel</code>.</li> <li>Definitions: <code>IAO:0000115</code>, <code>skos:definition</code>.</li> <li>Hierarchy: <code>rdfs:subClassOf</code>, <code>skos:broader</code>, <code>BFO:part_of</code> (normalized to <code>IS_A</code> or <code>PART_OF</code>).</li> <li>Deprecation: <code>owl:deprecated</code> true; replacements: <code>iao:0100001</code> or custom <code>replaced_by</code>.</li> <li>CURIEs: Provide <code>prefix \u2192 base IRI</code> map per ontology; compute <code>urn:concept:&lt;ont&gt;:&lt;CURIE&gt;</code>.</li> <li>Normalization: lowercased, NFC, punctuation trimmed (except hyphen and slash), English lemmatization.</li> <li> <p>Concept embeddings:</p> </li> <li> <p>Dense (Qwen3 2560\u2011d): text = <code>pref_label | definition | top5_synonyms</code>; same Parquet schema as chunks (swap <code>chunk_id \u2192 concept_id</code>).</p> </li> <li>Sparse (SPLADE): text same as above; topK=128.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b12-linker-calibration-decision-policy","title":"B12. Linker calibration &amp; decision policy","text":"<ul> <li>Labeling: 2,000 chunk\u2011concept candidate pairs labeled by domain curators.</li> <li>Splits: 5\u2011fold CV; train isotonic regression to map fused score \u2192 calibrated [0,1].</li> <li>Thresholds: <code>\u03c4_high=0.62</code>, <code>\u03c4_low=0.35</code> applied after calibration.</li> <li>Outputs: save calibration params in DuckDB as JSON attached to <code>run_id</code> (linker run).</li> <li>Metrics: precision/recall/F1; Expected Calibration Error (ECE).</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b13-hybrid-retrieval-fusion-fixed-math","title":"B13. Hybrid retrieval fusion (fixed math)","text":"<ul> <li>Retrieval: <code>dense@200</code> (FAISS), <code>sparse@200</code> (BM25 and SPLADE each produce lists; we interleave by highest score then take top 200 combined sparse).</li> <li>RRF: <code>RRF_k = 60</code>. Score = <code>\u03a3 1/(k + rank_i)</code> over participating rankers (dense, bm25, splade).</li> <li>KG boost: +0.08 if any linked concept of chunk \u2208 query concepts; +0.04 if within one ontology hop; max boost +0.12.</li> <li>MMR: doc\u2011level, <code>\u03bb=0.7</code>; similarity measured by cosine of mean chunk vectors per doc.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b14-search-api-openapi-31-final","title":"B14. Search API \u2014 OpenAPI 3.1 (final)","text":"<ul> <li>Auth: Bearer API key header <code>Authorization: Bearer &lt;token&gt;</code> (tokens read from env <code>SEARCH_API_KEYS</code>, comma\u2011separated).</li> <li>Rate limits: token\u2011bucket: 120 req/min per key; 1,000 req/day per key (in\u2011memory counters, process\u2011local).</li> <li> <p>Endpoints:</p> </li> <li> <p><code>POST /search</code></p> <ul> <li>Body:</li> </ul> <p><code>json   {     \"query\": \"string\",     \"k\": 10,     \"filters\": {\"year_from\": 2018, \"year_to\": 2025, \"source\": [\"openalex\",\"arxiv\"], \"license\":[\"CC-BY\",\"CC0\"]},     \"explain\": false   }</code> * 200 Response (per result):</p> <p><code>json   {     \"doc_id\": \"urn:doc:...\",     \"chunk_id\": \"urn:chunk:...\",     \"title\": \"string\",     \"section\": \"Methods\",     \"score\": 2.381,     \"signals\": {\"dense\": 0.73, \"sparse\": 0.61, \"rrf\": 0.025, \"kg_boost\": 0.08},     \"spans\": {\"start_char\": 120, \"end_char\": 420},     \"concepts\": [{\"concept_id\":\"urn:concept:mesh:D012345\",\"label\":\"...\",\"match\":\"direct\"}]   }</code> * Errors: <code>400</code> (bad input), <code>401</code> (auth), <code>429</code> (rate limit), <code>500</code> (server).   * <code>POST /graph/concepts</code></p> <ul> <li>Body: <code>{\"q\":\"string\",\"limit\":50}</code> \u2192 returns matching concept IDs + labels + paths to root.</li> <li><code>GET /healthz</code> \u2192 <code>{status:\"ok\", components:{faiss:\"ok\", bm25:\"ok\", vllm_embeddings:\"ok\", neo4j:\"ok\"}}</code>.</li> </ul> </li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b15-endtoend-stage-testing","title":"B15. End\u2011to\u2011end &amp; stage testing","text":"<ul> <li> <p>Seed fixtures:</p> </li> <li> <p>10 OA PDFs (mix arXiv/PMC/journal) on disk for offline tests.</p> </li> <li>Tiny ontology (50 concepts, 2 levels) + MeSH subset (for integration).</li> <li>Golden outputs: a) DocTags JSON; b) chunk Parquet; c) sample dense/sparse Parquets; d) FAISS index shards; e) SPLADE/BM25 Lucene dirs.</li> <li> <p>E2E test (<code>pytest -m e2e</code>):</p> </li> <li> <p>Harvest &amp; download (mock pyalex/unpaywall): expect N PDFs.</p> </li> <li>DocTags conversion: expect pages&gt;0, DocTags file exists, avg_logprob recorded.</li> <li>Chunking: expect chunks&gt;0; token counts within bounds; offsets monotonic.</li> <li>Dense embed: expect vectors dim=2560; Parquet rows==chunks.</li> <li>SPLADE encode &amp; BM25 index: Lucene dirs exist; postings &gt;0.</li> <li>FAISS build: index files exist; search for \u201ctopic\u201d returns &gt;0 results.</li> <li>Ontology ingest + concept embeddings: counts &gt;0; CURIEs valid.</li> <li>Linker: assertions produced; at least X% linked.</li> <li>KG upsert: node/edge counts as expected.</li> <li>Search API: <code>/search</code> returns 200 and k results; latency p95 &lt; 300\u202fms on fixtures.</li> <li>Stage unit tests: contract tests per ABC; property\u2011based tests (idempotency; stable IDs); golden tests for DocTags/chunks; benchmarking tests for encoder throughput.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b16-observability-diagnostics","title":"B16. Observability &amp; diagnostics","text":"<ul> <li>Logs: JSON lines with fields: <code>ts, level, module, event, run_id, doc_id, chunk_id, duration_ms, err_code, message</code>.</li> <li> <p>Metrics (Prometheus):</p> </li> <li> <p>Counters: <code>pdf_download_success_total</code>, <code>pdf_download_failure_total{reason}</code>, <code>chunks_total</code>, <code>dense_vectors_total</code>, <code>splade_vectors_total</code>, <code>faiss_queries_total</code>, <code>bm25_queries_total</code>, <code>splade_queries_total</code>.</p> </li> <li>Histograms: <code>download_latency_ms</code>, <code>doctags_latency_ms</code>, <code>chunking_latency_ms</code>, <code>embed_dense_latency_ms</code>, <code>splade_encode_latency_ms</code>, <code>faiss_search_latency_ms</code>, <code>search_total_latency_ms</code>.</li> <li>Gauges: <code>faiss_index_size_vectors</code>, <code>lucene_docs_count</code>, <code>duckdb_open_files</code>.</li> <li> <p>Tracing (OpenTelemetry):</p> </li> <li> <p>Spans: <code>harvest.search</code>, <code>download.pdf</code>, <code>docling.to_doctags</code>, <code>chunking.hybrid</code>, <code>embed.qwen3</code>, <code>encode.splade</code>, <code>index.faiss.add</code>, <code>index.lucene.build</code>, <code>linker.run</code>, <code>kg.upsert</code>, <code>api.search</code>.</p> </li> <li>Attributes: <code>{run_id, doc_id, shard_id, model, dim, nprobe, nnz}</code>.</li> <li> <p>Dashboards: provide JSON exports for:</p> </li> <li> <p>Ingest Health (download rates/errors).</p> </li> <li>Pipeline Throughput (chunks/min, embeddings/min).</li> <li>Search SLOs (p50/p95/p99 latency, error rate).</li> <li>Index Footprint (FAISS vectors, Lucene docs).</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b17-failure-handling-resilience","title":"B17. Failure handling &amp; resilience","text":"<ul> <li> <p>Error taxonomy:</p> </li> <li> <p><code>DownloadError</code>, <code>UnsupportedMIMEError</code>, <code>DoclingError</code>, <code>OCRTimeout</code>, <code>ChunkingError</code>, <code>EmbeddingError</code>, <code>SpladeOOM</code>, <code>FaissOOM</code>, <code>IndexBuildError</code>, <code>OntologyParseError</code>, <code>LinkerCalibrationError</code>, <code>Neo4jError</code>.</p> </li> <li> <p>Retry/backoff table:</p> </li> <li> <p>External HTTP (download, vLLM): retries=3, backoff 1/2/4\u202fs.</p> </li> <li>GPU OOM: reduce batch by 50%, retry up to 2 times; if still fails \u2192 quarantine.</li> <li>FAISS add error: split shard; retry once per split.</li> <li> <p>Quarantine:</p> </li> <li> <p>PDFs: <code>/data/quarantine/pdfs/\u2026</code></p> </li> <li>Parquet batches: <code>/data/quarantine/parquet/\u2026</code></li> <li>Incidents table: <code>registry.incidents(event, subject_id, error_class, message, created_at)</code>.</li> <li>Poison pill: if a <code>doc_id</code> fails at stage X twice, mark <code>documents.status='poison'</code> and exclude from subsequent runs until manual override.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b18-security-licensing-config-secrets","title":"B18. Security, licensing, config secrets","text":"<ul> <li>Local\u2011only network binding for vLLM and Neo4j.</li> <li>API keys for Search API read from env <code>SEARCH_API_KEYS</code> (comma\u2011separated).</li> <li>Licenses stored verbatim from source; <code>documents.license</code> required for downstream processing; refuse embedding/indexing if missing or not OA.</li> <li>.env template for secrets and contact email; never commit real secrets.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b19-code-quality-governance","title":"B19. Code quality &amp; governance","text":"<ul> <li>Pre\u2011commit: <code>ruff</code>, <code>black</code>, <code>mypy --strict</code>, <code>pyupgrade</code>, <code>interrogate</code> (docstring coverage).</li> <li>Type discipline: <code>from __future__ import annotations</code>, Protocol/ABC for interfaces, <code>Final</code> constants.</li> <li>Docs: <code>mkdocs</code> site with API docs (pdoc or mkdocstrings), how\u2011to guides, and ADRs (<code>/docs/adr/0001-...md</code>).</li> <li>Conventional commits and semantic versioning per package.</li> <li>Codeowners by directory.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b20-bootstrap-devex","title":"B20. Bootstrap &amp; DevEx","text":"<ul> <li> <p>Makefile targets:</p> </li> <li> <p><code>make bootstrap</code> \u2192 create venv, install deps, install CUDA\u201113 Torch wheels, build/install FAISS GPU/cuVS (or fetch wheel), install vLLM prerelease, create dirs under <code>/data/*</code>, apply migrations, install systemd units for vLLM and Nginx.</p> </li> <li><code>make run</code> \u2192 start router, start API, ensure vLLM services up.</li> <li><code>make e2e</code> \u2192 run the full fixture pipeline &amp; API checks.</li> <li><code>make clean</code> \u2192 remove indexes and temp outputs.</li> <li>Systemd units: <code>vllm-vlm.service</code>, <code>vllm-embed.service</code>, <code>nginx.service</code>, <code>search-api.service</code>.</li> <li>Directory layout:</li> </ul> <p><code>/data/pdfs   /data/doctags   /data/parquet/{chunks,dense,sparse,concepts}   /data/lucene/{bm25,splade}   /data/faiss   /data/catalog/catalog.duckdb   /data/quarantine/{pdfs,parquet}</code></p>"},{"location":"explanations/251025_HighLevelArchitecture/#b21-slasslos-performance-budgets","title":"B21. SLAs/SLOs &amp; performance budgets","text":"<ul> <li>Downloader: \u2265 95% of accessible OA PDFs succeed within 60\u202fs; retries included.</li> <li>DocTags: p95 \u2264 6\u202fs/10 pages (Granite\u2011Docling, RTX\u202f5090).</li> <li>Chunking: \u2265 5,000 chunks/min CPU.</li> <li>Dense embedding: \u2265 8,000 chunks/min (2560\u2011d) on RTX\u202f5090 under 80% VRAM.</li> <li>SPLADE encoding: \u2265 5,000 chunks/min (AMP).</li> <li>FAISS search: p95 \u2264 40\u202fms for 200\u2011NN; build time \u2264 2\u202fh per 10M vectors shard.</li> <li>Search API: p95 end\u2011to\u2011end \u2264 300\u202fms for <code>k=10</code>, concurrency 64.</li> <li>Linker run: \u2265 1M chunk\u2011concept candidate scorings/hour; ECE \u2264 0.08; F1 \u2265 0.70 on dev set.</li> <li>CI gates: fail if nDCG@10 drops &gt; 0.02 from last green or p95 latency \u2191 &gt; 20%.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#part-c-interfaces-module-contracts-ready-for-coding","title":"PART C \u2014 INTERFACES &amp; MODULE CONTRACTS (READY FOR CODING)","text":"<p>Below are Python interface signatures (type\u2011checked; exceptions documented). All implementations must adhere to these contracts.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#c1-registry-duckdb-registryapipy","title":"C1. Registry (DuckDB) \u2014 <code>registry/api.py</code>","text":"<pre><code>class Registry(Protocol):\n    def begin_dataset(self, kind: str, run_id: str) -&gt; str: ...\n    def commit_dataset(self, dataset_id: str, parquet_root: str, rows: int) -&gt; None: ...\n    def rollback_dataset(self, dataset_id: str) -&gt; None: ...\n\n    def insert_run(self, purpose: str, model_id: str | None, revision: str | None, config: dict) -&gt; str: ...\n    def close_run(self, run_id: str, success: bool, notes: str | None = None) -&gt; None: ...\n\n    def register_documents(self, docs: list[Doc]) -&gt; None: ...\n    def register_doctags(self, assets: list[DoctagsAsset]) -&gt; None: ...\n    def register_chunks(self, dataset_id: str, chunk_rows: int) -&gt; None: ...\n    def register_dense_run(self, run_id: str, model: str, dim: int, parquet_root: str) -&gt; None: ...\n    def register_sparse_run(self, run_id: str, model: str, vocab_size: int, parquet_root: str, backend: str) -&gt; None: ...\n    def register_faiss_shard(self, logical_index_id: str, run_id: str, shard_id: int, cfg: dict, index_uri: str, idmap_uri: str) -&gt; None: ...\n\n    def emit_event(self, event_name: str, subject_id: str, payload: dict) -&gt; None: ...\n    def incident(self, event: str, subject_id: str, error_class: str, message: str) -&gt; None: ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#c2-downloader-downloadharvesterpy","title":"C2. Downloader \u2014 <code>download/harvester.py</code>","text":"<pre><code>class Harvester(Protocol):\n    def search(self, topic: str, years: str, max_works: int) -&gt; list[dict]: ...\n    def resolve_pdf(self, work: dict) -&gt; str | None: ...\n    def download_pdf(self, url: str, target_path: str) -&gt; str: ...  # returns final path\n    def run(self, topic: str, years: str, max_works: int) -&gt; list[Doc]: ...\n</code></pre> <p>Exceptions: <code>DownloadError</code>, <code>UnsupportedMIMEError</code>.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#c3-doctags-chunking-doclingvlmpy-doclinghybridpy","title":"C3. DocTags &amp; chunking \u2014 <code>docling/vlm.py</code>, <code>docling/hybrid.py</code>","text":"<pre><code>class DocTagsConverter(Protocol):\n    def to_doctags(self, pdf_path: str) -&gt; dict: ...\n    def persist(self, doc_id: str, doctags: dict) -&gt; str: ...\n\nclass Chunker(Protocol):\n    def chunk(self, doctags_uri: str) -&gt; list[Chunk]: ...\n</code></pre> <p>Exceptions: <code>DoclingError</code>, <code>OCRTimeout</code>, <code>ChunkingError</code>.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#c4-embeddings-embeddings_denseqwen3py-embeddings_sparsespladepy-embeddings_sparsebm25py","title":"C4. Embeddings \u2014 <code>embeddings_dense/qwen3.py</code>, <code>embeddings_sparse/splade.py</code>, <code>embeddings_sparse/bm25.py</code>","text":"<pre><code>class DenseEmbedder(Protocol):\n    name: str\n    dim: int\n    def embed_texts(self, texts: list[str]) -&gt; \"np.ndarray\": ...\n\nclass SparseEncoder(Protocol):\n    name: str\n    def encode(self, texts: list[str]) -&gt; list[tuple[list[int], list[float]]]: ...\n\nclass SparseIndex(Protocol):\n    def build(self, docs_iterable: \"Iterable[tuple[str, dict]]\") -&gt; None: ...\n    def search(self, query: str, k: int, fields: dict | None = None) -&gt; list[tuple[str, float]]: ...\n</code></pre> <p>Exceptions: <code>EmbeddingError</code>, <code>SpladeOOM</code>, <code>IndexBuildError</code>.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#c5-vector-store-vectorstore_faissgpupy","title":"C5. Vector store \u2014 <code>vectorstore_faiss/gpu.py</code>","text":"<pre><code>class VectorStore(Protocol):\n    def train(self, train_vectors: \"np.ndarray\", *, factory: str, seed: int = 42) -&gt; None: ...\n    def add(self, keys: list[str], vectors: \"np.ndarray\") -&gt; None: ...\n    def search(self, query: \"np.ndarray\", k: int) -&gt; list[tuple[str, float]]: ...\n    def save(self, index_uri: str, idmap_uri: str) -&gt; None: ...\n    def load(self, index_uri: str, idmap_uri: str) -&gt; None: ...\n</code></pre> <p>Exceptions: <code>FaissOOM</code>, <code>IndexBuildError</code>.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#c6-ontology-catalog-ontologyloaderpy","title":"C6. Ontology &amp; catalog \u2014 <code>ontology/loader.py</code>","text":"<pre><code>class OntologyCatalog(Protocol):\n    def load(self, inputs: list[dict]) -&gt; list[Concept]: ...\n    def neighbors(self, concept_id: str, depth: int = 1) -&gt; list[str]: ...\n    def get(self, concept_id: str) -&gt; Concept | None: ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#c7-linker-linkinglinkerpy","title":"C7. Linker \u2014 <code>linking/linker.py</code>","text":"<pre><code>class Linker(Protocol):\n    def candidate_concepts(self, chunk_text: str, k_sparse: int, k_lex: int) -&gt; list[str]: ...\n    def score(self, chunk_vec: \"np.ndarray\", concept_vecs: dict[str, \"np.ndarray\"], features: dict) -&gt; float: ...\n    def calibrate(self, devset: list[tuple[float, int]]) -&gt; dict: ...\n    def link_chunk(self, chunk: Chunk) -&gt; list[LinkAssertion]: ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#c8-kg-builder-kg_builderneo4jpy","title":"C8. KG builder \u2014 <code>kg_builder/neo4j.py</code>","text":"<pre><code>class GraphStore(Protocol):\n    def upsert_docs(self, docs: list[Doc]) -&gt; None: ...\n    def upsert_chunks(self, chunks: list[Chunk]) -&gt; None: ...\n    def upsert_concepts(self, concepts: list[Concept]) -&gt; None: ...\n    def upsert_mentions(self, assertions: list[LinkAssertion]) -&gt; None: ...\n    def linked_concepts(self, chunk_id: str) -&gt; list[str]: ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#c9-search-api-search_apiapppy","title":"C9. Search API \u2014 <code>search_api/app.py</code>","text":"<ul> <li> <p>Bootstraps FAISS handle(s), BM25, SPLADE, OntologyCatalog, GraphStore, DenseEmbedder; exposes FastAPI app with routers:</p> </li> <li> <p><code>/search</code>, <code>/graph/concepts</code>, <code>/healthz</code>.</p> </li> <li>All responses validated with <code>pydantic</code> models.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#part-d-orchestration-prefect-2x","title":"PART D \u2014 ORCHESTRATION (PREFECT 2.x)","text":""},{"location":"explanations/251025_HighLevelArchitecture/#d1-flow-graph-idempotency-keys","title":"D1. Flow graph &amp; idempotency keys","text":"<pre><code>harvest_and_download \u2192 convert_to_doctags \u2192 chunk_with_docling \u2192 \\\nembed_dense_qwen3 \u2510                                        \\\nencode_splade_v3  \u251c\u2500\u2500 build_bm25_index \u2510                    \\\n                   \u2514\u2500\u2500 build_faiss_index  \u2192 ingest_ontologies \u2192 embed_concepts \\\n                                                   \u2192 link_chunks_to_concepts \u2192 kg_upsert\n</code></pre> <ul> <li> <p>Idempotency keys:</p> </li> <li> <p>Harvest: <code>(topic, years, work_id)</code></p> </li> <li>DocTags: <code>pdf_sha256</code></li> <li>Chunking: <code>(doc_id, chunker_version)</code></li> <li>Dense: <code>(chunk_dataset_id, model=Qwen3, dim=2560)</code></li> <li>SPLADE: <code>(chunk_dataset_id, model=SPLADE-v3, topk=256)</code></li> <li>FAISS: <code>(dense_run_id, factory, nlist, m, nprobe)</code></li> <li>Ontology: <code>(ontology_id, src_uri, loader_version)</code></li> <li>Linker: <code>(chunk_dataset_id, ontology_id, linker_version)</code></li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#d2-concurrency-retries","title":"D2. Concurrency &amp; retries","text":"<ul> <li>Download: 8 workers, retry matrix as in \u00a7B17.</li> <li>VLM/embeddings/SPLADE: GPU queue depth=2; batch auto\u2011tune; 3 retries on transient errors.</li> <li>Index builds: FAISS shards in parallel up to 2; Lucene index single\u2011writer with merges at end.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#d3-cli-commands-invoke-via-typer","title":"D3. CLI commands (invoke via Typer)","text":"<ul> <li><code>kgf harvest --topic \"LLM alignment\" --years 2018..2025 --max 20000</code></li> <li><code>kgf doctags --input /data/pdfs --out /data/doctags</code></li> <li><code>kgf chunk --doctags /data/doctags --out /data/parquet/chunks</code></li> <li><code>kgf embed-dense --chunks &lt;dataset_id&gt;</code></li> <li><code>kgf encode-splade --chunks &lt;dataset_id&gt;</code></li> <li><code>kgf index-bm25 --chunks &lt;dataset_id&gt;</code></li> <li><code>kgf index-faiss --dense-run &lt;run_id&gt;</code></li> <li><code>kgf ingest-ontology --id mesh --uri /data/ontologies/mesh.obo</code></li> <li><code>kgf embed-concepts --ontology mesh</code></li> <li><code>kgf link --chunks &lt;dataset_id&gt; --ontology mesh</code></li> <li><code>kgf kg-upsert --link-run &lt;run_id&gt;</code></li> <li><code>kgf api --port 8080</code></li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#part-e-endtoend-acceptance-criteria-per-workstream","title":"PART E \u2014 END\u2011TO\u2011END ACCEPTANCE CRITERIA (PER WORKSTREAM)","text":"<p>Downloader</p> <ul> <li>Given topic and year range, at least N documents discovered; \u226595% OA downloads succeed; duplicates deduped; license persisted.</li> </ul> <p>DocTags</p> <ul> <li>For each PDF, DocTags exists; pages count &gt; 0; avg_logprob recorded; OCR fallback pages tracked.</li> </ul> <p>Chunking</p> <ul> <li>For each <code>doc_id</code>, sum of chunk spans covers \u226590% of canonical text; token counts in [min,max]; offsets monotonic.</li> </ul> <p>Dense embeddings</p> <ul> <li>Row count == chunk rows; all vectors dim=2560; Parquet schema validated; L2 norms avg in [0.95,1.05] post\u2011norm.</li> </ul> <p>SPLADE/BM25</p> <ul> <li>SPLADE nnz distribution mean \u2248 220\u2013260; Lucene impact index has postings; BM25 index doc count == chunk count.</li> </ul> <p>FAISS</p> <ul> <li>Trained index exists; top\u2011K search returns results; p95 latency \u2264 40\u202fms for K=200 on fixture set.</li> </ul> <p>Ontology &amp; concepts</p> <ul> <li>Loaded concepts &gt; 0; no duplicate CURIEs; concept embeddings Parquet rows == concepts; ontology edges inserted.</li> </ul> <p>Linker</p> <ul> <li>Produces assertions; on fixtures F1 \u2265 0.70; ECE \u2264 0.08; calibration saved and applied.</li> </ul> <p>KG</p> <ul> <li>Node uniqueness constraints enforced; edge counts match assertions; query <code>linked_concepts(chunk)</code> returns expected IDs.</li> </ul> <p>Search API</p> <ul> <li><code>/healthz</code> OK; <code>/search</code> returns k results with explanations; p95 \u2264 300\u202fms on fixtures.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#part-f-sample-openapi-excerpt","title":"PART F \u2014 SAMPLE OPENAPI (EXCERPT)","text":"<pre><code>openapi: 3.1.0\ninfo: {title: KGForge Search API, version: 1.0.0}\npaths:\n  /search:\n    post:\n      security: [{ApiKeyAuth: []}]\n      requestBody:\n        content:\n          application/json:\n            schema:\n              type: object\n              required: [query]\n              properties:\n                query: {type: string, minLength: 1}\n                k: {type: integer, minimum: 1, maximum: 100, default: 10}\n                filters:\n                  type: object\n                  properties:\n                    year_from: {type: integer}\n                    year_to: {type: integer}\n                    source: {type: array, items: {type: string}}\n                    license: {type: array, items: {type: string}}\n                explain: {type: boolean, default: false}\n      responses:\n        '200':\n          description: OK\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  results:\n                    type: array\n                    items:\n                      $ref: '#/components/schemas/Result'\n        '400': {$ref: '#/components/responses/BadRequest'}\n        '401': {$ref: '#/components/responses/Unauthorized'}\n        '429': {$ref: '#/components/responses/TooManyRequests'}\n        '500': {$ref: '#/components/responses/ServerError'}\ncomponents:\n  securitySchemes:\n    ApiKeyAuth:\n      type: http\n      scheme: bearer\n  schemas:\n    Result:\n      type: object\n      required: [doc_id, chunk_id, title, section, score]\n      properties:\n        doc_id: {type: string}\n        chunk_id: {type: string}\n        title: {type: string}\n        section: {type: string}\n        score: {type: number}\n        signals:\n          type: object\n          properties:\n            dense: {type: number}\n            sparse: {type: number}\n            rrf: {type: number}\n            kg_boost: {type: number}\n        spans:\n          type: object\n          properties:\n            start_char: {type: integer}\n            end_char: {type: integer}\n        concepts:\n          type: array\n          items:\n            type: object\n            properties:\n              concept_id: {type: string}\n              label: {type: string}\n              match: {type: string, enum: [direct,nearby]}\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#part-g-implementation-checklists-by-workstream","title":"PART G \u2014 IMPLEMENTATION CHECKLISTS (BY WORKSTREAM)","text":"<ol> <li> <p>Download &amp; Harvest</p> </li> <li> <p>[ ] PyAlex client; config; retries; polite UA.</p> </li> <li>[ ] Fallback Unpaywall client; DOI resolver.</li> <li>[ ] PDF validator; MIME/size checks; hash; dedup; storage path; registry insert.</li> <li> <p>[ ] Metrics/logs/tests; CLI command; E2E hooks.</p> </li> <li> <p>DocTags Conversion</p> </li> <li> <p>[ ] vLLM client; page batches; OCR fallback; canonicalizer; provenance.</p> </li> <li> <p>[ ] Persist <code>.dt.json.zst</code>; register DoctagsAsset; golden tests.</p> </li> <li> <p>Hybrid Chunker</p> </li> <li> <p>[ ] Qwen tokenizer; section splitting; sliding windows; spans &amp; offsets.</p> </li> <li> <p>[ ] Parquet writer; ID scheme; tests for bounds &amp; coverage.</p> </li> <li> <p>Dense Embedder (Qwen3 2560)</p> </li> <li> <p>[ ] OpenAI API client; dimension param; batching; normalization; retries.</p> </li> <li> <p>[ ] Parquet writer; registry run; throughput metric; tests.</p> </li> <li> <p>SPLADE\u2011v3 &amp; BM25</p> </li> <li> <p>[ ] Torch encoder; AMP; top\u2011K; Parquet writer; impact index streaming.</p> </li> <li> <p>[ ] BM25 analyzer config; indexer; search adapter; tests.</p> </li> <li> <p>FAISS GPU/cuVS</p> </li> <li> <p>[ ] Loader; training sampler; add &amp; search; shard manager; save/load.</p> </li> <li> <p>[ ] Query adapter (top\u2011K); memory gauges; tests.</p> </li> <li> <p>Ontology &amp; Concepts</p> </li> <li> <p>[ ] Parsers (OBO/OWL/SKOS); normalization; CURIEs; edges.</p> </li> <li> <p>[ ] Dense &amp; SPLADE embeddings for concepts; Parquet &amp; registry.</p> </li> <li> <p>Linker</p> </li> <li> <p>[ ] Candidate gen (SPLADE+lexicon); feature extractor; fusion; calibration; thresholds.</p> </li> <li> <p>[ ] Assertions Parquet; metrics; tests.</p> </li> <li> <p>KG Builder (Neo4j)</p> </li> <li> <p>[ ] Node/edge upserts; uniqueness constraints; indexes.</p> </li> <li> <p>[ ] Linked concept lookups; tests.</p> </li> <li> <p>Search API</p> <ul> <li>[ ] Startup wiring; RRF; KG boosts; MMR; filters; auth; rate limits.</li> <li>[ ] OpenAPI; integration tests; latency regression tests.</li> </ul> </li> <li> <p>Registry &amp; Orchestration</p> <ul> <li>[ ] Migrations; two\u2011phase dataset registration; events &amp; incidents.</li> <li>[ ] Prefect flows; CLI; idempotency keys; dashboards.</li> </ul> </li> <li> <p>Ops</p> <ul> <li>[ ] systemd units; Nginx config; health checks; log rotation; backup scripts (Parquet+DuckDB).</li> </ul> </li> </ol>"},{"location":"explanations/251025_HighLevelArchitecture/#part-h-example-config-final-ready-to-commit","title":"PART H \u2014 EXAMPLE CONFIG (FINAL; READY TO COMMIT)","text":"<pre><code>system:\n  os: ubuntu-24.04\n  threads: 14\n  seed: 42\n  parquet_root: /data/parquet\n  artifacts_root: /data/artifacts\n  duckdb_path: /data/catalog/catalog.duckdb\n\nruntime:\n  python: \"3.13\"\n  cuda: \"13.0\"\n  torch: \"2.9\"\n  duckdb_min: \"1.4.1\"\n  vllm_channel: \"pre-release-cuda13\"\n\nnetwork:\n  nginx_port: 80\n  vlm_port: 8001\n  emb_port: 8002\n  api_port: 8080\n\nharvest:\n  provider: pyalex\n  per_page: 200\n  max_works: 20000\n  years: \"&gt;=2018\"\n  filters:\n    is_oa: true\n    has_oa_published_version: true\n  fallbacks:\n    unpaywall: true\n    arxiv: true\n    pmc: true\n  concurrency: 8\n  timeout_sec: 60\n  retries: 3\n\ndoc_conversion:\n  vlm_model: ibm-granite/granite-docling-258M\n  vlm_revision: untied\n  endpoint: http://localhost/vlm/\n  dpi: 220\n  page_batch: 8\n  ocr_fallback: true\n  max_pages: 2000\n  timeout_sec: 120\n\nchunking:\n  engine: docling_hybrid\n  target_tokens: 400\n  overlap_tokens: 80\n  min_tokens: 120\n  max_tokens: 480\n\ndense_embedding:\n  model: Qwen/Qwen3-Embedding-4B\n  endpoint: http://localhost/v1/embeddings\n  output_dim: 2560\n  parquet_out: ${system.parquet_root}/dense/model=Qwen3-Embedding-4B/run=${run_id}\n\nsparse_embedding:\n  splade:\n    model: naver/splade-v3-distilbert\n    device: cuda\n    amp: fp16\n    max_seq_len: 512\n    topk: 256\n    parquet_out: ${system.parquet_root}/sparse/model=SPLADE-v3-distilbert/run=${run_id}\n  bm25:\n    k1: 0.9\n    b: 0.4\n    field_boosts: { title: 2.0, section: 1.2, body: 1.0 }\n    index_dir: /data/lucene/bm25\n\nfaiss:\n  index_factory: OPQ64,IVF8192,PQ64\n  nprobe: 64\n  train_samples: 10000000\n  shards:\n    max_vectors_per_shard: 10000000\n  gpu: true\n  cuvs: true\n  output_dir: /data/faiss/qwen3_ivfpq\n\nontology:\n  inputs:\n    - { ontology_id: mesh, format: obo, uri: /data/ontologies/mesh.obo }\n    - { ontology_id: go,   format: obo, uri: /data/ontologies/go.obo }\n  concept_embed:\n    dense_model: Qwen3-Embedding-4B\n    dense_dim: 2560\n    splade_model: SPLADE-v3-distilbert\n    splade_topk: 128\n\nlinker:\n  candidates: { splade_topk: 100, lexicon_topk: 50 }\n  fusion_weights: { dense: 0.55, sparse: 0.35, lexical: 0.10, depth_bonus_per_level: 0.02, depth_cap: 0.10 }\n  thresholds: { high: 0.62, low: 0.35 }\n  calibration: isotonic\n\ngraph:\n  backend: neo4j\n  uri: bolt://localhost:7687\n  user: neo4j\n  password_env: NEO4J_PASSWORD\n\nsearch:\n  k: 10\n  dense_candidates: 200\n  sparse_candidates: 200\n  rrf_k: 60\n  mmr_lambda: 0.7\n  kg_boosts: { direct: 0.08, one_hop: 0.04 }\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#part-i-what-happens-next","title":"PART I \u2014 WHAT HAPPENS NEXT","text":"<p>With these resolved specifications:</p> <ul> <li>Each workstream has clear interfaces, error types, Parquet schemas, DuckDB DDL, router/API contracts, orchestration flows, observability, SLAs, and acceptance tests.</li> <li>Teams can now draft detailed implementation plans and begin coding, confident that components will integrate cleanly on the single\u2011box target environment.</li> </ul> <p>If you want, I can generate skeleton packages (directories, ABCs, Pydantic models, Typer CLI, Prefect flows, Makefile, pre\u2011commit, and sample tests) to serve as the starting point for implementation.</p>"},{"location":"how-to/","title":"How-to Guides","text":"<p>Practical recipes for common workflows will live here. Add a page per topic and extend the toctree below.</p> <pre><code>:maxdepth: 1\n:hidden:\n</code></pre>"}]}
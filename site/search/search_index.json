{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"kgfoundry Documentation","text":"<p>Welcome to the kgfoundry documentation set. Use the sections below to find onboarding guides, deep dives, API reference material, and architecture decisions.</p> <pre><code>:maxdepth: 2\n:caption: Guide\ngetting-started\nhow-to/index\n</code></pre> <pre><code>:maxdepth: 2\n:caption: Explanations\nexplanations/index\n</code></pre> <pre><code>:maxdepth: 2\n:caption: Reference\nautoapi/index\narchitecture/index\nreference/test-matrix\nreference/observability/metrics\nreference/observability/logs\nreference/observability/traces\nreference/observability/config\nreference/schemas/index\nreference/graphs/index\npolicies/visibility-policy\n</code></pre> <pre><code>:hidden:\n:glob:\n\nautoapi/src/*/index\n</code></pre> <pre><code>:maxdepth: 1\n:caption: Gallery\ngallery/index\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide walks you through the documentation workflow for kgfoundry.</p> <ol> <li>Install the documentation extras once: <code>pip install -e \".[docs]\"</code>.</li> <li>Generate docstrings and normalize formatting with <code>make docstrings</code>.</li> <li>Refresh package READMEs with deep links via <code>make readmes</code> (optionally set <code>DOCS_LINK_MODE</code> before running).</li> <li>Build the docs corpus with <code>make html</code>, <code>make json</code>, and <code>make symbols</code> as needed.</li> <li>Run <code>make watch</code> for live reloading while editing content or code.</li> </ol> <p>Export <code>DOCS_LINK_MODE=github</code> when you need GitHub permalinks instead of local editor links. Override <code>DOCS_EDITOR</code> to switch between VS Code and PyCharm deep links.</p>"},{"location":"_build/config/","title":"Observability Instrumentation","text":""},{"location":"_build/config/#logs","title":"Logs","text":"<p>Collected 1 structured log template(s); see <code>docs/_build/log_events.json</code>.</p>"},{"location":"_build/graphs/subsystems_meta/","title":"Subsystem Graph Metadata","text":"<p>Cycle enumeration skipped: No</p> <p>Cycle enumeration completed normally.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/docling/","title":"<code>docling</code>","text":"<p>Docling utilities.</p>"},{"location":"api/docling/canonicalizer/","title":"<code>docling.canonicalizer</code>","text":"<p>Canonicalizer utilities.</p>"},{"location":"api/docling/canonicalizer/#docling.canonicalizer.canonicalize_text","title":"<code>canonicalize_text(blocks)</code>","text":"<p>Compute canonicalize text.</p> <p>Carry out the canonicalize text operation.</p>"},{"location":"api/docling/canonicalizer/#docling.canonicalizer.canonicalize_text--parameters","title":"Parameters","text":"<p>blocks : List[str]     Description for <code>blocks</code>.</p>"},{"location":"api/docling/canonicalizer/#docling.canonicalizer.canonicalize_text--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/docling/canonicalizer/#docling.canonicalizer.canonicalize_text--examples","title":"Examples","text":"<p>from docling.canonicalizer import canonicalize_text result = canonicalize_text(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/docling/canonicalizer.py</code> <pre><code>def canonicalize_text(blocks: list[str]) -&gt; str:\n    \"\"\"Compute canonicalize text.\n\n    Carry out the canonicalize text operation.\n\n    Parameters\n    ----------\n    blocks : List[str]\n        Description for ``blocks``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from docling.canonicalizer import canonicalize_text\n    &gt;&gt;&gt; result = canonicalize_text(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n\n    def norm(s: str) -&gt; str:\n        \"\"\"Compute norm.\n\n        Carry out the norm operation.\n\n        Parameters\n        ----------\n        s : str\n            Description for ``s``.\n\n        Returns\n        -------\n        str\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from docling.canonicalizer import norm\n        &gt;&gt;&gt; result = norm(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        s = unicodedata.normalize(\"NFC\", s)\n        s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n        s = re.sub(r\"[\\u2022\\u25E6\\u2013]\", \"-\", s)  # bullets/dashes\n        s = re.sub(r\"[\\x00-\\x1F]\", \" \", s)\n        return re.sub(r\"\\s+\", \" \", s).strip()\n\n    normed = [norm(b) for b in blocks if b.strip()]\n    return \"\\n\".join(normed)\n</code></pre>"},{"location":"api/docling/hybrid/","title":"<code>docling.hybrid</code>","text":"<p>Hybrid utilities.</p>"},{"location":"api/docling/hybrid/#docling.hybrid.HybridChunker","title":"<code>HybridChunker</code>","text":"<p>Describe HybridChunker.</p> Source code in <code>src/docling/hybrid.py</code> <pre><code>class HybridChunker:\n    \"\"\"Describe HybridChunker.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/docling/vlm/","title":"<code>docling.vlm</code>","text":"<p>Vlm utilities.</p>"},{"location":"api/docling/vlm/#docling.vlm.GraniteDoclingVLM","title":"<code>GraniteDoclingVLM</code>","text":"<p>Describe GraniteDoclingVLM.</p> Source code in <code>src/docling/vlm.py</code> <pre><code>class GraniteDoclingVLM:\n    \"\"\"Describe GraniteDoclingVLM.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/download/","title":"<code>download</code>","text":"<p>Download utilities.</p>"},{"location":"api/download/cli/","title":"<code>download.cli</code>","text":"<p>Cli utilities.</p>"},{"location":"api/download/cli/#download.cli.harvest","title":"<code>harvest(topic, years='&gt;=2018', max_works=20000)</code>","text":"<p>Compute harvest.</p> <p>Carry out the harvest operation.</p>"},{"location":"api/download/cli/#download.cli.harvest--parameters","title":"Parameters","text":"<p>topic : str     Description for <code>topic</code>. years : str | None     Description for <code>years</code>. max_works : int | None     Description for <code>max_works</code>.</p>"},{"location":"api/download/cli/#download.cli.harvest--examples","title":"Examples","text":"<p>from download.cli import harvest harvest(..., ..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/download/cli.py</code> <pre><code>@app.command()\ndef harvest(topic: str, years: str = \"&gt;=2018\", max_works: int = 20000) -&gt; None:\n    \"\"\"Compute harvest.\n\n    Carry out the harvest operation.\n\n    Parameters\n    ----------\n    topic : str\n        Description for ``topic``.\n    years : str | None\n        Description for ``years``.\n    max_works : int | None\n        Description for ``max_works``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from download.cli import harvest\n    &gt;&gt;&gt; harvest(..., ..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    typer.echo(f\"[dry-run] would harvest topic={topic!r}, years={years}, max_works={max_works}\")\n</code></pre>"},{"location":"api/download/harvester/","title":"<code>download.harvester</code>","text":"<p>Harvester utilities.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester","title":"<code>OpenAccessHarvester</code>","text":"<p>Download documents via OpenAlex.</p> <p>Raise :class:<code>kgfoundry_common.errors.DownloadError</code> when retrieval fails.</p> Source code in <code>src/download/harvester.py</code> <pre><code>class OpenAccessHarvester:\n    \"\"\"Download documents via OpenAlex.\n\n    Raise :class:`kgfoundry_common.errors.DownloadError` when retrieval fails.\n    \"\"\"\n\n    def __init__(\n        self,\n        user_agent: str,\n        contact_email: str,\n        openalex_base: str = \"https://api.openalex.org\",\n        unpaywall_base: str = \"https://api.unpaywall.org\",\n        pdf_host_base: str | None = None,\n        out_dir: str = \"/data/pdfs\",\n    ) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        user_agent : str\n            Description for ``user_agent``.\n        contact_email : str\n            Description for ``contact_email``.\n        openalex_base : str | None\n            Description for ``openalex_base``.\n        unpaywall_base : str | None\n            Description for ``unpaywall_base``.\n        pdf_host_base : str | None\n            Description for ``pdf_host_base``.\n        out_dir : str | None\n            Description for ``out_dir``.\n        \"\"\"\n\n\n        self.ua = user_agent\n        self.email = contact_email\n        self.openalex = openalex_base.rstrip(\"/\")\n        self.unpaywall = unpaywall_base.rstrip(\"/\")\n        self.pdf_host = (pdf_host_base or \"\").rstrip(\"/\")\n        self.out_dir = out_dir\n        os.makedirs(self.out_dir, exist_ok=True)\n        self.session = requests.Session()\n        self.session.headers.update({\"User-Agent\": f\"{self.ua} ({self.email})\"})\n\n    def search(self, topic: str, years: str, max_works: int) -&gt; list[dict[str, Any]]:\n        \"\"\"Compute search.\n\n        Carry out the search operation.\n\n        Parameters\n        ----------\n        topic : str\n            Description for ``topic``.\n        years : str\n            Description for ``years``.\n        max_works : int\n            Description for ``max_works``.\n\n        Returns\n        -------\n        List[dict[str, typing.Any]]\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from download.harvester import search\n        &gt;&gt;&gt; result = search(..., ..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        \"\"\"\n        url = f\"{self.openalex}/works\"\n        params: dict[str, str | int] = {\n            \"topic\": topic,\n            \"per_page\": min(200, max_works),\n            \"cursor\": \"*\",\n        }\n        if years:\n            params[\"filter\"] = years\n        response = self.session.get(url, params=params, timeout=30)\n        response.raise_for_status()\n        data = response.json()\n        return data.get(\"results\", [])[:max_works]\n\n    def resolve_pdf(self, work: dict[str, Any]) -&gt; str | None:\n        \"\"\"Compute resolve pdf.\n\n        Carry out the resolve pdf operation.\n\n        Parameters\n        ----------\n        work : collections.abc.Mapping\n            Description for ``work``.\n\n        Returns\n        -------\n        str | None\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from download.harvester import resolve_pdf\n        &gt;&gt;&gt; result = resolve_pdf(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        \"\"\"\n        best = work.get(\"best_oa_location\") or {}\n        if best and best.get(\"pdf_url\"):\n            return best[\"pdf_url\"]\n        for location in work.get(\"locations\", []):\n            if location.get(\"pdf_url\"):\n                return location[\"pdf_url\"]\n        doi = work.get(\"doi\")\n        if doi:\n            response = self.session.get(\n                f\"{self.unpaywall}/v2/{doi}\", params={\"email\": self.email}, timeout=15\n            )\n            if response.ok:\n                payload = response.json()\n                url = (payload.get(\"best_oa_location\") or {}).get(\"url_for_pdf\")\n                if url:\n                    return url\n        if self.pdf_host and doi:\n            return f\"{self.pdf_host}/pdf/{doi.replace('/', '_')}.pdf\"\n        return None\n\n    def download_pdf(self, url: str, target_path: str) -&gt; str:\n        \"\"\"Compute download pdf.\n\n        Carry out the download pdf operation.\n\n        Parameters\n        ----------\n        url : str\n            Description for ``url``.\n        target_path : str\n            Description for ``target_path``.\n\n        Returns\n        -------\n        str\n            Description of return value.\n\n        Raises\n        ------\n        DownloadError\n            Raised when validation fails.\n        UnsupportedMIMEError\n            Raised when validation fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from download.harvester import download_pdf\n        &gt;&gt;&gt; result = download_pdf(..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        \"\"\"\n        response = self.session.get(url, timeout=60)\n        if response.status_code != HTTP_OK:\n            message = f\"Bad status {response.status_code} for {url}\"\n            raise DownloadError(message)\n        content_type = response.headers.get(\"Content-Type\", \"application/pdf\")\n        if not content_type.startswith(\"application/\"):\n            message = f\"Not a PDF-like content type: {content_type}\"\n            raise UnsupportedMIMEError(message)\n        with open(target_path, \"wb\") as file_handle:\n            file_handle.write(response.content)\n        return target_path\n\n    def run(self, topic: str, years: str, max_works: int) -&gt; list[Doc]:\n        \"\"\"Compute run.\n\n        Carry out the run operation.\n\n        Parameters\n        ----------\n        topic : str\n            Description for ``topic``.\n        years : str\n            Description for ``years``.\n        max_works : int\n            Description for ``max_works``.\n\n        Returns\n        -------\n        List[src.kgfoundry_common.models.Doc]\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from download.harvester import run\n        &gt;&gt;&gt; result = run(..., ..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        \"\"\"\n        docs: list[Doc] = []\n        works = self.search(topic, years, max_works)\n        for work in works:\n            pdf_url = self.resolve_pdf(work)\n            if not pdf_url:\n                continue\n            filename = (work.get(\"doi\") or work.get(\"id\") or str(int(time.time() * 1000))).replace(\n                \"/\", \"_\"\n            ) + \".pdf\"\n            destination = os.path.join(self.out_dir, filename)\n            self.download_pdf(pdf_url, destination)\n            doc = Doc(\n                id=f\"urn:doc:source:openalex:{work.get('id', 'unknown')}\",\n                openalex_id=work.get(\"id\"),\n                doi=work.get(\"doi\"),\n                title=work.get(\"title\", \"\"),\n                authors=[],\n                pub_date=None,\n                license=None,\n                language=\"en\",\n                pdf_uri=destination,\n                source=\"openalex\",\n                content_hash=None,\n            )\n            docs.append(doc)\n        return docs\n</code></pre>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.__init__","title":"<code>__init__(user_agent, contact_email, openalex_base='https://api.openalex.org', unpaywall_base='https://api.unpaywall.org', pdf_host_base=None, out_dir='/data/pdfs')</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.__init__--parameters","title":"Parameters","text":"<p>user_agent : str     Description for <code>user_agent</code>. contact_email : str     Description for <code>contact_email</code>. openalex_base : str | None     Description for <code>openalex_base</code>. unpaywall_base : str | None     Description for <code>unpaywall_base</code>. pdf_host_base : str | None     Description for <code>pdf_host_base</code>. out_dir : str | None     Description for <code>out_dir</code>.</p> Source code in <code>src/download/harvester.py</code> <pre><code>def __init__(\n    self,\n    user_agent: str,\n    contact_email: str,\n    openalex_base: str = \"https://api.openalex.org\",\n    unpaywall_base: str = \"https://api.unpaywall.org\",\n    pdf_host_base: str | None = None,\n    out_dir: str = \"/data/pdfs\",\n) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    user_agent : str\n        Description for ``user_agent``.\n    contact_email : str\n        Description for ``contact_email``.\n    openalex_base : str | None\n        Description for ``openalex_base``.\n    unpaywall_base : str | None\n        Description for ``unpaywall_base``.\n    pdf_host_base : str | None\n        Description for ``pdf_host_base``.\n    out_dir : str | None\n        Description for ``out_dir``.\n    \"\"\"\n\n\n    self.ua = user_agent\n    self.email = contact_email\n    self.openalex = openalex_base.rstrip(\"/\")\n    self.unpaywall = unpaywall_base.rstrip(\"/\")\n    self.pdf_host = (pdf_host_base or \"\").rstrip(\"/\")\n    self.out_dir = out_dir\n    os.makedirs(self.out_dir, exist_ok=True)\n    self.session = requests.Session()\n    self.session.headers.update({\"User-Agent\": f\"{self.ua} ({self.email})\"})\n</code></pre>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.download_pdf","title":"<code>download_pdf(url, target_path)</code>","text":"<p>Compute download pdf.</p> <p>Carry out the download pdf operation.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.download_pdf--parameters","title":"Parameters","text":"<p>url : str     Description for <code>url</code>. target_path : str     Description for <code>target_path</code>.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.download_pdf--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.download_pdf--raises","title":"Raises","text":"<p>DownloadError     Raised when validation fails. UnsupportedMIMEError     Raised when validation fails.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.download_pdf--examples","title":"Examples","text":"<p>from download.harvester import download_pdf result = download_pdf(..., ...) result  # doctest: +ELLIPSIS</p> Source code in <code>src/download/harvester.py</code> <pre><code>def download_pdf(self, url: str, target_path: str) -&gt; str:\n    \"\"\"Compute download pdf.\n\n    Carry out the download pdf operation.\n\n    Parameters\n    ----------\n    url : str\n        Description for ``url``.\n    target_path : str\n        Description for ``target_path``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Raises\n    ------\n    DownloadError\n        Raised when validation fails.\n    UnsupportedMIMEError\n        Raised when validation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from download.harvester import download_pdf\n    &gt;&gt;&gt; result = download_pdf(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    \"\"\"\n    response = self.session.get(url, timeout=60)\n    if response.status_code != HTTP_OK:\n        message = f\"Bad status {response.status_code} for {url}\"\n        raise DownloadError(message)\n    content_type = response.headers.get(\"Content-Type\", \"application/pdf\")\n    if not content_type.startswith(\"application/\"):\n        message = f\"Not a PDF-like content type: {content_type}\"\n        raise UnsupportedMIMEError(message)\n    with open(target_path, \"wb\") as file_handle:\n        file_handle.write(response.content)\n    return target_path\n</code></pre>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.resolve_pdf","title":"<code>resolve_pdf(work)</code>","text":"<p>Compute resolve pdf.</p> <p>Carry out the resolve pdf operation.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.resolve_pdf--parameters","title":"Parameters","text":"<p>work : collections.abc.Mapping     Description for <code>work</code>.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.resolve_pdf--returns","title":"Returns","text":"<p>str | None     Description of return value.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.resolve_pdf--examples","title":"Examples","text":"<p>from download.harvester import resolve_pdf result = resolve_pdf(...) result  # doctest: +ELLIPSIS</p> Source code in <code>src/download/harvester.py</code> <pre><code>def resolve_pdf(self, work: dict[str, Any]) -&gt; str | None:\n    \"\"\"Compute resolve pdf.\n\n    Carry out the resolve pdf operation.\n\n    Parameters\n    ----------\n    work : collections.abc.Mapping\n        Description for ``work``.\n\n    Returns\n    -------\n    str | None\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from download.harvester import resolve_pdf\n    &gt;&gt;&gt; result = resolve_pdf(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    \"\"\"\n    best = work.get(\"best_oa_location\") or {}\n    if best and best.get(\"pdf_url\"):\n        return best[\"pdf_url\"]\n    for location in work.get(\"locations\", []):\n        if location.get(\"pdf_url\"):\n            return location[\"pdf_url\"]\n    doi = work.get(\"doi\")\n    if doi:\n        response = self.session.get(\n            f\"{self.unpaywall}/v2/{doi}\", params={\"email\": self.email}, timeout=15\n        )\n        if response.ok:\n            payload = response.json()\n            url = (payload.get(\"best_oa_location\") or {}).get(\"url_for_pdf\")\n            if url:\n                return url\n    if self.pdf_host and doi:\n        return f\"{self.pdf_host}/pdf/{doi.replace('/', '_')}.pdf\"\n    return None\n</code></pre>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.run","title":"<code>run(topic, years, max_works)</code>","text":"<p>Compute run.</p> <p>Carry out the run operation.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.run--parameters","title":"Parameters","text":"<p>topic : str     Description for <code>topic</code>. years : str     Description for <code>years</code>. max_works : int     Description for <code>max_works</code>.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.run--returns","title":"Returns","text":"<p>List[src.kgfoundry_common.models.Doc]     Description of return value.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.run--examples","title":"Examples","text":"<p>from download.harvester import run result = run(..., ..., ...) result  # doctest: +ELLIPSIS</p> Source code in <code>src/download/harvester.py</code> <pre><code>def run(self, topic: str, years: str, max_works: int) -&gt; list[Doc]:\n    \"\"\"Compute run.\n\n    Carry out the run operation.\n\n    Parameters\n    ----------\n    topic : str\n        Description for ``topic``.\n    years : str\n        Description for ``years``.\n    max_works : int\n        Description for ``max_works``.\n\n    Returns\n    -------\n    List[src.kgfoundry_common.models.Doc]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from download.harvester import run\n    &gt;&gt;&gt; result = run(..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    \"\"\"\n    docs: list[Doc] = []\n    works = self.search(topic, years, max_works)\n    for work in works:\n        pdf_url = self.resolve_pdf(work)\n        if not pdf_url:\n            continue\n        filename = (work.get(\"doi\") or work.get(\"id\") or str(int(time.time() * 1000))).replace(\n            \"/\", \"_\"\n        ) + \".pdf\"\n        destination = os.path.join(self.out_dir, filename)\n        self.download_pdf(pdf_url, destination)\n        doc = Doc(\n            id=f\"urn:doc:source:openalex:{work.get('id', 'unknown')}\",\n            openalex_id=work.get(\"id\"),\n            doi=work.get(\"doi\"),\n            title=work.get(\"title\", \"\"),\n            authors=[],\n            pub_date=None,\n            license=None,\n            language=\"en\",\n            pdf_uri=destination,\n            source=\"openalex\",\n            content_hash=None,\n        )\n        docs.append(doc)\n    return docs\n</code></pre>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.search","title":"<code>search(topic, years, max_works)</code>","text":"<p>Compute search.</p> <p>Carry out the search operation.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.search--parameters","title":"Parameters","text":"<p>topic : str     Description for <code>topic</code>. years : str     Description for <code>years</code>. max_works : int     Description for <code>max_works</code>.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.search--returns","title":"Returns","text":"<p>List[dict[str, typing.Any]]     Description of return value.</p>"},{"location":"api/download/harvester/#download.harvester.OpenAccessHarvester.search--examples","title":"Examples","text":"<p>from download.harvester import search result = search(..., ..., ...) result  # doctest: +ELLIPSIS</p> Source code in <code>src/download/harvester.py</code> <pre><code>def search(self, topic: str, years: str, max_works: int) -&gt; list[dict[str, Any]]:\n    \"\"\"Compute search.\n\n    Carry out the search operation.\n\n    Parameters\n    ----------\n    topic : str\n        Description for ``topic``.\n    years : str\n        Description for ``years``.\n    max_works : int\n        Description for ``max_works``.\n\n    Returns\n    -------\n    List[dict[str, typing.Any]]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from download.harvester import search\n    &gt;&gt;&gt; result = search(..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    \"\"\"\n    url = f\"{self.openalex}/works\"\n    params: dict[str, str | int] = {\n        \"topic\": topic,\n        \"per_page\": min(200, max_works),\n        \"cursor\": \"*\",\n    }\n    if years:\n        params[\"filter\"] = years\n    response = self.session.get(url, params=params, timeout=30)\n    response.raise_for_status()\n    data = response.json()\n    return data.get(\"results\", [])[:max_works]\n</code></pre>"},{"location":"api/embeddings_dense/","title":"<code>embeddings_dense</code>","text":"<p>Embeddings Dense utilities.</p>"},{"location":"api/embeddings_dense/base/","title":"<code>embeddings_dense.base</code>","text":"<p>Base utilities.</p>"},{"location":"api/embeddings_dense/base/#embeddings_dense.base.DenseEmbeddingModel","title":"<code>DenseEmbeddingModel</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Describe DenseEmbeddingModel.</p> Source code in <code>src/embeddings_dense/base.py</code> <pre><code>class DenseEmbeddingModel(Protocol):\n    \"\"\"Describe DenseEmbeddingModel.\"\"\"\n\n    def encode(self, texts: Sequence[str]) -&gt; NDArray[np.float32]:\n        \"\"\"Compute encode.\n\n        Carry out the encode operation.\n\n        Parameters\n        ----------\n        texts : collections.abc.Sequence\n            Description for ``texts``.\n\n        Returns\n        -------\n        numpy.typing.NDArray\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from embeddings_dense.base import encode\n        &gt;&gt;&gt; result = encode(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/embeddings_dense/base/#embeddings_dense.base.DenseEmbeddingModel.encode","title":"<code>encode(texts)</code>","text":"<p>Compute encode.</p> <p>Carry out the encode operation.</p>"},{"location":"api/embeddings_dense/base/#embeddings_dense.base.DenseEmbeddingModel.encode--parameters","title":"Parameters","text":"<p>texts : collections.abc.Sequence     Description for <code>texts</code>.</p>"},{"location":"api/embeddings_dense/base/#embeddings_dense.base.DenseEmbeddingModel.encode--returns","title":"Returns","text":"<p>numpy.typing.NDArray     Description of return value.</p>"},{"location":"api/embeddings_dense/base/#embeddings_dense.base.DenseEmbeddingModel.encode--examples","title":"Examples","text":"<p>from embeddings_dense.base import encode result = encode(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/embeddings_dense/base.py</code> <pre><code>def encode(self, texts: Sequence[str]) -&gt; NDArray[np.float32]:\n    \"\"\"Compute encode.\n\n    Carry out the encode operation.\n\n    Parameters\n    ----------\n    texts : collections.abc.Sequence\n        Description for ``texts``.\n\n    Returns\n    -------\n    numpy.typing.NDArray\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_dense.base import encode\n    &gt;&gt;&gt; result = encode(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/embeddings_dense/qwen3/","title":"<code>embeddings_dense.qwen3</code>","text":"<p>Qwen3 utilities.</p>"},{"location":"api/embeddings_dense/qwen3/#embeddings_dense.qwen3.Qwen3Embedder","title":"<code>Qwen3Embedder</code>","text":"<p>Describe Qwen3Embedder.</p> Source code in <code>src/embeddings_dense/qwen3.py</code> <pre><code>class Qwen3Embedder:\n    \"\"\"Describe Qwen3Embedder.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/embeddings_sparse/","title":"<code>embeddings_sparse</code>","text":"<p>Embeddings Sparse utilities.</p>"},{"location":"api/embeddings_sparse/base/","title":"<code>embeddings_sparse.base</code>","text":"<p>Base utilities.</p>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseEncoder","title":"<code>SparseEncoder</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Describe SparseEncoder.</p> Source code in <code>src/embeddings_sparse/base.py</code> <pre><code>class SparseEncoder(Protocol):\n    \"\"\"Describe SparseEncoder.\"\"\"\n\n    name: str\n\n    def encode(self, texts: list[str]) -&gt; list[tuple[list[int], list[float]]]:\n        \"\"\"Compute encode.\n\n        Carry out the encode operation.\n\n        Parameters\n        ----------\n        texts : List[str]\n            Description for ``texts``.\n\n        Returns\n        -------\n        List[Tuple[List[int], List[float]]]\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from embeddings_sparse.base import encode\n        &gt;&gt;&gt; result = encode(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseEncoder.encode","title":"<code>encode(texts)</code>","text":"<p>Compute encode.</p> <p>Carry out the encode operation.</p>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseEncoder.encode--parameters","title":"Parameters","text":"<p>texts : List[str]     Description for <code>texts</code>.</p>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseEncoder.encode--returns","title":"Returns","text":"<p>List[Tuple[List[int], List[float]]]     Description of return value.</p>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseEncoder.encode--examples","title":"Examples","text":"<p>from embeddings_sparse.base import encode result = encode(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/embeddings_sparse/base.py</code> <pre><code>def encode(self, texts: list[str]) -&gt; list[tuple[list[int], list[float]]]:\n    \"\"\"Compute encode.\n\n    Carry out the encode operation.\n\n    Parameters\n    ----------\n    texts : List[str]\n        Description for ``texts``.\n\n    Returns\n    -------\n    List[Tuple[List[int], List[float]]]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.base import encode\n    &gt;&gt;&gt; result = encode(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseIndex","title":"<code>SparseIndex</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Describe SparseIndex.</p> Source code in <code>src/embeddings_sparse/base.py</code> <pre><code>class SparseIndex(Protocol):\n    \"\"\"Describe SparseIndex.\"\"\"\n\n    def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n        \"\"\"Compute build.\n\n        Carry out the build operation.\n\n        Parameters\n        ----------\n        docs_iterable : collections.abc.Iterable\n            Description for ``docs_iterable``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from embeddings_sparse.base import build\n        &gt;&gt;&gt; build(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        ...\n\n    def search(\n        self, query: str, k: int, fields: Mapping[str, str] | None = None\n    ) -&gt; list[tuple[str, float]]:\n        \"\"\"Compute search.\n\n        Carry out the search operation.\n\n        Parameters\n        ----------\n        query : str\n            Description for ``query``.\n        k : int\n            Description for ``k``.\n        fields : Mapping[str, str] | None\n            Description for ``fields``.\n\n        Returns\n        -------\n        List[Tuple[str, float]]\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from embeddings_sparse.base import search\n        &gt;&gt;&gt; result = search(..., ..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseIndex.build","title":"<code>build(docs_iterable)</code>","text":"<p>Compute build.</p> <p>Carry out the build operation.</p>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseIndex.build--parameters","title":"Parameters","text":"<p>docs_iterable : collections.abc.Iterable     Description for <code>docs_iterable</code>.</p>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseIndex.build--examples","title":"Examples","text":"<p>from embeddings_sparse.base import build build(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/embeddings_sparse/base.py</code> <pre><code>def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n    \"\"\"Compute build.\n\n    Carry out the build operation.\n\n    Parameters\n    ----------\n    docs_iterable : collections.abc.Iterable\n        Description for ``docs_iterable``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.base import build\n    &gt;&gt;&gt; build(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseIndex.search","title":"<code>search(query, k, fields=None)</code>","text":"<p>Compute search.</p> <p>Carry out the search operation.</p>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseIndex.search--parameters","title":"Parameters","text":"<p>query : str     Description for <code>query</code>. k : int     Description for <code>k</code>. fields : Mapping[str, str] | None     Description for <code>fields</code>.</p>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseIndex.search--returns","title":"Returns","text":"<p>List[Tuple[str, float]]     Description of return value.</p>"},{"location":"api/embeddings_sparse/base/#embeddings_sparse.base.SparseIndex.search--examples","title":"Examples","text":"<p>from embeddings_sparse.base import search result = search(..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/embeddings_sparse/base.py</code> <pre><code>def search(\n    self, query: str, k: int, fields: Mapping[str, str] | None = None\n) -&gt; list[tuple[str, float]]:\n    \"\"\"Compute search.\n\n    Carry out the search operation.\n\n    Parameters\n    ----------\n    query : str\n        Description for ``query``.\n    k : int\n        Description for ``k``.\n    fields : Mapping[str, str] | None\n        Description for ``fields``.\n\n    Returns\n    -------\n    List[Tuple[str, float]]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.base import search\n    &gt;&gt;&gt; result = search(..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/","title":"<code>embeddings_sparse.bm25</code>","text":"<p>Bm25 utilities.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.BM25Doc","title":"<code>BM25Doc</code>  <code>dataclass</code>","text":"<p>Describe BM25Doc.</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>@dataclass\nclass BM25Doc:\n    \"\"\"Describe BM25Doc.\"\"\"\n\n    doc_id: str\n    length: int\n    fields: dict[str, str]\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25","title":"<code>LuceneBM25</code>","text":"<p>Describe LuceneBM25.</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>class LuceneBM25:\n    \"\"\"Describe LuceneBM25.\"\"\"\n\n    def __init__(\n        self,\n        index_dir: str,\n        k1: float = 0.9,\n        b: float = 0.4,\n        field_boosts: dict[str, float] | None = None,\n    ) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        index_dir : str\n            Description for ``index_dir``.\n        k1 : float | None\n            Description for ``k1``.\n        b : float | None\n            Description for ``b``.\n        field_boosts : Mapping[str, float] | None\n            Description for ``field_boosts``.\n        \"\"\"\n\n\n        self.index_dir = index_dir\n        self.k1 = k1\n        self.b = b\n        self.field_boosts = field_boosts or {\"title\": 2.0, \"section\": 1.2, \"body\": 1.0}\n        self._searcher: Any | None = None\n\n    def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n        \"\"\"Compute build.\n\n        Carry out the build operation.\n\n        Parameters\n        ----------\n        docs_iterable : collections.abc.Iterable\n            Description for ``docs_iterable``.\n\n        Raises\n        ------\n        RuntimeError\n            Raised when validation fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from embeddings_sparse.bm25 import build\n        &gt;&gt;&gt; build(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        try:\n            from pyserini.analysis import get_lucene_analyzer\n            from pyserini.index import IndexWriter\n        except Exception as exc:\n            message = \"Pyserini/Lucene not available\"\n            raise RuntimeError(message) from exc\n        os.makedirs(self.index_dir, exist_ok=True)\n        analyzer = get_lucene_analyzer(stemmer=\"english\", stopwords=True)\n        writer = IndexWriter(self.index_dir, analyzer=analyzer, keep_stopwords=False)\n        for doc_id, fields in docs_iterable:\n            # combine fields with boosts in a \"contents\" field for simplicity\n            title = fields.get(\"title\", \"\")\n            section = fields.get(\"section\", \"\")\n            body = fields.get(\"body\", \"\")\n            contents = \" \".join(\n                [\n                    (title + \" \") * int(self.field_boosts.get(\"title\", 1.0)),\n                    (section + \" \") * int(self.field_boosts.get(\"section\", 1.0)),\n                    body,\n                ]\n            )\n            writer.add_document(docid=doc_id, contents=contents)\n        writer.close()\n\n    def _ensure_searcher(self) -&gt; None:\n        \"\"\"Compute ensure searcher.\n\n        Carry out the ensure searcher operation.\n        \"\"\"\n        if self._searcher is not None:\n            return\n        from pyserini.search.lucene import LuceneSearcher\n\n        self._searcher = LuceneSearcher(self.index_dir)\n        self._searcher.set_bm25(self.k1, self.b)\n\n    def search(\n        self, query: str, k: int, fields: dict[str, str] | None = None\n    ) -&gt; list[tuple[str, float]]:\n        \"\"\"Compute search.\n\n        Carry out the search operation.\n\n        Parameters\n        ----------\n        query : str\n            Description for ``query``.\n        k : int\n            Description for ``k``.\n        fields : Mapping[str, str] | None\n            Description for ``fields``.\n\n        Returns\n        -------\n        List[Tuple[str, float]]\n            Description of return value.\n\n        Raises\n        ------\n        RuntimeError\n            Raised when validation fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from embeddings_sparse.bm25 import search\n        &gt;&gt;&gt; result = search(..., ..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        self._ensure_searcher()\n        if self._searcher is None:\n            message = \"Lucene searcher not initialized\"\n            raise RuntimeError(message)\n        hits = self._searcher.search(query, k=k)\n        return [(h.docid, float(h.score)) for h in hits]\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25.__init__","title":"<code>__init__(index_dir, k1=0.9, b=0.4, field_boosts=None)</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25.__init__--parameters","title":"Parameters","text":"<p>index_dir : str     Description for <code>index_dir</code>. k1 : float | None     Description for <code>k1</code>. b : float | None     Description for <code>b</code>. field_boosts : Mapping[str, float] | None     Description for <code>field_boosts</code>.</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def __init__(\n    self,\n    index_dir: str,\n    k1: float = 0.9,\n    b: float = 0.4,\n    field_boosts: dict[str, float] | None = None,\n) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    index_dir : str\n        Description for ``index_dir``.\n    k1 : float | None\n        Description for ``k1``.\n    b : float | None\n        Description for ``b``.\n    field_boosts : Mapping[str, float] | None\n        Description for ``field_boosts``.\n    \"\"\"\n\n\n    self.index_dir = index_dir\n    self.k1 = k1\n    self.b = b\n    self.field_boosts = field_boosts or {\"title\": 2.0, \"section\": 1.2, \"body\": 1.0}\n    self._searcher: Any | None = None\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25.build","title":"<code>build(docs_iterable)</code>","text":"<p>Compute build.</p> <p>Carry out the build operation.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25.build--parameters","title":"Parameters","text":"<p>docs_iterable : collections.abc.Iterable     Description for <code>docs_iterable</code>.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25.build--raises","title":"Raises","text":"<p>RuntimeError     Raised when validation fails.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25.build--examples","title":"Examples","text":"<p>from embeddings_sparse.bm25 import build build(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n    \"\"\"Compute build.\n\n    Carry out the build operation.\n\n    Parameters\n    ----------\n    docs_iterable : collections.abc.Iterable\n        Description for ``docs_iterable``.\n\n    Raises\n    ------\n    RuntimeError\n        Raised when validation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.bm25 import build\n    &gt;&gt;&gt; build(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    try:\n        from pyserini.analysis import get_lucene_analyzer\n        from pyserini.index import IndexWriter\n    except Exception as exc:\n        message = \"Pyserini/Lucene not available\"\n        raise RuntimeError(message) from exc\n    os.makedirs(self.index_dir, exist_ok=True)\n    analyzer = get_lucene_analyzer(stemmer=\"english\", stopwords=True)\n    writer = IndexWriter(self.index_dir, analyzer=analyzer, keep_stopwords=False)\n    for doc_id, fields in docs_iterable:\n        # combine fields with boosts in a \"contents\" field for simplicity\n        title = fields.get(\"title\", \"\")\n        section = fields.get(\"section\", \"\")\n        body = fields.get(\"body\", \"\")\n        contents = \" \".join(\n            [\n                (title + \" \") * int(self.field_boosts.get(\"title\", 1.0)),\n                (section + \" \") * int(self.field_boosts.get(\"section\", 1.0)),\n                body,\n            ]\n        )\n        writer.add_document(docid=doc_id, contents=contents)\n    writer.close()\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25.search","title":"<code>search(query, k, fields=None)</code>","text":"<p>Compute search.</p> <p>Carry out the search operation.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25.search--parameters","title":"Parameters","text":"<p>query : str     Description for <code>query</code>. k : int     Description for <code>k</code>. fields : Mapping[str, str] | None     Description for <code>fields</code>.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25.search--returns","title":"Returns","text":"<p>List[Tuple[str, float]]     Description of return value.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25.search--raises","title":"Raises","text":"<p>RuntimeError     Raised when validation fails.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.LuceneBM25.search--examples","title":"Examples","text":"<p>from embeddings_sparse.bm25 import search result = search(..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def search(\n    self, query: str, k: int, fields: dict[str, str] | None = None\n) -&gt; list[tuple[str, float]]:\n    \"\"\"Compute search.\n\n    Carry out the search operation.\n\n    Parameters\n    ----------\n    query : str\n        Description for ``query``.\n    k : int\n        Description for ``k``.\n    fields : Mapping[str, str] | None\n        Description for ``fields``.\n\n    Returns\n    -------\n    List[Tuple[str, float]]\n        Description of return value.\n\n    Raises\n    ------\n    RuntimeError\n        Raised when validation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.bm25 import search\n    &gt;&gt;&gt; result = search(..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    self._ensure_searcher()\n    if self._searcher is None:\n        message = \"Lucene searcher not initialized\"\n        raise RuntimeError(message)\n    hits = self._searcher.search(query, k=k)\n    return [(h.docid, float(h.score)) for h in hits]\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25","title":"<code>PurePythonBM25</code>","text":"<p>Describe PurePythonBM25.</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>class PurePythonBM25:\n    \"\"\"Describe PurePythonBM25.\"\"\"\n\n    def __init__(\n        self,\n        index_dir: str,\n        k1: float = 0.9,\n        b: float = 0.4,\n        field_boosts: dict[str, float] | None = None,\n    ) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        index_dir : str\n            Description for ``index_dir``.\n        k1 : float | None\n            Description for ``k1``.\n        b : float | None\n            Description for ``b``.\n        field_boosts : Mapping[str, float] | None\n            Description for ``field_boosts``.\n        \"\"\"\n\n\n        self.index_dir = index_dir\n        self.k1 = k1\n        self.b = b\n        self.field_boosts = field_boosts or {\"title\": 2.0, \"section\": 1.2, \"body\": 1.0}\n        self.df: dict[str, int] = {}\n        self.postings: dict[str, dict[str, int]] = {}\n        self.docs: dict[str, BM25Doc] = {}\n        self.N = 0\n        self.avgdl = 0.0\n\n    @staticmethod\n    def _tokenize(text: str) -&gt; list[str]:\n        \"\"\"Compute tokenize.\n\n        Carry out the tokenize operation.\n\n        Parameters\n        ----------\n        text : str\n            Description for ``text``.\n\n        Returns\n        -------\n        List[str]\n            Description of return value.\n        \"\"\"\n        return [t.lower() for t in TOKEN_RE.findall(text)]\n\n    def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n        \"\"\"Compute build.\n\n        Carry out the build operation.\n\n        Parameters\n        ----------\n        docs_iterable : collections.abc.Iterable\n            Description for ``docs_iterable``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from embeddings_sparse.bm25 import build\n        &gt;&gt;&gt; build(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        os.makedirs(self.index_dir, exist_ok=True)\n        df: dict[str, int] = defaultdict(int)\n        postings: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))\n        docs: dict[str, BM25Doc] = {}\n        lengths: list[int] = []\n        for doc_id, fields in docs_iterable:\n            body = fields.get(\"body\", \"\")\n            section = fields.get(\"section\", \"\")\n            title = fields.get(\"title\", \"\")\n            # field boosts applied at scoring time; here we merge for length calc\n            toks = self._tokenize(title + \" \" + section + \" \" + body)\n            lengths.append(len(toks))\n            docs[doc_id] = BM25Doc(\n                doc_id=doc_id,\n                length=len(toks),\n                fields={\"title\": title, \"section\": section, \"body\": body},\n            )\n            seen = set()\n            for tok in toks:\n                postings[tok][doc_id] += 1\n                if tok not in seen:\n                    df[tok] += 1\n                    seen.add(tok)\n        self.N = len(docs)\n        self.avgdl = sum(lengths) / max(1, len(lengths))\n        self.df = dict(df)\n        # convert defaultdicts\n        self.postings = {t: dict(ps) for t, ps in postings.items()}\n        self.docs = docs\n        # persist\n        with open(os.path.join(self.index_dir, \"pure_bm25.pkl\"), \"wb\") as f:\n            pickle.dump(\n                {\n                    \"k1\": self.k1,\n                    \"b\": self.b,\n                    \"field_boosts\": self.field_boosts,\n                    \"df\": self.df,\n                    \"postings\": self.postings,\n                    \"docs\": self.docs,\n                    \"N\": self.N,\n                    \"avgdl\": self.avgdl,\n                },\n                f,\n                protocol=pickle.HIGHEST_PROTOCOL,\n            )\n\n    def load(self) -&gt; None:\n        \"\"\"Compute load.\n\n        Carry out the load operation.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from embeddings_sparse.bm25 import load\n        &gt;&gt;&gt; load()  # doctest: +ELLIPSIS\n        \"\"\"\n        path = os.path.join(self.index_dir, \"pure_bm25.pkl\")\n        with open(path, \"rb\") as f:\n            data = pickle.load(f)\n        self.k1 = data[\"k1\"]\n        self.b = data[\"b\"]\n        self.field_boosts = data[\"field_boosts\"]\n        self.df = data[\"df\"]\n        self.postings = data[\"postings\"]\n        self.docs = data[\"docs\"]\n        self.N = data[\"N\"]\n        self.avgdl = data[\"avgdl\"]\n\n    def _idf(self, term: str) -&gt; float:\n        \"\"\"Compute idf.\n\n        Carry out the idf operation.\n\n        Parameters\n        ----------\n        term : str\n            Description for ``term``.\n\n        Returns\n        -------\n        float\n            Description of return value.\n        \"\"\"\n        n_t = self.df.get(term, 0)\n        if n_t == 0:\n            return 0.0\n        # BM25 idf variant\n        return math.log((self.N - n_t + 0.5) / (n_t + 0.5) + 1.0)\n\n    def search(\n        self, query: str, k: int, fields: Mapping[str, str] | None = None\n    ) -&gt; list[tuple[str, float]]:\n        \"\"\"Compute search.\n\n        Carry out the search operation.\n\n        Parameters\n        ----------\n        query : str\n            Description for ``query``.\n        k : int\n            Description for ``k``.\n        fields : Mapping[str, str] | None\n            Description for ``fields``.\n\n        Returns\n        -------\n        List[Tuple[str, float]]\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from embeddings_sparse.bm25 import search\n        &gt;&gt;&gt; result = search(..., ..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        # naive field weighting at score aggregation (title/section/body contributions)\n        tokens = self._tokenize(query)\n        scores: dict[str, float] = defaultdict(float)\n        for term in tokens:\n            idf = self._idf(term)\n            postings = self.postings.get(term)\n            if not postings:\n                continue\n            for doc_id, tf in postings.items():\n                doc = self.docs[doc_id]\n                dl = doc.length or 1\n                denom = tf + self.k1 * (1 - self.b + self.b * (dl / self.avgdl))\n                contrib = idf * ((tf * (self.k1 + 1)) / (denom))\n                scores[doc_id] += contrib\n        return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.__init__","title":"<code>__init__(index_dir, k1=0.9, b=0.4, field_boosts=None)</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.__init__--parameters","title":"Parameters","text":"<p>index_dir : str     Description for <code>index_dir</code>. k1 : float | None     Description for <code>k1</code>. b : float | None     Description for <code>b</code>. field_boosts : Mapping[str, float] | None     Description for <code>field_boosts</code>.</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def __init__(\n    self,\n    index_dir: str,\n    k1: float = 0.9,\n    b: float = 0.4,\n    field_boosts: dict[str, float] | None = None,\n) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    index_dir : str\n        Description for ``index_dir``.\n    k1 : float | None\n        Description for ``k1``.\n    b : float | None\n        Description for ``b``.\n    field_boosts : Mapping[str, float] | None\n        Description for ``field_boosts``.\n    \"\"\"\n\n\n    self.index_dir = index_dir\n    self.k1 = k1\n    self.b = b\n    self.field_boosts = field_boosts or {\"title\": 2.0, \"section\": 1.2, \"body\": 1.0}\n    self.df: dict[str, int] = {}\n    self.postings: dict[str, dict[str, int]] = {}\n    self.docs: dict[str, BM25Doc] = {}\n    self.N = 0\n    self.avgdl = 0.0\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.build","title":"<code>build(docs_iterable)</code>","text":"<p>Compute build.</p> <p>Carry out the build operation.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.build--parameters","title":"Parameters","text":"<p>docs_iterable : collections.abc.Iterable     Description for <code>docs_iterable</code>.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.build--examples","title":"Examples","text":"<p>from embeddings_sparse.bm25 import build build(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n    \"\"\"Compute build.\n\n    Carry out the build operation.\n\n    Parameters\n    ----------\n    docs_iterable : collections.abc.Iterable\n        Description for ``docs_iterable``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.bm25 import build\n    &gt;&gt;&gt; build(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    os.makedirs(self.index_dir, exist_ok=True)\n    df: dict[str, int] = defaultdict(int)\n    postings: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))\n    docs: dict[str, BM25Doc] = {}\n    lengths: list[int] = []\n    for doc_id, fields in docs_iterable:\n        body = fields.get(\"body\", \"\")\n        section = fields.get(\"section\", \"\")\n        title = fields.get(\"title\", \"\")\n        # field boosts applied at scoring time; here we merge for length calc\n        toks = self._tokenize(title + \" \" + section + \" \" + body)\n        lengths.append(len(toks))\n        docs[doc_id] = BM25Doc(\n            doc_id=doc_id,\n            length=len(toks),\n            fields={\"title\": title, \"section\": section, \"body\": body},\n        )\n        seen = set()\n        for tok in toks:\n            postings[tok][doc_id] += 1\n            if tok not in seen:\n                df[tok] += 1\n                seen.add(tok)\n    self.N = len(docs)\n    self.avgdl = sum(lengths) / max(1, len(lengths))\n    self.df = dict(df)\n    # convert defaultdicts\n    self.postings = {t: dict(ps) for t, ps in postings.items()}\n    self.docs = docs\n    # persist\n    with open(os.path.join(self.index_dir, \"pure_bm25.pkl\"), \"wb\") as f:\n        pickle.dump(\n            {\n                \"k1\": self.k1,\n                \"b\": self.b,\n                \"field_boosts\": self.field_boosts,\n                \"df\": self.df,\n                \"postings\": self.postings,\n                \"docs\": self.docs,\n                \"N\": self.N,\n                \"avgdl\": self.avgdl,\n            },\n            f,\n            protocol=pickle.HIGHEST_PROTOCOL,\n        )\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.load","title":"<code>load()</code>","text":"<p>Compute load.</p> <p>Carry out the load operation.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.load--examples","title":"Examples","text":"<p>from embeddings_sparse.bm25 import load load()  # doctest: +ELLIPSIS</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Compute load.\n\n    Carry out the load operation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.bm25 import load\n    &gt;&gt;&gt; load()  # doctest: +ELLIPSIS\n    \"\"\"\n    path = os.path.join(self.index_dir, \"pure_bm25.pkl\")\n    with open(path, \"rb\") as f:\n        data = pickle.load(f)\n    self.k1 = data[\"k1\"]\n    self.b = data[\"b\"]\n    self.field_boosts = data[\"field_boosts\"]\n    self.df = data[\"df\"]\n    self.postings = data[\"postings\"]\n    self.docs = data[\"docs\"]\n    self.N = data[\"N\"]\n    self.avgdl = data[\"avgdl\"]\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.search","title":"<code>search(query, k, fields=None)</code>","text":"<p>Compute search.</p> <p>Carry out the search operation.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.search--parameters","title":"Parameters","text":"<p>query : str     Description for <code>query</code>. k : int     Description for <code>k</code>. fields : Mapping[str, str] | None     Description for <code>fields</code>.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.search--returns","title":"Returns","text":"<p>List[Tuple[str, float]]     Description of return value.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.PurePythonBM25.search--examples","title":"Examples","text":"<p>from embeddings_sparse.bm25 import search result = search(..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def search(\n    self, query: str, k: int, fields: Mapping[str, str] | None = None\n) -&gt; list[tuple[str, float]]:\n    \"\"\"Compute search.\n\n    Carry out the search operation.\n\n    Parameters\n    ----------\n    query : str\n        Description for ``query``.\n    k : int\n        Description for ``k``.\n    fields : Mapping[str, str] | None\n        Description for ``fields``.\n\n    Returns\n    -------\n    List[Tuple[str, float]]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.bm25 import search\n    &gt;&gt;&gt; result = search(..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    # naive field weighting at score aggregation (title/section/body contributions)\n    tokens = self._tokenize(query)\n    scores: dict[str, float] = defaultdict(float)\n    for term in tokens:\n        idf = self._idf(term)\n        postings = self.postings.get(term)\n        if not postings:\n            continue\n        for doc_id, tf in postings.items():\n            doc = self.docs[doc_id]\n            dl = doc.length or 1\n            denom = tf + self.k1 * (1 - self.b + self.b * (dl / self.avgdl))\n            contrib = idf * ((tf * (self.k1 + 1)) / (denom))\n            scores[doc_id] += contrib\n    return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n</code></pre>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.get_bm25","title":"<code>get_bm25(backend, index_dir, *, k1=0.9, b=0.4, field_boosts=None)</code>","text":"<p>Compute get bm25.</p> <p>Carry out the get bm25 operation.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.get_bm25--parameters","title":"Parameters","text":"<p>backend : str     Description for <code>backend</code>. index_dir : str     Description for <code>index_dir</code>. k1 : float | None     Description for <code>k1</code>. b : float | None     Description for <code>b</code>. field_boosts : Mapping[str, float] | None     Description for <code>field_boosts</code>.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.get_bm25--returns","title":"Returns","text":"<p>PurePythonBM25 | LuceneBM25     Description of return value.</p>"},{"location":"api/embeddings_sparse/bm25/#embeddings_sparse.bm25.get_bm25--examples","title":"Examples","text":"<p>from embeddings_sparse.bm25 import get_bm25 result = get_bm25(..., ..., ..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/embeddings_sparse/bm25.py</code> <pre><code>def get_bm25(\n    backend: str,\n    index_dir: str,\n    *,\n    k1: float = 0.9,\n    b: float = 0.4,\n    field_boosts: dict[str, float] | None = None,\n) -&gt; PurePythonBM25 | LuceneBM25:\n    \"\"\"Compute get bm25.\n\n    Carry out the get bm25 operation.\n\n    Parameters\n    ----------\n    backend : str\n        Description for ``backend``.\n    index_dir : str\n        Description for ``index_dir``.\n    k1 : float | None\n        Description for ``k1``.\n    b : float | None\n        Description for ``b``.\n    field_boosts : Mapping[str, float] | None\n        Description for ``field_boosts``.\n\n    Returns\n    -------\n    PurePythonBM25 | LuceneBM25\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.bm25 import get_bm25\n    &gt;&gt;&gt; result = get_bm25(..., ..., ..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    if backend == \"lucene\":\n        try:\n            return LuceneBM25(index_dir, k1=k1, b=b, field_boosts=field_boosts)\n        except Exception:\n            # allow fallback creation\n            pass\n    return PurePythonBM25(index_dir, k1=k1, b=b, field_boosts=field_boosts)\n</code></pre>"},{"location":"api/embeddings_sparse/splade/","title":"<code>embeddings_sparse.splade</code>","text":"<p>Splade utilities.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.LuceneImpactIndex","title":"<code>LuceneImpactIndex</code>","text":"<p>Describe LuceneImpactIndex.</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>class LuceneImpactIndex:\n    \"\"\"Describe LuceneImpactIndex.\"\"\"\n\n    def __init__(self, index_dir: str) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        index_dir : str\n            Description for ``index_dir``.\n        \"\"\"\n\n\n        self.index_dir = index_dir\n        self._searcher: Any | None = None\n\n    def _ensure(self) -&gt; None:\n        \"\"\"Compute ensure.\n\n        Carry out the ensure operation.\n\n        Raises\n        ------\n        RuntimeError\n            Raised when validation fails.\n        \"\"\"\n        if self._searcher is not None:\n            return\n        try:\n            from pyserini.search.lucene import LuceneImpactSearcher\n        except Exception as exc:  # pragma: no cover - defensive for optional dep\n            message = \"Pyserini not available for SPLADE impact search\"\n            raise RuntimeError(message) from exc\n        self._searcher = LuceneImpactSearcher(self.index_dir)\n\n    def search(self, query: str, k: int) -&gt; list[tuple[str, float]]:\n        \"\"\"Compute search.\n\n        Carry out the search operation.\n\n        Parameters\n        ----------\n        query : str\n            Description for ``query``.\n        k : int\n            Description for ``k``.\n\n        Returns\n        -------\n        List[Tuple[str, float]]\n            Description of return value.\n\n        Raises\n        ------\n        RuntimeError\n            Raised when validation fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from embeddings_sparse.splade import search\n        &gt;&gt;&gt; result = search(..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        self._ensure()\n        if self._searcher is None:\n            message = \"Lucene impact searcher not initialized\"\n            raise RuntimeError(message)\n        hits = self._searcher.search(query, k=k)  # expects SPLADE-encoded string\n        return [(hit.docid, float(hit.score)) for hit in hits]\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.LuceneImpactIndex.__init__","title":"<code>__init__(index_dir)</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.LuceneImpactIndex.__init__--parameters","title":"Parameters","text":"<p>index_dir : str     Description for <code>index_dir</code>.</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def __init__(self, index_dir: str) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    index_dir : str\n        Description for ``index_dir``.\n    \"\"\"\n\n\n    self.index_dir = index_dir\n    self._searcher: Any | None = None\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.LuceneImpactIndex.search","title":"<code>search(query, k)</code>","text":"<p>Compute search.</p> <p>Carry out the search operation.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.LuceneImpactIndex.search--parameters","title":"Parameters","text":"<p>query : str     Description for <code>query</code>. k : int     Description for <code>k</code>.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.LuceneImpactIndex.search--returns","title":"Returns","text":"<p>List[Tuple[str, float]]     Description of return value.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.LuceneImpactIndex.search--raises","title":"Raises","text":"<p>RuntimeError     Raised when validation fails.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.LuceneImpactIndex.search--examples","title":"Examples","text":"<p>from embeddings_sparse.splade import search result = search(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def search(self, query: str, k: int) -&gt; list[tuple[str, float]]:\n    \"\"\"Compute search.\n\n    Carry out the search operation.\n\n    Parameters\n    ----------\n    query : str\n        Description for ``query``.\n    k : int\n        Description for ``k``.\n\n    Returns\n    -------\n    List[Tuple[str, float]]\n        Description of return value.\n\n    Raises\n    ------\n    RuntimeError\n        Raised when validation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.splade import search\n    &gt;&gt;&gt; result = search(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    self._ensure()\n    if self._searcher is None:\n        message = \"Lucene impact searcher not initialized\"\n        raise RuntimeError(message)\n    hits = self._searcher.search(query, k=k)  # expects SPLADE-encoded string\n    return [(hit.docid, float(hit.score)) for hit in hits]\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex","title":"<code>PureImpactIndex</code>","text":"<p>Describe PureImpactIndex.</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>class PureImpactIndex:\n    \"\"\"Describe PureImpactIndex.\"\"\"\n\n    def __init__(self, index_dir: str) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        index_dir : str\n            Description for ``index_dir``.\n        \"\"\"\n\n\n        self.index_dir = index_dir\n        self.df: dict[str, int] = {}\n        self.N = 0\n        self.postings: dict[str, dict[str, float]] = {}\n\n    @staticmethod\n    def _tokenize(text: str) -&gt; list[str]:\n        \"\"\"Compute tokenize.\n\n        Carry out the tokenize operation.\n\n        Parameters\n        ----------\n        text : str\n            Description for ``text``.\n\n        Returns\n        -------\n        List[str]\n            Description of return value.\n        \"\"\"\n        return [token.lower() for token in TOKEN_RE.findall(text)]\n\n    def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n        \"\"\"Compute build.\n\n        Carry out the build operation.\n\n        Parameters\n        ----------\n        docs_iterable : collections.abc.Iterable\n            Description for ``docs_iterable``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from embeddings_sparse.splade import build\n        &gt;&gt;&gt; build(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        os.makedirs(self.index_dir, exist_ok=True)\n        df: dict[str, int] = defaultdict(int)\n        postings: dict[str, dict[str, float]] = defaultdict(lambda: defaultdict(float))\n        doc_count = 0\n        for doc_id, fields in docs_iterable:\n            text = \" \".join(\n                [fields.get(\"title\", \"\"), fields.get(\"section\", \"\"), fields.get(\"body\", \"\")]\n            )\n            tokens = self._tokenize(text)\n            doc_count += 1\n            counts = Counter(tokens)\n            for token, term_freq in counts.items():\n                df[token] += 1\n                postings[token][doc_id] = math.log1p(term_freq)\n        self.N = doc_count\n        self.df = dict(df)\n        self.postings = {\n            token: {\n                doc: weight * math.log((doc_count - df[token] + 0.5) / (df[token] + 0.5) + 1.0)\n                for doc, weight in docs.items()\n            }\n            for token, docs in postings.items()\n        }\n        with open(os.path.join(self.index_dir, \"impact.pkl\"), \"wb\") as handle:\n            pickle.dump(\n                {\"df\": self.df, \"N\": self.N, \"postings\": self.postings},\n                handle,\n                protocol=pickle.HIGHEST_PROTOCOL,\n            )\n\n    def load(self) -&gt; None:\n        \"\"\"Compute load.\n\n        Carry out the load operation.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from embeddings_sparse.splade import load\n        &gt;&gt;&gt; load()  # doctest: +ELLIPSIS\n        \"\"\"\n        with open(os.path.join(self.index_dir, \"impact.pkl\"), \"rb\") as handle:\n            data = pickle.load(handle)\n        self.df = data[\"df\"]\n        self.N = data[\"N\"]\n        self.postings = data[\"postings\"]\n\n    def search(self, query: str, k: int) -&gt; list[tuple[str, float]]:\n        \"\"\"Compute search.\n\n        Carry out the search operation.\n\n        Parameters\n        ----------\n        query : str\n            Description for ``query``.\n        k : int\n            Description for ``k``.\n\n        Returns\n        -------\n        List[Tuple[str, float]]\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from embeddings_sparse.splade import search\n        &gt;&gt;&gt; result = search(..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        tokens = self._tokenize(query)\n        scores: dict[str, float] = defaultdict(float)\n        for token in tokens:\n            postings = self.postings.get(token)\n            if not postings:\n                continue\n            for doc_id, weight in postings.items():\n                scores[doc_id] += weight\n        return sorted(scores.items(), key=lambda item: item[1], reverse=True)[:k]\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.__init__","title":"<code>__init__(index_dir)</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.__init__--parameters","title":"Parameters","text":"<p>index_dir : str     Description for <code>index_dir</code>.</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def __init__(self, index_dir: str) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    index_dir : str\n        Description for ``index_dir``.\n    \"\"\"\n\n\n    self.index_dir = index_dir\n    self.df: dict[str, int] = {}\n    self.N = 0\n    self.postings: dict[str, dict[str, float]] = {}\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.build","title":"<code>build(docs_iterable)</code>","text":"<p>Compute build.</p> <p>Carry out the build operation.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.build--parameters","title":"Parameters","text":"<p>docs_iterable : collections.abc.Iterable     Description for <code>docs_iterable</code>.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.build--examples","title":"Examples","text":"<p>from embeddings_sparse.splade import build build(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def build(self, docs_iterable: Iterable[tuple[str, dict[str, str]]]) -&gt; None:\n    \"\"\"Compute build.\n\n    Carry out the build operation.\n\n    Parameters\n    ----------\n    docs_iterable : collections.abc.Iterable\n        Description for ``docs_iterable``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.splade import build\n    &gt;&gt;&gt; build(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    os.makedirs(self.index_dir, exist_ok=True)\n    df: dict[str, int] = defaultdict(int)\n    postings: dict[str, dict[str, float]] = defaultdict(lambda: defaultdict(float))\n    doc_count = 0\n    for doc_id, fields in docs_iterable:\n        text = \" \".join(\n            [fields.get(\"title\", \"\"), fields.get(\"section\", \"\"), fields.get(\"body\", \"\")]\n        )\n        tokens = self._tokenize(text)\n        doc_count += 1\n        counts = Counter(tokens)\n        for token, term_freq in counts.items():\n            df[token] += 1\n            postings[token][doc_id] = math.log1p(term_freq)\n    self.N = doc_count\n    self.df = dict(df)\n    self.postings = {\n        token: {\n            doc: weight * math.log((doc_count - df[token] + 0.5) / (df[token] + 0.5) + 1.0)\n            for doc, weight in docs.items()\n        }\n        for token, docs in postings.items()\n    }\n    with open(os.path.join(self.index_dir, \"impact.pkl\"), \"wb\") as handle:\n        pickle.dump(\n            {\"df\": self.df, \"N\": self.N, \"postings\": self.postings},\n            handle,\n            protocol=pickle.HIGHEST_PROTOCOL,\n        )\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.load","title":"<code>load()</code>","text":"<p>Compute load.</p> <p>Carry out the load operation.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.load--examples","title":"Examples","text":"<p>from embeddings_sparse.splade import load load()  # doctest: +ELLIPSIS</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Compute load.\n\n    Carry out the load operation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.splade import load\n    &gt;&gt;&gt; load()  # doctest: +ELLIPSIS\n    \"\"\"\n    with open(os.path.join(self.index_dir, \"impact.pkl\"), \"rb\") as handle:\n        data = pickle.load(handle)\n    self.df = data[\"df\"]\n    self.N = data[\"N\"]\n    self.postings = data[\"postings\"]\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.search","title":"<code>search(query, k)</code>","text":"<p>Compute search.</p> <p>Carry out the search operation.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.search--parameters","title":"Parameters","text":"<p>query : str     Description for <code>query</code>. k : int     Description for <code>k</code>.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.search--returns","title":"Returns","text":"<p>List[Tuple[str, float]]     Description of return value.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.PureImpactIndex.search--examples","title":"Examples","text":"<p>from embeddings_sparse.splade import search result = search(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def search(self, query: str, k: int) -&gt; list[tuple[str, float]]:\n    \"\"\"Compute search.\n\n    Carry out the search operation.\n\n    Parameters\n    ----------\n    query : str\n        Description for ``query``.\n    k : int\n        Description for ``k``.\n\n    Returns\n    -------\n    List[Tuple[str, float]]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.splade import search\n    &gt;&gt;&gt; result = search(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    tokens = self._tokenize(query)\n    scores: dict[str, float] = defaultdict(float)\n    for token in tokens:\n        postings = self.postings.get(token)\n        if not postings:\n            continue\n        for doc_id, weight in postings.items():\n            scores[doc_id] += weight\n    return sorted(scores.items(), key=lambda item: item[1], reverse=True)[:k]\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.SPLADEv3Encoder","title":"<code>SPLADEv3Encoder</code>","text":"<p>Describe SPLADEv3Encoder.</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>class SPLADEv3Encoder:\n    \"\"\"Describe SPLADEv3Encoder.\"\"\"\n\n    name = \"SPLADE-v3-distilbert\"\n\n    def __init__(\n        self,\n        model_id: str = \"naver/splade-v3-distilbert\",\n        device: str = \"cuda\",\n        topk: int = 256,\n        max_seq_len: int = 512,\n    ) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        model_id : str | None\n            Description for ``model_id``.\n        device : str | None\n            Description for ``device``.\n        topk : int | None\n            Description for ``topk``.\n        max_seq_len : int | None\n            Description for ``max_seq_len``.\n        \"\"\"\n\n\n        self.model_id = model_id\n        self.device = device\n        self.topk = topk\n        self.max_seq_len = max_seq_len\n\n    def encode(self, texts: list[str]) -&gt; list[tuple[list[int], list[float]]]:\n        \"\"\"Compute encode.\n\n        Carry out the encode operation.\n\n        Parameters\n        ----------\n        texts : List[str]\n            Description for ``texts``.\n\n        Returns\n        -------\n        List[Tuple[List[int], List[float]]]\n            Description of return value.\n\n        Raises\n        ------\n        NotImplementedError\n            Raised when validation fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from embeddings_sparse.splade import encode\n        &gt;&gt;&gt; result = encode(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        message = (\n            \"SPLADE encoding is not implemented in the skeleton. Use the Lucene \"\n            \"impact index variant if available.\"\n        )\n        raise NotImplementedError(message)\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.SPLADEv3Encoder.__init__","title":"<code>__init__(model_id='naver/splade-v3-distilbert', device='cuda', topk=256, max_seq_len=512)</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.SPLADEv3Encoder.__init__--parameters","title":"Parameters","text":"<p>model_id : str | None     Description for <code>model_id</code>. device : str | None     Description for <code>device</code>. topk : int | None     Description for <code>topk</code>. max_seq_len : int | None     Description for <code>max_seq_len</code>.</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def __init__(\n    self,\n    model_id: str = \"naver/splade-v3-distilbert\",\n    device: str = \"cuda\",\n    topk: int = 256,\n    max_seq_len: int = 512,\n) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    model_id : str | None\n        Description for ``model_id``.\n    device : str | None\n        Description for ``device``.\n    topk : int | None\n        Description for ``topk``.\n    max_seq_len : int | None\n        Description for ``max_seq_len``.\n    \"\"\"\n\n\n    self.model_id = model_id\n    self.device = device\n    self.topk = topk\n    self.max_seq_len = max_seq_len\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.SPLADEv3Encoder.encode","title":"<code>encode(texts)</code>","text":"<p>Compute encode.</p> <p>Carry out the encode operation.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.SPLADEv3Encoder.encode--parameters","title":"Parameters","text":"<p>texts : List[str]     Description for <code>texts</code>.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.SPLADEv3Encoder.encode--returns","title":"Returns","text":"<p>List[Tuple[List[int], List[float]]]     Description of return value.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.SPLADEv3Encoder.encode--raises","title":"Raises","text":"<p>NotImplementedError     Raised when validation fails.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.SPLADEv3Encoder.encode--examples","title":"Examples","text":"<p>from embeddings_sparse.splade import encode result = encode(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def encode(self, texts: list[str]) -&gt; list[tuple[list[int], list[float]]]:\n    \"\"\"Compute encode.\n\n    Carry out the encode operation.\n\n    Parameters\n    ----------\n    texts : List[str]\n        Description for ``texts``.\n\n    Returns\n    -------\n    List[Tuple[List[int], List[float]]]\n        Description of return value.\n\n    Raises\n    ------\n    NotImplementedError\n        Raised when validation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.splade import encode\n    &gt;&gt;&gt; result = encode(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    message = (\n        \"SPLADE encoding is not implemented in the skeleton. Use the Lucene \"\n        \"impact index variant if available.\"\n    )\n    raise NotImplementedError(message)\n</code></pre>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.get_splade","title":"<code>get_splade(backend, index_dir)</code>","text":"<p>Compute get splade.</p> <p>Carry out the get splade operation.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.get_splade--parameters","title":"Parameters","text":"<p>backend : str     Description for <code>backend</code>. index_dir : str     Description for <code>index_dir</code>.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.get_splade--returns","title":"Returns","text":"<p>PureImpactIndex | LuceneImpactIndex     Description of return value.</p>"},{"location":"api/embeddings_sparse/splade/#embeddings_sparse.splade.get_splade--examples","title":"Examples","text":"<p>from embeddings_sparse.splade import get_splade result = get_splade(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/embeddings_sparse/splade.py</code> <pre><code>def get_splade(backend: str, index_dir: str) -&gt; PureImpactIndex | LuceneImpactIndex:\n    \"\"\"Compute get splade.\n\n    Carry out the get splade operation.\n\n    Parameters\n    ----------\n    backend : str\n        Description for ``backend``.\n    index_dir : str\n        Description for ``index_dir``.\n\n    Returns\n    -------\n    PureImpactIndex | LuceneImpactIndex\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from embeddings_sparse.splade import get_splade\n    &gt;&gt;&gt; result = get_splade(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    if backend == \"lucene\":\n        try:\n            return LuceneImpactIndex(index_dir)\n        except Exception:  # pragma: no cover - fallback to pure-python path\n            pass\n    return PureImpactIndex(index_dir)\n</code></pre>"},{"location":"api/kf_common/","title":"<code>kf_common</code>","text":"<p>Kgfoundry common exports.</p>"},{"location":"api/kg_builder/","title":"<code>kg_builder</code>","text":"<p>Kg Builder utilities.</p>"},{"location":"api/kg_builder/mock_kg/","title":"<code>kg_builder.mock_kg</code>","text":"<p>Mock Kg utilities.</p>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG","title":"<code>MockKG</code>","text":"<p>Describe MockKG.</p> Source code in <code>src/kg_builder/mock_kg.py</code> <pre><code>class MockKG:\n    \"\"\"Describe MockKG.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n        \"\"\"\n\n\n        self.chunk2concepts: dict[str, set[str]] = {}\n        self.neighbors: dict[str, set[str]] = {}\n\n    def add_mention(self, chunk_id: str, concept_id: str) -&gt; None:\n        \"\"\"Compute add mention.\n\n        Carry out the add mention operation.\n\n        Parameters\n        ----------\n        chunk_id : str\n            Description for ``chunk_id``.\n        concept_id : str\n            Description for ``concept_id``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from kg_builder.mock_kg import add_mention\n        &gt;&gt;&gt; add_mention(..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        self.chunk2concepts.setdefault(chunk_id, set()).add(concept_id)\n\n    def add_edge(self, a: str, b: str) -&gt; None:\n        \"\"\"Compute add edge.\n\n        Carry out the add edge operation.\n\n        Parameters\n        ----------\n        a : str\n            Description for ``a``.\n        b : str\n            Description for ``b``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from kg_builder.mock_kg import add_edge\n        &gt;&gt;&gt; add_edge(..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        self.neighbors.setdefault(a, set()).add(b)\n        self.neighbors.setdefault(b, set()).add(a)\n\n    def linked_concepts(self, chunk_id: str) -&gt; list[str]:\n        \"\"\"Compute linked concepts.\n\n        Carry out the linked concepts operation.\n\n        Parameters\n        ----------\n        chunk_id : str\n            Description for ``chunk_id``.\n\n        Returns\n        -------\n        List[str]\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from kg_builder.mock_kg import linked_concepts\n        &gt;&gt;&gt; result = linked_concepts(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        return sorted(self.chunk2concepts.get(chunk_id, set()))\n\n    def one_hop(self, concept_id: str) -&gt; list[str]:\n        \"\"\"Compute one hop.\n\n        Carry out the one hop operation.\n\n        Parameters\n        ----------\n        concept_id : str\n            Description for ``concept_id``.\n\n        Returns\n        -------\n        List[str]\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from kg_builder.mock_kg import one_hop\n        &gt;&gt;&gt; result = one_hop(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        return sorted(self.neighbors.get(concept_id, set()))\n</code></pre>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.__init__","title":"<code>__init__()</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p> Source code in <code>src/kg_builder/mock_kg.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n    \"\"\"\n\n\n    self.chunk2concepts: dict[str, set[str]] = {}\n    self.neighbors: dict[str, set[str]] = {}\n</code></pre>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.add_edge","title":"<code>add_edge(a, b)</code>","text":"<p>Compute add edge.</p> <p>Carry out the add edge operation.</p>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.add_edge--parameters","title":"Parameters","text":"<p>a : str     Description for <code>a</code>. b : str     Description for <code>b</code>.</p>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.add_edge--examples","title":"Examples","text":"<p>from kg_builder.mock_kg import add_edge add_edge(..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/kg_builder/mock_kg.py</code> <pre><code>def add_edge(self, a: str, b: str) -&gt; None:\n    \"\"\"Compute add edge.\n\n    Carry out the add edge operation.\n\n    Parameters\n    ----------\n    a : str\n        Description for ``a``.\n    b : str\n        Description for ``b``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kg_builder.mock_kg import add_edge\n    &gt;&gt;&gt; add_edge(..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    self.neighbors.setdefault(a, set()).add(b)\n    self.neighbors.setdefault(b, set()).add(a)\n</code></pre>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.add_mention","title":"<code>add_mention(chunk_id, concept_id)</code>","text":"<p>Compute add mention.</p> <p>Carry out the add mention operation.</p>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.add_mention--parameters","title":"Parameters","text":"<p>chunk_id : str     Description for <code>chunk_id</code>. concept_id : str     Description for <code>concept_id</code>.</p>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.add_mention--examples","title":"Examples","text":"<p>from kg_builder.mock_kg import add_mention add_mention(..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/kg_builder/mock_kg.py</code> <pre><code>def add_mention(self, chunk_id: str, concept_id: str) -&gt; None:\n    \"\"\"Compute add mention.\n\n    Carry out the add mention operation.\n\n    Parameters\n    ----------\n    chunk_id : str\n        Description for ``chunk_id``.\n    concept_id : str\n        Description for ``concept_id``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kg_builder.mock_kg import add_mention\n    &gt;&gt;&gt; add_mention(..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    self.chunk2concepts.setdefault(chunk_id, set()).add(concept_id)\n</code></pre>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.linked_concepts","title":"<code>linked_concepts(chunk_id)</code>","text":"<p>Compute linked concepts.</p> <p>Carry out the linked concepts operation.</p>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.linked_concepts--parameters","title":"Parameters","text":"<p>chunk_id : str     Description for <code>chunk_id</code>.</p>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.linked_concepts--returns","title":"Returns","text":"<p>List[str]     Description of return value.</p>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.linked_concepts--examples","title":"Examples","text":"<p>from kg_builder.mock_kg import linked_concepts result = linked_concepts(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/kg_builder/mock_kg.py</code> <pre><code>def linked_concepts(self, chunk_id: str) -&gt; list[str]:\n    \"\"\"Compute linked concepts.\n\n    Carry out the linked concepts operation.\n\n    Parameters\n    ----------\n    chunk_id : str\n        Description for ``chunk_id``.\n\n    Returns\n    -------\n    List[str]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kg_builder.mock_kg import linked_concepts\n    &gt;&gt;&gt; result = linked_concepts(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return sorted(self.chunk2concepts.get(chunk_id, set()))\n</code></pre>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.one_hop","title":"<code>one_hop(concept_id)</code>","text":"<p>Compute one hop.</p> <p>Carry out the one hop operation.</p>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.one_hop--parameters","title":"Parameters","text":"<p>concept_id : str     Description for <code>concept_id</code>.</p>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.one_hop--returns","title":"Returns","text":"<p>List[str]     Description of return value.</p>"},{"location":"api/kg_builder/mock_kg/#kg_builder.mock_kg.MockKG.one_hop--examples","title":"Examples","text":"<p>from kg_builder.mock_kg import one_hop result = one_hop(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/kg_builder/mock_kg.py</code> <pre><code>def one_hop(self, concept_id: str) -&gt; list[str]:\n    \"\"\"Compute one hop.\n\n    Carry out the one hop operation.\n\n    Parameters\n    ----------\n    concept_id : str\n        Description for ``concept_id``.\n\n    Returns\n    -------\n    List[str]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kg_builder.mock_kg import one_hop\n    &gt;&gt;&gt; result = one_hop(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return sorted(self.neighbors.get(concept_id, set()))\n</code></pre>"},{"location":"api/kg_builder/neo4j_store/","title":"<code>kg_builder.neo4j_store</code>","text":"<p>Neo4J Store utilities.</p>"},{"location":"api/kg_builder/neo4j_store/#kg_builder.neo4j_store.Neo4jStore","title":"<code>Neo4jStore</code>","text":"<p>Describe Neo4jStore.</p> Source code in <code>src/kg_builder/neo4j_store.py</code> <pre><code>class Neo4jStore:\n    \"\"\"Describe Neo4jStore.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgfoundry/","title":"<code>kgfoundry</code>","text":"<p>Kgfoundry utilities.</p>"},{"location":"api/kgfoundry/#kgfoundry.__dir__","title":"<code>__dir__()</code>","text":"<p>Compute dir.</p> <p>Carry out the dir operation.</p>"},{"location":"api/kgfoundry/#kgfoundry.__dir__--returns","title":"Returns","text":"<p>List[str]     Description of return value.</p> Source code in <code>src/kgfoundry/__init__.py</code> <pre><code>def __dir__() -&gt; list[str]:\n    \"\"\"Compute dir.\n\n    Carry out the dir operation.\n\n    Returns\n    -------\n    List[str]\n        Description of return value.\n    \"\"\"\n    return sorted(set(__all__))\n</code></pre>"},{"location":"api/kgfoundry/#kgfoundry.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Compute getattr.</p> <p>Carry out the getattr operation.</p>"},{"location":"api/kgfoundry/#kgfoundry.__getattr__--parameters","title":"Parameters","text":"<p>name : str     Description for <code>name</code>.</p>"},{"location":"api/kgfoundry/#kgfoundry.__getattr__--returns","title":"Returns","text":"<p>object     Description of return value.</p>"},{"location":"api/kgfoundry/#kgfoundry.__getattr__--raises","title":"Raises","text":"<p>AttributeError     Raised when validation fails.</p> Source code in <code>src/kgfoundry/__init__.py</code> <pre><code>def __getattr__(name: str) -&gt; object:\n    \"\"\"Compute getattr.\n\n    Carry out the getattr operation.\n\n    Parameters\n    ----------\n    name : str\n        Description for ``name``.\n\n    Returns\n    -------\n    object\n        Description of return value.\n\n    Raises\n    ------\n    AttributeError\n        Raised when validation fails.\n    \"\"\"\n    if name not in _ALIASES:\n        message = f\"module {__name__!r} has no attribute {name!r}\"\n        raise AttributeError(message) from None\n    return _load(name)\n</code></pre>"},{"location":"api/kgfoundry_common/","title":"<code>kgfoundry_common</code>","text":"<p>Kgfoundry Common utilities.</p>"},{"location":"api/kgfoundry_common/config/","title":"<code>kgfoundry_common.config</code>","text":"<p>Config utilities.</p>"},{"location":"api/kgfoundry_common/config/#kgfoundry_common.config.load_config","title":"<code>load_config(path)</code>","text":"<p>Compute load config.</p> <p>Carry out the load config operation.</p>"},{"location":"api/kgfoundry_common/config/#kgfoundry_common.config.load_config--parameters","title":"Parameters","text":"<p>path : str     Description for <code>path</code>.</p>"},{"location":"api/kgfoundry_common/config/#kgfoundry_common.config.load_config--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/kgfoundry_common/config/#kgfoundry_common.config.load_config--examples","title":"Examples","text":"<p>from kgfoundry_common.config import load_config result = load_config(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/kgfoundry_common/config.py</code> <pre><code>def load_config(path: str) -&gt; dict[str, Any]:\n    \"\"\"Compute load config.\n\n    Carry out the load config operation.\n\n    Parameters\n    ----------\n    path : str\n        Description for ``path``.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kgfoundry_common.config import load_config\n    &gt;&gt;&gt; result = load_config(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    with open(path, encoding=\"utf-8\") as f:\n        return yaml.safe_load(f)\n</code></pre>"},{"location":"api/kgfoundry_common/errors/","title":"<code>kgfoundry_common.errors</code>","text":"<p>Errors utilities.</p>"},{"location":"api/kgfoundry_common/errors/#kgfoundry_common.errors.ChunkingError","title":"<code>ChunkingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Describe ChunkingError.</p> Source code in <code>src/kgfoundry_common/errors.py</code> <pre><code>class ChunkingError(Exception):\n    \"\"\"Describe ChunkingError.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgfoundry_common/errors/#kgfoundry_common.errors.DoclingError","title":"<code>DoclingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Describe DoclingError.</p> Source code in <code>src/kgfoundry_common/errors.py</code> <pre><code>class DoclingError(Exception):\n    \"\"\"Describe DoclingError.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgfoundry_common/errors/#kgfoundry_common.errors.DownloadError","title":"<code>DownloadError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Describe DownloadError.</p> Source code in <code>src/kgfoundry_common/errors.py</code> <pre><code>class DownloadError(Exception):\n    \"\"\"Describe DownloadError.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgfoundry_common/errors/#kgfoundry_common.errors.EmbeddingError","title":"<code>EmbeddingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Describe EmbeddingError.</p> Source code in <code>src/kgfoundry_common/errors.py</code> <pre><code>class EmbeddingError(Exception):\n    \"\"\"Describe EmbeddingError.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgfoundry_common/errors/#kgfoundry_common.errors.IndexBuildError","title":"<code>IndexBuildError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Describe IndexBuildError.</p> Source code in <code>src/kgfoundry_common/errors.py</code> <pre><code>class IndexBuildError(Exception):\n    \"\"\"Describe IndexBuildError.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgfoundry_common/errors/#kgfoundry_common.errors.LinkerCalibrationError","title":"<code>LinkerCalibrationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Describe LinkerCalibrationError.</p> Source code in <code>src/kgfoundry_common/errors.py</code> <pre><code>class LinkerCalibrationError(Exception):\n    \"\"\"Describe LinkerCalibrationError.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgfoundry_common/errors/#kgfoundry_common.errors.Neo4jError","title":"<code>Neo4jError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Describe Neo4jError.</p> Source code in <code>src/kgfoundry_common/errors.py</code> <pre><code>class Neo4jError(Exception):\n    \"\"\"Describe Neo4jError.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgfoundry_common/errors/#kgfoundry_common.errors.OCRTimeoutError","title":"<code>OCRTimeoutError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Describe OCRTimeoutError.</p> Source code in <code>src/kgfoundry_common/errors.py</code> <pre><code>class OCRTimeoutError(Exception):\n    \"\"\"Describe OCRTimeoutError.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgfoundry_common/errors/#kgfoundry_common.errors.OntologyParseError","title":"<code>OntologyParseError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Describe OntologyParseError.</p> Source code in <code>src/kgfoundry_common/errors.py</code> <pre><code>class OntologyParseError(Exception):\n    \"\"\"Describe OntologyParseError.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgfoundry_common/errors/#kgfoundry_common.errors.SpladeOOMError","title":"<code>SpladeOOMError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Describe SpladeOOMError.</p> Source code in <code>src/kgfoundry_common/errors.py</code> <pre><code>class SpladeOOMError(Exception):\n    \"\"\"Describe SpladeOOMError.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgfoundry_common/errors/#kgfoundry_common.errors.UnsupportedMIMEError","title":"<code>UnsupportedMIMEError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Describe UnsupportedMIMEError.</p> Source code in <code>src/kgfoundry_common/errors.py</code> <pre><code>class UnsupportedMIMEError(Exception):\n    \"\"\"Describe UnsupportedMIMEError.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/kgfoundry_common/exceptions/","title":"<code>kgfoundry_common.exceptions</code>","text":"<p>Exceptions utilities.</p>"},{"location":"api/kgfoundry_common/ids/","title":"<code>kgfoundry_common.ids</code>","text":"<p>Ids utilities.</p>"},{"location":"api/kgfoundry_common/ids/#kgfoundry_common.ids.urn_chunk","title":"<code>urn_chunk(doc_hash, start, end)</code>","text":"<p>Compute urn chunk.</p> <p>Carry out the urn chunk operation.</p>"},{"location":"api/kgfoundry_common/ids/#kgfoundry_common.ids.urn_chunk--parameters","title":"Parameters","text":"<p>doc_hash : str     Description for <code>doc_hash</code>. start : int     Description for <code>start</code>. end : int     Description for <code>end</code>.</p>"},{"location":"api/kgfoundry_common/ids/#kgfoundry_common.ids.urn_chunk--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/kgfoundry_common/ids/#kgfoundry_common.ids.urn_chunk--examples","title":"Examples","text":"<p>from kgfoundry_common.ids import urn_chunk result = urn_chunk(..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/kgfoundry_common/ids.py</code> <pre><code>def urn_chunk(doc_hash: str, start: int, end: int) -&gt; str:\n    \"\"\"Compute urn chunk.\n\n    Carry out the urn chunk operation.\n\n    Parameters\n    ----------\n    doc_hash : str\n        Description for ``doc_hash``.\n    start : int\n        Description for ``start``.\n    end : int\n        Description for ``end``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kgfoundry_common.ids import urn_chunk\n    &gt;&gt;&gt; result = urn_chunk(..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return f\"urn:chunk:{doc_hash.split(':')[-1]}:{start}-{end}\"\n</code></pre>"},{"location":"api/kgfoundry_common/ids/#kgfoundry_common.ids.urn_doc_from_text","title":"<code>urn_doc_from_text(text)</code>","text":"<p>Compute urn doc from text.</p> <p>Carry out the urn doc from text operation.</p>"},{"location":"api/kgfoundry_common/ids/#kgfoundry_common.ids.urn_doc_from_text--parameters","title":"Parameters","text":"<p>text : str     Description for <code>text</code>.</p>"},{"location":"api/kgfoundry_common/ids/#kgfoundry_common.ids.urn_doc_from_text--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/kgfoundry_common/ids/#kgfoundry_common.ids.urn_doc_from_text--examples","title":"Examples","text":"<p>from kgfoundry_common.ids import urn_doc_from_text result = urn_doc_from_text(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/kgfoundry_common/ids.py</code> <pre><code>def urn_doc_from_text(text: str) -&gt; str:\n    \"\"\"Compute urn doc from text.\n\n    Carry out the urn doc from text operation.\n\n    Parameters\n    ----------\n    text : str\n        Description for ``text``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kgfoundry_common.ids import urn_doc_from_text\n    &gt;&gt;&gt; result = urn_doc_from_text(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    h = hashlib.sha256(text.encode(\"utf-8\")).digest()[:16]\n    b32 = base64.b32encode(h).decode(\"ascii\").strip(\"=\").lower()\n    return f\"urn:doc:sha256:{b32}\"\n</code></pre>"},{"location":"api/kgfoundry_common/logging/","title":"<code>kgfoundry_common.logging</code>","text":"<p>Logging utilities.</p>"},{"location":"api/kgfoundry_common/logging/#kgfoundry_common.logging.JsonFormatter","title":"<code>JsonFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>Describe JsonFormatter.</p> Source code in <code>src/kgfoundry_common/logging.py</code> <pre><code>class JsonFormatter(logging.Formatter):\n    \"\"\"Describe JsonFormatter.\"\"\"\n\n    def format(self, record: logging.LogRecord) -&gt; str:\n        \"\"\"Compute format.\n\n        Carry out the format operation.\n\n        Parameters\n        ----------\n        record : logging.LogRecord\n            Description for ``record``.\n\n        Returns\n        -------\n        str\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from kgfoundry_common.logging import format\n        &gt;&gt;&gt; result = format(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        data = {\n            \"ts\": self.formatTime(record, \"%Y-%m-%dT%H:%M:%S\"),\n            \"level\": record.levelname,\n            \"name\": record.name,\n            \"message\": record.getMessage(),\n        }\n        for k in (\"run_id\", \"doc_id\", \"chunk_id\"):\n            v = getattr(record, k, None)\n            if v:\n                data[k] = v\n        return json.dumps(data)\n</code></pre>"},{"location":"api/kgfoundry_common/logging/#kgfoundry_common.logging.JsonFormatter.format","title":"<code>format(record)</code>","text":"<p>Compute format.</p> <p>Carry out the format operation.</p>"},{"location":"api/kgfoundry_common/logging/#kgfoundry_common.logging.JsonFormatter.format--parameters","title":"Parameters","text":"<p>record : logging.LogRecord     Description for <code>record</code>.</p>"},{"location":"api/kgfoundry_common/logging/#kgfoundry_common.logging.JsonFormatter.format--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/kgfoundry_common/logging/#kgfoundry_common.logging.JsonFormatter.format--examples","title":"Examples","text":"<p>from kgfoundry_common.logging import format result = format(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/kgfoundry_common/logging.py</code> <pre><code>def format(self, record: logging.LogRecord) -&gt; str:\n    \"\"\"Compute format.\n\n    Carry out the format operation.\n\n    Parameters\n    ----------\n    record : logging.LogRecord\n        Description for ``record``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kgfoundry_common.logging import format\n    &gt;&gt;&gt; result = format(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    data = {\n        \"ts\": self.formatTime(record, \"%Y-%m-%dT%H:%M:%S\"),\n        \"level\": record.levelname,\n        \"name\": record.name,\n        \"message\": record.getMessage(),\n    }\n    for k in (\"run_id\", \"doc_id\", \"chunk_id\"):\n        v = getattr(record, k, None)\n        if v:\n            data[k] = v\n    return json.dumps(data)\n</code></pre>"},{"location":"api/kgfoundry_common/logging/#kgfoundry_common.logging.setup_logging","title":"<code>setup_logging(level=logging.INFO)</code>","text":"<p>Compute setup logging.</p> <p>Carry out the setup logging operation.</p>"},{"location":"api/kgfoundry_common/logging/#kgfoundry_common.logging.setup_logging--parameters","title":"Parameters","text":"<p>level : int | None     Description for <code>level</code>.</p>"},{"location":"api/kgfoundry_common/logging/#kgfoundry_common.logging.setup_logging--examples","title":"Examples","text":"<p>from kgfoundry_common.logging import setup_logging setup_logging(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/kgfoundry_common/logging.py</code> <pre><code>def setup_logging(level: int = logging.INFO) -&gt; None:\n    \"\"\"Compute setup logging.\n\n    Carry out the setup logging operation.\n\n    Parameters\n    ----------\n    level : int | None\n        Description for ``level``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kgfoundry_common.logging import setup_logging\n    &gt;&gt;&gt; setup_logging(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setFormatter(JsonFormatter())\n    logging.basicConfig(level=level, handlers=[handler])\n</code></pre>"},{"location":"api/kgfoundry_common/models/","title":"<code>kgfoundry_common.models</code>","text":"<p>Models utilities.</p>"},{"location":"api/kgfoundry_common/models/#kgfoundry_common.models.Chunk","title":"<code>Chunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Describe Chunk.</p> Source code in <code>src/kgfoundry_common/models.py</code> <pre><code>class Chunk(BaseModel):\n    \"\"\"Describe Chunk.\"\"\"\n\n    id: Id\n    doc_id: Id\n    section: str | None\n    start_char: int\n    end_char: int\n    tokens: int\n    doctags_span: dict[str, int]\n</code></pre>"},{"location":"api/kgfoundry_common/models/#kgfoundry_common.models.Doc","title":"<code>Doc</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Describe Doc.</p> Source code in <code>src/kgfoundry_common/models.py</code> <pre><code>class Doc(BaseModel):\n    \"\"\"Describe Doc.\"\"\"\n\n    id: Id\n    openalex_id: str | None = None\n    doi: str | None = None\n    arxiv_id: str | None = None\n    pmcid: str | None = None\n    title: str = \"\"\n    authors: list[str] = []\n    pub_date: str | None = None\n    license: str | None = None\n    language: str | None = \"en\"\n    pdf_uri: str = \"\"\n    source: str = \"unknown\"\n    content_hash: str | None = None\n</code></pre>"},{"location":"api/kgfoundry_common/models/#kgfoundry_common.models.DoctagsAsset","title":"<code>DoctagsAsset</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Describe DoctagsAsset.</p> Source code in <code>src/kgfoundry_common/models.py</code> <pre><code>class DoctagsAsset(BaseModel):\n    \"\"\"Describe DoctagsAsset.\"\"\"\n\n    doc_id: Id\n    doctags_uri: str\n    pages: int\n    vlm_model: str\n    vlm_revision: str\n    avg_logprob: float | None = None\n</code></pre>"},{"location":"api/kgfoundry_common/models/#kgfoundry_common.models.LinkAssertion","title":"<code>LinkAssertion</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Describe LinkAssertion.</p> Source code in <code>src/kgfoundry_common/models.py</code> <pre><code>class LinkAssertion(BaseModel):\n    \"\"\"Describe LinkAssertion.\"\"\"\n\n    id: Id\n    chunk_id: Id\n    concept_id: Id\n    score: float\n    decision: Literal[\"link\", \"reject\", \"uncertain\"]\n    evidence_span: str | None = None\n    features: dict[str, float] = {}\n    run_id: str\n</code></pre>"},{"location":"api/kgfoundry_common/navmap_types/","title":"<code>kgfoundry_common.navmap_types</code>","text":"<p>Navmap Types utilities.</p>"},{"location":"api/kgfoundry_common/navmap_types/#kgfoundry_common.navmap_types.NavMap","title":"<code>NavMap</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Describe NavMap.</p> Source code in <code>src/kgfoundry_common/navmap_types.py</code> <pre><code>class NavMap(TypedDict, total=False):\n    \"\"\"Describe NavMap.\"\"\"\n\n    title: str\n    synopsis: str\n    exports: list[str]\n    sections: list[NavSection]\n    see_also: list[str]\n    tags: list[str]\n    since: str\n    deprecated: str\n    symbols: dict[str, SymbolMeta]\n    edit_scopes: dict[str, list[str]]\n    deps: list[str]\n</code></pre>"},{"location":"api/kgfoundry_common/navmap_types/#kgfoundry_common.navmap_types.NavSection","title":"<code>NavSection</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Describe NavSection.</p> Source code in <code>src/kgfoundry_common/navmap_types.py</code> <pre><code>class NavSection(TypedDict):\n    \"\"\"Describe NavSection.\"\"\"\n\n    id: str\n    title: str\n    symbols: list[str]\n</code></pre>"},{"location":"api/kgfoundry_common/navmap_types/#kgfoundry_common.navmap_types.SymbolMeta","title":"<code>SymbolMeta</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Describe SymbolMeta.</p> Source code in <code>src/kgfoundry_common/navmap_types.py</code> <pre><code>class SymbolMeta(TypedDict, total=False):\n    \"\"\"Describe SymbolMeta.\"\"\"\n\n    since: str\n    stability: Literal[\"frozen\", \"stable\", \"experimental\", \"internal\"]\n    side_effects: list[Literal[\"none\", \"fs\", \"net\", \"gpu\", \"db\"]]\n    thread_safety: Literal[\"reentrant\", \"threadsafe\", \"not-threadsafe\"]\n    async_ok: bool\n    perf_budget_ms: float\n    tests: list[str]\n    replaced_by: NotRequired[str]\n    deprecated_msg: NotRequired[str]\n    contracts: NotRequired[list[str]]\n    coverage_target: NotRequired[float]\n</code></pre>"},{"location":"api/kgfoundry_common/parquet_io/","title":"<code>kgfoundry_common.parquet_io</code>","text":"<p>Parquet Io utilities.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetChunkWriter","title":"<code>ParquetChunkWriter</code>","text":"<p>Describe ParquetChunkWriter.</p> Source code in <code>src/kgfoundry_common/parquet_io.py</code> <pre><code>class ParquetChunkWriter:\n    \"\"\"Describe ParquetChunkWriter.\"\"\"\n\n    @staticmethod\n    def chunk_schema() -&gt; pa.schema:\n        \"\"\"Compute chunk schema.\n\n        Carry out the chunk schema operation.\n\n        Returns\n        -------\n        pa.schema\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from kgfoundry_common.parquet_io import chunk_schema\n        &gt;&gt;&gt; result = chunk_schema()\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        return pa.schema(\n            [\n                pa.field(\"chunk_id\", pa.string()),\n                pa.field(\"doc_id\", pa.string()),\n                pa.field(\"section\", pa.string()),\n                pa.field(\"start_char\", pa.int32()),\n                pa.field(\"end_char\", pa.int32()),\n                pa.field(\n                    \"doctags_span\",\n                    pa.struct(\n                        [\n                            pa.field(\"node_id\", pa.string()),\n                            pa.field(\"start\", pa.int32()),\n                            pa.field(\"end\", pa.int32()),\n                        ]\n                    ).with_nullable(True),\n                ),\n                pa.field(\"text\", pa.string()),\n                pa.field(\"tokens\", pa.int32()),\n                pa.field(\"created_at\", pa.timestamp(\"ms\")),\n            ]\n        )\n\n    def __init__(self, root: str, model: str = \"docling_hybrid\", run_id: str = \"dev\") -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        root : str\n            Description for ``root``.\n        model : str | None\n            Description for ``model``.\n        run_id : str | None\n            Description for ``run_id``.\n        \"\"\"\n\n\n        self.root = Path(root) / f\"model={model}\" / f\"run_id={run_id}\" / \"shard=00000\"\n        self.root.mkdir(parents=True, exist_ok=True)\n\n    def write(self, rows: Iterable[dict[str, Any]]) -&gt; str:\n        \"\"\"Compute write.\n\n        Carry out the write operation.\n\n        Parameters\n        ----------\n        rows : collections.abc.Iterable\n            Description for ``rows``.\n\n        Returns\n        -------\n        str\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from kgfoundry_common.parquet_io import write\n        &gt;&gt;&gt; result = write(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        table = pa.Table.from_pylist(list(rows), schema=self.chunk_schema())\n        pq.write_table(\n            table,\n            self.root / \"part-00000.parquet\",\n            compression=\"ZSTD\",\n            compression_level=ZSTD_LEVEL,\n            data_page_size=ROW_GROUP_SIZE,\n        )\n        return str(self.root.parent.parent)\n</code></pre>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetChunkWriter.__init__","title":"<code>__init__(root, model='docling_hybrid', run_id='dev')</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetChunkWriter.__init__--parameters","title":"Parameters","text":"<p>root : str     Description for <code>root</code>. model : str | None     Description for <code>model</code>. run_id : str | None     Description for <code>run_id</code>.</p> Source code in <code>src/kgfoundry_common/parquet_io.py</code> <pre><code>def __init__(self, root: str, model: str = \"docling_hybrid\", run_id: str = \"dev\") -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    root : str\n        Description for ``root``.\n    model : str | None\n        Description for ``model``.\n    run_id : str | None\n        Description for ``run_id``.\n    \"\"\"\n\n\n    self.root = Path(root) / f\"model={model}\" / f\"run_id={run_id}\" / \"shard=00000\"\n    self.root.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetChunkWriter.chunk_schema","title":"<code>chunk_schema()</code>  <code>staticmethod</code>","text":"<p>Compute chunk schema.</p> <p>Carry out the chunk schema operation.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetChunkWriter.chunk_schema--returns","title":"Returns","text":"<p>pa.schema     Description of return value.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetChunkWriter.chunk_schema--examples","title":"Examples","text":"<p>from kgfoundry_common.parquet_io import chunk_schema result = chunk_schema() result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/kgfoundry_common/parquet_io.py</code> <pre><code>@staticmethod\ndef chunk_schema() -&gt; pa.schema:\n    \"\"\"Compute chunk schema.\n\n    Carry out the chunk schema operation.\n\n    Returns\n    -------\n    pa.schema\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kgfoundry_common.parquet_io import chunk_schema\n    &gt;&gt;&gt; result = chunk_schema()\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return pa.schema(\n        [\n            pa.field(\"chunk_id\", pa.string()),\n            pa.field(\"doc_id\", pa.string()),\n            pa.field(\"section\", pa.string()),\n            pa.field(\"start_char\", pa.int32()),\n            pa.field(\"end_char\", pa.int32()),\n            pa.field(\n                \"doctags_span\",\n                pa.struct(\n                    [\n                        pa.field(\"node_id\", pa.string()),\n                        pa.field(\"start\", pa.int32()),\n                        pa.field(\"end\", pa.int32()),\n                    ]\n                ).with_nullable(True),\n            ),\n            pa.field(\"text\", pa.string()),\n            pa.field(\"tokens\", pa.int32()),\n            pa.field(\"created_at\", pa.timestamp(\"ms\")),\n        ]\n    )\n</code></pre>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetChunkWriter.write","title":"<code>write(rows)</code>","text":"<p>Compute write.</p> <p>Carry out the write operation.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetChunkWriter.write--parameters","title":"Parameters","text":"<p>rows : collections.abc.Iterable     Description for <code>rows</code>.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetChunkWriter.write--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetChunkWriter.write--examples","title":"Examples","text":"<p>from kgfoundry_common.parquet_io import write result = write(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/kgfoundry_common/parquet_io.py</code> <pre><code>def write(self, rows: Iterable[dict[str, Any]]) -&gt; str:\n    \"\"\"Compute write.\n\n    Carry out the write operation.\n\n    Parameters\n    ----------\n    rows : collections.abc.Iterable\n        Description for ``rows``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kgfoundry_common.parquet_io import write\n    &gt;&gt;&gt; result = write(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    table = pa.Table.from_pylist(list(rows), schema=self.chunk_schema())\n    pq.write_table(\n        table,\n        self.root / \"part-00000.parquet\",\n        compression=\"ZSTD\",\n        compression_level=ZSTD_LEVEL,\n        data_page_size=ROW_GROUP_SIZE,\n    )\n    return str(self.root.parent.parent)\n</code></pre>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter","title":"<code>ParquetVectorWriter</code>","text":"<p>Describe ParquetVectorWriter.</p> Source code in <code>src/kgfoundry_common/parquet_io.py</code> <pre><code>class ParquetVectorWriter:\n    \"\"\"Describe ParquetVectorWriter.\"\"\"\n\n    @staticmethod\n    def dense_schema(dim: int) -&gt; pa.schema:\n        \"\"\"Compute dense schema.\n\n        Carry out the dense schema operation.\n\n        Parameters\n        ----------\n        dim : int\n            Description for ``dim``.\n\n        Returns\n        -------\n        pa.schema\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from kgfoundry_common.parquet_io import dense_schema\n        &gt;&gt;&gt; result = dense_schema(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        return pa.schema(\n            [\n                pa.field(\"chunk_id\", pa.string()),\n                pa.field(\"model\", pa.string()),\n                pa.field(\"run_id\", pa.string()),\n                pa.field(\"dim\", pa.int16()),\n                pa.field(\"vector\", pa.list_(pa.float32(), list_size=dim)),\n                pa.field(\"l2_norm\", pa.float32()),\n                pa.field(\"created_at\", pa.timestamp(\"ms\")),\n            ]\n        )\n\n    def __init__(self, root: str) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        root : str\n            Description for ``root``.\n        \"\"\"\n\n\n        self.root = Path(root)\n\n    def write_dense(\n        self,\n        model: str,\n        run_id: str,\n        dim: int,\n        records: Iterable[tuple[str, list[float], float]],\n        shard: int = 0,\n    ) -&gt; str:\n        \"\"\"Compute write dense.\n\n        Carry out the write dense operation.\n\n        Parameters\n        ----------\n        model : str\n            Description for ``model``.\n        run_id : str\n            Description for ``run_id``.\n        dim : int\n            Description for ``dim``.\n        records : collections.abc.Iterable\n            Description for ``records``.\n        shard : int | None\n            Description for ``shard``.\n\n        Returns\n        -------\n        str\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from kgfoundry_common.parquet_io import write_dense\n        &gt;&gt;&gt; result = write_dense(..., ..., ..., ..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        part_dir = self.root / f\"model={model}\" / f\"run_id={run_id}\" / f\"shard={shard:05d}\"\n        part_dir.mkdir(parents=True, exist_ok=True)\n        now = int(dt.datetime.now(dt.UTC).timestamp() * 1000)\n        rows = [\n            {\n                \"chunk_id\": cid,\n                \"model\": model,\n                \"run_id\": run_id,\n                \"dim\": dim,\n                \"vector\": vec,\n                \"l2_norm\": float(l2),\n                \"created_at\": now,\n            }\n            for cid, vec, l2 in records\n        ]\n        table = pa.Table.from_pylist(rows, schema=self.dense_schema(dim))\n        pq.write_table(\n            table,\n            part_dir / \"part-00000.parquet\",\n            compression=\"ZSTD\",\n            compression_level=ZSTD_LEVEL,\n            data_page_size=ROW_GROUP_SIZE,\n        )\n        return str(self.root)\n\n    @staticmethod\n    def splade_schema() -&gt; pa.schema:\n        \"\"\"Compute splade schema.\n\n        Carry out the splade schema operation.\n\n        Returns\n        -------\n        pa.schema\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from kgfoundry_common.parquet_io import splade_schema\n        &gt;&gt;&gt; result = splade_schema()\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        return pa.schema(\n            [\n                pa.field(\"chunk_id\", pa.string()),\n                pa.field(\"model\", pa.string()),\n                pa.field(\"run_id\", pa.string()),\n                pa.field(\"vocab_ids\", pa.list_(pa.int32())),\n                pa.field(\"weights\", pa.list_(pa.float32())),\n                pa.field(\"nnz\", pa.int16()),\n                pa.field(\"created_at\", pa.timestamp(\"ms\")),\n            ]\n        )\n\n    def write_splade(\n        self,\n        model: str,\n        run_id: str,\n        records: Iterable[tuple[str, list[int], list[float]]],\n        shard: int = 0,\n    ) -&gt; str:\n        \"\"\"Compute write splade.\n\n        Carry out the write splade operation.\n\n        Parameters\n        ----------\n        model : str\n            Description for ``model``.\n        run_id : str\n            Description for ``run_id``.\n        records : collections.abc.Iterable\n            Description for ``records``.\n        shard : int | None\n            Description for ``shard``.\n\n        Returns\n        -------\n        str\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from kgfoundry_common.parquet_io import write_splade\n        &gt;&gt;&gt; result = write_splade(..., ..., ..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        part_dir = self.root / f\"model={model}\" / f\"run_id={run_id}\" / f\"shard={shard:05d}\"\n        part_dir.mkdir(parents=True, exist_ok=True)\n        now = int(dt.datetime.now(dt.UTC).timestamp() * 1000)\n        rows = [\n            {\n                \"chunk_id\": cid,\n                \"model\": model,\n                \"run_id\": run_id,\n                \"vocab_ids\": ids,\n                \"weights\": wts,\n                \"nnz\": len(ids),\n                \"created_at\": now,\n            }\n            for cid, ids, wts in records\n        ]\n        table = pa.Table.from_pylist(rows, schema=self.splade_schema())\n        pq.write_table(\n            table,\n            part_dir / \"part-00000.parquet\",\n            compression=\"ZSTD\",\n            compression_level=ZSTD_LEVEL,\n            data_page_size=ROW_GROUP_SIZE,\n        )\n        return str(self.root)\n</code></pre>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.__init__","title":"<code>__init__(root)</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.__init__--parameters","title":"Parameters","text":"<p>root : str     Description for <code>root</code>.</p> Source code in <code>src/kgfoundry_common/parquet_io.py</code> <pre><code>def __init__(self, root: str) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    root : str\n        Description for ``root``.\n    \"\"\"\n\n\n    self.root = Path(root)\n</code></pre>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.dense_schema","title":"<code>dense_schema(dim)</code>  <code>staticmethod</code>","text":"<p>Compute dense schema.</p> <p>Carry out the dense schema operation.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.dense_schema--parameters","title":"Parameters","text":"<p>dim : int     Description for <code>dim</code>.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.dense_schema--returns","title":"Returns","text":"<p>pa.schema     Description of return value.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.dense_schema--examples","title":"Examples","text":"<p>from kgfoundry_common.parquet_io import dense_schema result = dense_schema(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/kgfoundry_common/parquet_io.py</code> <pre><code>@staticmethod\ndef dense_schema(dim: int) -&gt; pa.schema:\n    \"\"\"Compute dense schema.\n\n    Carry out the dense schema operation.\n\n    Parameters\n    ----------\n    dim : int\n        Description for ``dim``.\n\n    Returns\n    -------\n    pa.schema\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kgfoundry_common.parquet_io import dense_schema\n    &gt;&gt;&gt; result = dense_schema(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return pa.schema(\n        [\n            pa.field(\"chunk_id\", pa.string()),\n            pa.field(\"model\", pa.string()),\n            pa.field(\"run_id\", pa.string()),\n            pa.field(\"dim\", pa.int16()),\n            pa.field(\"vector\", pa.list_(pa.float32(), list_size=dim)),\n            pa.field(\"l2_norm\", pa.float32()),\n            pa.field(\"created_at\", pa.timestamp(\"ms\")),\n        ]\n    )\n</code></pre>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.splade_schema","title":"<code>splade_schema()</code>  <code>staticmethod</code>","text":"<p>Compute splade schema.</p> <p>Carry out the splade schema operation.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.splade_schema--returns","title":"Returns","text":"<p>pa.schema     Description of return value.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.splade_schema--examples","title":"Examples","text":"<p>from kgfoundry_common.parquet_io import splade_schema result = splade_schema() result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/kgfoundry_common/parquet_io.py</code> <pre><code>@staticmethod\ndef splade_schema() -&gt; pa.schema:\n    \"\"\"Compute splade schema.\n\n    Carry out the splade schema operation.\n\n    Returns\n    -------\n    pa.schema\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kgfoundry_common.parquet_io import splade_schema\n    &gt;&gt;&gt; result = splade_schema()\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return pa.schema(\n        [\n            pa.field(\"chunk_id\", pa.string()),\n            pa.field(\"model\", pa.string()),\n            pa.field(\"run_id\", pa.string()),\n            pa.field(\"vocab_ids\", pa.list_(pa.int32())),\n            pa.field(\"weights\", pa.list_(pa.float32())),\n            pa.field(\"nnz\", pa.int16()),\n            pa.field(\"created_at\", pa.timestamp(\"ms\")),\n        ]\n    )\n</code></pre>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.write_dense","title":"<code>write_dense(model, run_id, dim, records, shard=0)</code>","text":"<p>Compute write dense.</p> <p>Carry out the write dense operation.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.write_dense--parameters","title":"Parameters","text":"<p>model : str     Description for <code>model</code>. run_id : str     Description for <code>run_id</code>. dim : int     Description for <code>dim</code>. records : collections.abc.Iterable     Description for <code>records</code>. shard : int | None     Description for <code>shard</code>.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.write_dense--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.write_dense--examples","title":"Examples","text":"<p>from kgfoundry_common.parquet_io import write_dense result = write_dense(..., ..., ..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/kgfoundry_common/parquet_io.py</code> <pre><code>def write_dense(\n    self,\n    model: str,\n    run_id: str,\n    dim: int,\n    records: Iterable[tuple[str, list[float], float]],\n    shard: int = 0,\n) -&gt; str:\n    \"\"\"Compute write dense.\n\n    Carry out the write dense operation.\n\n    Parameters\n    ----------\n    model : str\n        Description for ``model``.\n    run_id : str\n        Description for ``run_id``.\n    dim : int\n        Description for ``dim``.\n    records : collections.abc.Iterable\n        Description for ``records``.\n    shard : int | None\n        Description for ``shard``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kgfoundry_common.parquet_io import write_dense\n    &gt;&gt;&gt; result = write_dense(..., ..., ..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    part_dir = self.root / f\"model={model}\" / f\"run_id={run_id}\" / f\"shard={shard:05d}\"\n    part_dir.mkdir(parents=True, exist_ok=True)\n    now = int(dt.datetime.now(dt.UTC).timestamp() * 1000)\n    rows = [\n        {\n            \"chunk_id\": cid,\n            \"model\": model,\n            \"run_id\": run_id,\n            \"dim\": dim,\n            \"vector\": vec,\n            \"l2_norm\": float(l2),\n            \"created_at\": now,\n        }\n        for cid, vec, l2 in records\n    ]\n    table = pa.Table.from_pylist(rows, schema=self.dense_schema(dim))\n    pq.write_table(\n        table,\n        part_dir / \"part-00000.parquet\",\n        compression=\"ZSTD\",\n        compression_level=ZSTD_LEVEL,\n        data_page_size=ROW_GROUP_SIZE,\n    )\n    return str(self.root)\n</code></pre>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.write_splade","title":"<code>write_splade(model, run_id, records, shard=0)</code>","text":"<p>Compute write splade.</p> <p>Carry out the write splade operation.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.write_splade--parameters","title":"Parameters","text":"<p>model : str     Description for <code>model</code>. run_id : str     Description for <code>run_id</code>. records : collections.abc.Iterable     Description for <code>records</code>. shard : int | None     Description for <code>shard</code>.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.write_splade--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/kgfoundry_common/parquet_io/#kgfoundry_common.parquet_io.ParquetVectorWriter.write_splade--examples","title":"Examples","text":"<p>from kgfoundry_common.parquet_io import write_splade result = write_splade(..., ..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/kgfoundry_common/parquet_io.py</code> <pre><code>def write_splade(\n    self,\n    model: str,\n    run_id: str,\n    records: Iterable[tuple[str, list[int], list[float]]],\n    shard: int = 0,\n) -&gt; str:\n    \"\"\"Compute write splade.\n\n    Carry out the write splade operation.\n\n    Parameters\n    ----------\n    model : str\n        Description for ``model``.\n    run_id : str\n        Description for ``run_id``.\n    records : collections.abc.Iterable\n        Description for ``records``.\n    shard : int | None\n        Description for ``shard``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from kgfoundry_common.parquet_io import write_splade\n    &gt;&gt;&gt; result = write_splade(..., ..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    part_dir = self.root / f\"model={model}\" / f\"run_id={run_id}\" / f\"shard={shard:05d}\"\n    part_dir.mkdir(parents=True, exist_ok=True)\n    now = int(dt.datetime.now(dt.UTC).timestamp() * 1000)\n    rows = [\n        {\n            \"chunk_id\": cid,\n            \"model\": model,\n            \"run_id\": run_id,\n            \"vocab_ids\": ids,\n            \"weights\": wts,\n            \"nnz\": len(ids),\n            \"created_at\": now,\n        }\n        for cid, ids, wts in records\n    ]\n    table = pa.Table.from_pylist(rows, schema=self.splade_schema())\n    pq.write_table(\n        table,\n        part_dir / \"part-00000.parquet\",\n        compression=\"ZSTD\",\n        compression_level=ZSTD_LEVEL,\n        data_page_size=ROW_GROUP_SIZE,\n    )\n    return str(self.root)\n</code></pre>"},{"location":"api/linking/","title":"<code>linking</code>","text":"<p>Linking utilities.</p>"},{"location":"api/linking/calibration/","title":"<code>linking.calibration</code>","text":"<p>Calibration utilities.</p>"},{"location":"api/linking/calibration/#linking.calibration.isotonic_calibrate","title":"<code>isotonic_calibrate(pairs)</code>","text":"<p>Compute isotonic calibrate.</p> <p>Carry out the isotonic calibrate operation.</p>"},{"location":"api/linking/calibration/#linking.calibration.isotonic_calibrate--parameters","title":"Parameters","text":"<p>pairs : List[Tuple[float, int]]     Description for <code>pairs</code>.</p>"},{"location":"api/linking/calibration/#linking.calibration.isotonic_calibrate--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/linking/calibration/#linking.calibration.isotonic_calibrate--examples","title":"Examples","text":"<p>from linking.calibration import isotonic_calibrate result = isotonic_calibrate(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/linking/calibration.py</code> <pre><code>def isotonic_calibrate(pairs: list[tuple[float, int]]) -&gt; dict[str, object]:\n    \"\"\"Compute isotonic calibrate.\n\n    Carry out the isotonic calibrate operation.\n\n    Parameters\n    ----------\n    pairs : List[Tuple[float, int]]\n        Description for ``pairs``.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from linking.calibration import isotonic_calibrate\n    &gt;&gt;&gt; result = isotonic_calibrate(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    # NOTE: fit isotonic regression parameters when calibrator is implemented\n    return {\"kind\": \"isotonic\", \"params\": []}\n</code></pre>"},{"location":"api/linking/linker/","title":"<code>linking.linker</code>","text":"<p>Linker utilities.</p>"},{"location":"api/linking/linker/#linking.linker.Linker","title":"<code>Linker</code>","text":"<p>Describe Linker.</p> Source code in <code>src/linking/linker.py</code> <pre><code>class Linker:\n    \"\"\"Describe Linker.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/observability/","title":"<code>observability</code>","text":"<p>Observability utilities.</p>"},{"location":"api/observability/metrics/","title":"<code>observability.metrics</code>","text":"<p>Metrics utilities.</p>"},{"location":"api/ontology/","title":"<code>ontology</code>","text":"<p>Ontology utilities.</p>"},{"location":"api/ontology/catalog/","title":"<code>ontology.catalog</code>","text":"<p>Catalog utilities.</p>"},{"location":"api/ontology/catalog/#ontology.catalog.Concept","title":"<code>Concept</code>  <code>dataclass</code>","text":"<p>Lightweight concept record used for typing within the ontology layer.</p> Source code in <code>src/ontology/catalog.py</code> <pre><code>@dataclass\nclass Concept:\n    \"\"\"Lightweight concept record used for typing within the ontology layer.\"\"\"\n\n    id: str\n    label: str | None = None\n</code></pre>"},{"location":"api/ontology/catalog/#ontology.catalog.OntologyCatalog","title":"<code>OntologyCatalog</code>","text":"<p>Describe OntologyCatalog.</p> Source code in <code>src/ontology/catalog.py</code> <pre><code>class OntologyCatalog:\n    \"\"\"Describe OntologyCatalog.\"\"\"\n\n    def __init__(self, concepts: list[Concept]) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        concepts : List[src.ontology.catalog.Concept]\n            Description for ``concepts``.\n        \"\"\"\n\n\n        self.by_id = {concept.id: concept for concept in concepts}\n\n    def neighbors(self, concept_id: str, depth: int = 1) -&gt; set[str]:\n        \"\"\"Compute neighbors.\n\n        Carry out the neighbors operation.\n\n        Parameters\n        ----------\n        concept_id : str\n            Description for ``concept_id``.\n        depth : int | None\n            Description for ``depth``.\n\n        Returns\n        -------\n        collections.abc.Set\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from ontology.catalog import neighbors\n        &gt;&gt;&gt; result = neighbors(..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        # NOTE: return neighbor concept IDs up to depth when ontology data is wired\n        return set()\n\n    def hydrate(self, concept_id: str) -&gt; dict[str, Any]:\n        \"\"\"Compute hydrate.\n\n        Carry out the hydrate operation.\n\n        Parameters\n        ----------\n        concept_id : str\n            Description for ``concept_id``.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from ontology.catalog import hydrate\n        &gt;&gt;&gt; result = hydrate(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        return {}\n</code></pre>"},{"location":"api/ontology/catalog/#ontology.catalog.OntologyCatalog.__init__","title":"<code>__init__(concepts)</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/ontology/catalog/#ontology.catalog.OntologyCatalog.__init__--parameters","title":"Parameters","text":"<p>concepts : List[src.ontology.catalog.Concept]     Description for <code>concepts</code>.</p> Source code in <code>src/ontology/catalog.py</code> <pre><code>def __init__(self, concepts: list[Concept]) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    concepts : List[src.ontology.catalog.Concept]\n        Description for ``concepts``.\n    \"\"\"\n\n\n    self.by_id = {concept.id: concept for concept in concepts}\n</code></pre>"},{"location":"api/ontology/catalog/#ontology.catalog.OntologyCatalog.hydrate","title":"<code>hydrate(concept_id)</code>","text":"<p>Compute hydrate.</p> <p>Carry out the hydrate operation.</p>"},{"location":"api/ontology/catalog/#ontology.catalog.OntologyCatalog.hydrate--parameters","title":"Parameters","text":"<p>concept_id : str     Description for <code>concept_id</code>.</p>"},{"location":"api/ontology/catalog/#ontology.catalog.OntologyCatalog.hydrate--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/ontology/catalog/#ontology.catalog.OntologyCatalog.hydrate--examples","title":"Examples","text":"<p>from ontology.catalog import hydrate result = hydrate(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/ontology/catalog.py</code> <pre><code>def hydrate(self, concept_id: str) -&gt; dict[str, Any]:\n    \"\"\"Compute hydrate.\n\n    Carry out the hydrate operation.\n\n    Parameters\n    ----------\n    concept_id : str\n        Description for ``concept_id``.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from ontology.catalog import hydrate\n    &gt;&gt;&gt; result = hydrate(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api/ontology/catalog/#ontology.catalog.OntologyCatalog.neighbors","title":"<code>neighbors(concept_id, depth=1)</code>","text":"<p>Compute neighbors.</p> <p>Carry out the neighbors operation.</p>"},{"location":"api/ontology/catalog/#ontology.catalog.OntologyCatalog.neighbors--parameters","title":"Parameters","text":"<p>concept_id : str     Description for <code>concept_id</code>. depth : int | None     Description for <code>depth</code>.</p>"},{"location":"api/ontology/catalog/#ontology.catalog.OntologyCatalog.neighbors--returns","title":"Returns","text":"<p>collections.abc.Set     Description of return value.</p>"},{"location":"api/ontology/catalog/#ontology.catalog.OntologyCatalog.neighbors--examples","title":"Examples","text":"<p>from ontology.catalog import neighbors result = neighbors(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/ontology/catalog.py</code> <pre><code>def neighbors(self, concept_id: str, depth: int = 1) -&gt; set[str]:\n    \"\"\"Compute neighbors.\n\n    Carry out the neighbors operation.\n\n    Parameters\n    ----------\n    concept_id : str\n        Description for ``concept_id``.\n    depth : int | None\n        Description for ``depth``.\n\n    Returns\n    -------\n    collections.abc.Set\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from ontology.catalog import neighbors\n    &gt;&gt;&gt; result = neighbors(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    # NOTE: return neighbor concept IDs up to depth when ontology data is wired\n    return set()\n</code></pre>"},{"location":"api/ontology/loader/","title":"<code>ontology.loader</code>","text":"<p>Loader utilities.</p>"},{"location":"api/ontology/loader/#ontology.loader.OntologyLoader","title":"<code>OntologyLoader</code>","text":"<p>Describe OntologyLoader.</p> Source code in <code>src/ontology/loader.py</code> <pre><code>class OntologyLoader:\n    \"\"\"Describe OntologyLoader.\"\"\"\n\n    ...\n</code></pre>"},{"location":"api/orchestration/","title":"<code>orchestration</code>","text":"<p>Orchestration utilities.</p>"},{"location":"api/orchestration/cli/","title":"<code>orchestration.cli</code>","text":"<p>Cli utilities.</p>"},{"location":"api/orchestration/cli/#orchestration.cli.api","title":"<code>api(port=8080)</code>","text":"<p>Compute api.</p> <p>Carry out the api operation.</p>"},{"location":"api/orchestration/cli/#orchestration.cli.api--parameters","title":"Parameters","text":"<p>port : int | None     Description for <code>port</code>.</p>"},{"location":"api/orchestration/cli/#orchestration.cli.api--examples","title":"Examples","text":"<p>from orchestration.cli import api api(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/orchestration/cli.py</code> <pre><code>@app.command()\ndef api(port: int = 8080) -&gt; None:\n    \"\"\"Compute api.\n\n    Carry out the api operation.\n\n    Parameters\n    ----------\n    port : int | None\n        Description for ``port``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from orchestration.cli import api\n    &gt;&gt;&gt; api(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    import uvicorn\n\n    uvicorn.run(\"search_api.app:app\", host=\"0.0.0.0\", port=port, reload=False)\n</code></pre>"},{"location":"api/orchestration/cli/#orchestration.cli.e2e","title":"<code>e2e()</code>","text":"<p>Compute e2e.</p> <p>Carry out the e2e operation.</p>"},{"location":"api/orchestration/cli/#orchestration.cli.e2e--raises","title":"Raises","text":"<p>typer.Exit     Raised when validation fails.</p>"},{"location":"api/orchestration/cli/#orchestration.cli.e2e--examples","title":"Examples","text":"<p>from orchestration.cli import e2e e2e()  # doctest: +ELLIPSIS</p> Source code in <code>src/orchestration/cli.py</code> <pre><code>@app.command()\ndef e2e() -&gt; None:\n    \"\"\"Compute e2e.\n\n    Carry out the e2e operation.\n\n    Raises\n    ------\n    typer.Exit\n        Raised when validation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from orchestration.cli import e2e\n    &gt;&gt;&gt; e2e()  # doctest: +ELLIPSIS\n    \"\"\"\n    try:\n        from orchestration.flows import e2e_flow\n    except ModuleNotFoundError as exc:  # pragma: no cover - defensive messaging\n        typer.echo(\n            \"Prefect is required for the e2e pipeline command. \"\n            \"Install it via `pip install -e '.[gpu]'` or add `prefect` manually.\",\n            err=True,\n        )\n        raise typer.Exit(code=1) from exc\n\n    stages = e2e_flow()\n    for step in stages:\n        typer.echo(step)\n</code></pre>"},{"location":"api/orchestration/cli/#orchestration.cli.index_bm25","title":"<code>index_bm25(chunks_parquet=typer.Argument(..., help='Path to Parquet/JSONL with chunks'), backend=typer.Option('lucene', help='lucene|pure'), index_dir=typer.Option('./_indices/bm25', help='Output index directory'))</code>","text":"<p>Compute index bm25.</p> <p>Carry out the index bm25 operation.</p>"},{"location":"api/orchestration/cli/#orchestration.cli.index_bm25--parameters","title":"Parameters","text":"<p>chunks_parquet : str | None     Description for <code>chunks_parquet</code>. backend : str | None     Description for <code>backend</code>. index_dir : str | None     Description for <code>index_dir</code>.</p>"},{"location":"api/orchestration/cli/#orchestration.cli.index_bm25--examples","title":"Examples","text":"<p>from orchestration.cli import index_bm25 index_bm25(..., ..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/orchestration/cli.py</code> <pre><code>@app.command()\ndef index_bm25(\n    chunks_parquet: str = typer.Argument(..., help=\"Path to Parquet/JSONL with chunks\"),\n    backend: str = typer.Option(\"lucene\", help=\"lucene|pure\"),\n    index_dir: str = typer.Option(\"./_indices/bm25\", help=\"Output index directory\"),\n) -&gt; None:\n    \"\"\"Compute index bm25.\n\n    Carry out the index bm25 operation.\n\n    Parameters\n    ----------\n    chunks_parquet : str | None\n        Description for ``chunks_parquet``.\n    backend : str | None\n        Description for ``backend``.\n    index_dir : str | None\n        Description for ``index_dir``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from orchestration.cli import index_bm25\n    &gt;&gt;&gt; index_bm25(..., ..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    os.makedirs(index_dir, exist_ok=True)\n    # Very small loader that supports JSONL in this skeleton (Parquet in real pipeline).\n    docs: list[tuple[str, dict[str, str]]] = []\n    if chunks_parquet.endswith(\".jsonl\"):\n        with open(chunks_parquet, encoding=\"utf-8\") as fh:\n            for line in fh:\n                rec = json.loads(line)\n                docs.append(\n                    (\n                        rec[\"chunk_id\"],\n                        {\n                            \"title\": rec.get(\"title\", \"\"),\n                            \"section\": rec.get(\"section\", \"\"),\n                            \"body\": rec.get(\"text\", \"\"),\n                        },\n                    )\n                )\n    else:\n        # naive: expect a JSON file with list under skeleton; replace with Parquet\n        # reader in implementation\n        with open(chunks_parquet, encoding=\"utf-8\") as fh:\n            data = json.load(fh)\n        for rec in data:\n            docs.append(\n                (\n                    rec[\"chunk_id\"],\n                    {\n                        \"title\": rec.get(\"title\", \"\"),\n                        \"section\": rec.get(\"section\", \"\"),\n                        \"body\": rec.get(\"text\", \"\"),\n                    },\n                )\n            )\n    idx = get_bm25(backend, index_dir, k1=0.9, b=0.4)\n    idx.build(docs)\n    typer.echo(f\"BM25 index built at {index_dir} using backend={backend} ({type(idx).__name__})\")\n</code></pre>"},{"location":"api/orchestration/cli/#orchestration.cli.index_faiss","title":"<code>index_faiss(dense_vectors=typer.Argument(..., help='Path to dense vectors JSON (skeleton)'), index_path=typer.Option('./_indices/faiss/shard_000.idx', help='Output index (CPU .idx)'))</code>","text":"<p>Compute index faiss.</p> <p>Carry out the index faiss operation.</p>"},{"location":"api/orchestration/cli/#orchestration.cli.index_faiss--parameters","title":"Parameters","text":"<p>dense_vectors : str | None     Description for <code>dense_vectors</code>. index_path : str | None     Description for <code>index_path</code>.</p>"},{"location":"api/orchestration/cli/#orchestration.cli.index_faiss--examples","title":"Examples","text":"<p>from orchestration.cli import index_faiss index_faiss(..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/orchestration/cli.py</code> <pre><code>@app.command()\ndef index_faiss(\n    dense_vectors: str = typer.Argument(..., help=\"Path to dense vectors JSON (skeleton)\"),\n    index_path: str = typer.Option(\n        \"./_indices/faiss/shard_000.idx\", help=\"Output index (CPU .idx)\"\n    ),\n) -&gt; None:\n    \"\"\"Compute index faiss.\n\n    Carry out the index faiss operation.\n\n    Parameters\n    ----------\n    dense_vectors : str | None\n        Description for ``dense_vectors``.\n    index_path : str | None\n        Description for ``index_path``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from orchestration.cli import index_faiss\n    &gt;&gt;&gt; index_faiss(..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    os.makedirs(os.path.dirname(index_path), exist_ok=True)\n    with open(dense_vectors, encoding=\"utf-8\") as fh:\n        vecs = json.load(fh)\n    keys = [r[\"key\"] for r in vecs]\n    vectors = np.array([r[\"vector\"] for r in vecs], dtype=\"float32\")\n    # Train and add\n    vs = faiss_gpu.FaissGpuIndex()\n    vs.train(vectors[: min(len(vectors), 10000)])  # small train set\n    vs.add(keys, vectors)\n    # Save CPU form when possible\n    try:\n        vs.save(index_path, None)\n        typer.echo(f\"FAISS index saved to {index_path}\")\n    except Exception as e:\n        typer.echo(f\"Saved fallback matrix (npz) due to {e!r}\")\n        vs.save(index_path, None)\n</code></pre>"},{"location":"api/orchestration/fixture_flow/","title":"<code>orchestration.fixture_flow</code>","text":"<p>Fixture Flow utilities.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.fixture_pipeline","title":"<code>fixture_pipeline(root='/data', db_path='/data/catalog/catalog.duckdb')</code>","text":"<p>Compute fixture pipeline.</p> <p>Carry out the fixture pipeline operation.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.fixture_pipeline--parameters","title":"Parameters","text":"<p>root : str | None     Description for <code>root</code>. db_path : str | None     Description for <code>db_path</code>.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.fixture_pipeline--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.fixture_pipeline--examples","title":"Examples","text":"<p>from orchestration.fixture_flow import fixture_pipeline result = fixture_pipeline(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/orchestration/fixture_flow.py</code> <pre><code>@flow(name=\"kgfoundry_fixture_pipeline\")\ndef fixture_pipeline(\n    root: str = \"/data\", db_path: str = \"/data/catalog/catalog.duckdb\"\n) -&gt; dict[str, list[str]]:\n    \"\"\"Compute fixture pipeline.\n\n    Carry out the fixture pipeline operation.\n\n    Parameters\n    ----------\n    root : str | None\n        Description for ``root``.\n    db_path : str | None\n        Description for ``db_path``.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from orchestration.fixture_flow import fixture_pipeline\n    &gt;&gt;&gt; result = fixture_pipeline(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    t_prepare_dirs(root)\n    chunks_info = t_write_fixture_chunks(f\"{root}/parquet/chunks\")\n    dense_info = t_write_fixture_dense(f\"{root}/parquet/dense\")\n    sparse_info = t_write_fixture_splade(f\"{root}/parquet/sparse\")\n    return t_register_in_duckdb(db_path, chunks_info, dense_info, sparse_info)\n</code></pre>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_prepare_dirs","title":"<code>t_prepare_dirs(root)</code>","text":"<p>Compute t prepare dirs.</p> <p>Carry out the t prepare dirs operation.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_prepare_dirs--parameters","title":"Parameters","text":"<p>root : str     Description for <code>root</code>.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_prepare_dirs--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_prepare_dirs--examples","title":"Examples","text":"<p>from orchestration.fixture_flow import t_prepare_dirs result = t_prepare_dirs(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/orchestration/fixture_flow.py</code> <pre><code>@task\ndef t_prepare_dirs(root: str) -&gt; dict[str, bool]:\n    \"\"\"Compute t prepare dirs.\n\n    Carry out the t prepare dirs operation.\n\n    Parameters\n    ----------\n    root : str\n        Description for ``root``.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from orchestration.fixture_flow import t_prepare_dirs\n    &gt;&gt;&gt; result = t_prepare_dirs(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    path = Path(root)\n    (path / \"parquet\" / \"dense\").mkdir(parents=True, exist_ok=True)\n    (path / \"parquet\" / \"sparse\").mkdir(parents=True, exist_ok=True)\n    (path / \"parquet\" / \"chunks\").mkdir(parents=True, exist_ok=True)\n    (path / \"catalog\").mkdir(parents=True, exist_ok=True)\n    return {\"ok\": True}\n</code></pre>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_register_in_duckdb","title":"<code>t_register_in_duckdb(db_path, chunks_info, dense_info, sparse_info)</code>","text":"<p>Compute t register in duckdb.</p> <p>Carry out the t register in duckdb operation.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_register_in_duckdb--parameters","title":"Parameters","text":"<p>db_path : str     Description for <code>db_path</code>. chunks_info : Tuple[str, int]     Description for <code>chunks_info</code>. dense_info : Tuple[str, int]     Description for <code>dense_info</code>. sparse_info : Tuple[str, int]     Description for <code>sparse_info</code>.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_register_in_duckdb--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_register_in_duckdb--examples","title":"Examples","text":"<p>from orchestration.fixture_flow import t_register_in_duckdb result = t_register_in_duckdb(..., ..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/orchestration/fixture_flow.py</code> <pre><code>@task\ndef t_register_in_duckdb(\n    db_path: str,\n    chunks_info: tuple[str, int],\n    dense_info: tuple[str, int],\n    sparse_info: tuple[str, int],\n) -&gt; dict[str, list[str]]:\n    \"\"\"Compute t register in duckdb.\n\n    Carry out the t register in duckdb operation.\n\n    Parameters\n    ----------\n    db_path : str\n        Description for ``db_path``.\n    chunks_info : Tuple[str, int]\n        Description for ``chunks_info``.\n    dense_info : Tuple[str, int]\n        Description for ``dense_info``.\n    sparse_info : Tuple[str, int]\n        Description for ``sparse_info``.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from orchestration.fixture_flow import t_register_in_duckdb\n    &gt;&gt;&gt; result = t_register_in_duckdb(..., ..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    registry = DuckDBRegistryHelper(db_path)\n    dense_run = registry.new_run(\"dense_embed\", \"Qwen3-Embedding-4B\", \"main\", {\"dim\": 2560})\n    sparse_run = registry.new_run(\"splade_encode\", \"SPLADE-v3-distilbert\", \"main\", {\"topk\": 256})\n\n    ds_chunks = registry.begin_dataset(\"chunks\", dense_run)\n    registry.commit_dataset(ds_chunks, chunks_info[0], rows=chunks_info[1])\n\n    ds_dense = registry.begin_dataset(\"dense\", dense_run)\n    registry.commit_dataset(ds_dense, dense_info[0], rows=dense_info[1])\n\n    ds_sparse = registry.begin_dataset(\"sparse\", sparse_run)\n    registry.commit_dataset(ds_sparse, sparse_info[0], rows=sparse_info[1])\n\n    registry.register_documents(\n        [\n            Doc(\n                id=\"urn:doc:fixture:0001\",\n                title=\"Fixture Doc\",\n                authors=[],\n                pdf_uri=\"/dev/null\",\n                source=\"fixture\",\n                openalex_id=None,\n                doi=None,\n                arxiv_id=None,\n                pmcid=None,\n                pub_date=None,\n                license=\"CC0\",\n                language=\"en\",\n                content_hash=\"\",\n            )\n        ]\n    )\n    registry.close_run(dense_run, True)\n    registry.close_run(sparse_run, True)\n    return {\"runs\": [dense_run, sparse_run]}\n</code></pre>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_chunks","title":"<code>t_write_fixture_chunks(chunks_root)</code>","text":"<p>Compute t write fixture chunks.</p> <p>Carry out the t write fixture chunks operation.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_chunks--parameters","title":"Parameters","text":"<p>chunks_root : str     Description for <code>chunks_root</code>.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_chunks--returns","title":"Returns","text":"<p>Tuple[str, int]     Description of return value.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_chunks--examples","title":"Examples","text":"<p>from orchestration.fixture_flow import t_write_fixture_chunks result = t_write_fixture_chunks(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/orchestration/fixture_flow.py</code> <pre><code>@task\ndef t_write_fixture_chunks(chunks_root: str) -&gt; tuple[str, int]:\n    \"\"\"Compute t write fixture chunks.\n\n    Carry out the t write fixture chunks operation.\n\n    Parameters\n    ----------\n    chunks_root : str\n        Description for ``chunks_root``.\n\n    Returns\n    -------\n    Tuple[str, int]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from orchestration.fixture_flow import t_write_fixture_chunks\n    &gt;&gt;&gt; result = t_write_fixture_chunks(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    writer = ParquetChunkWriter(chunks_root, model=\"docling_hybrid\", run_id=\"fixture\")\n    rows = [\n        {\n            \"chunk_id\": \"urn:chunk:fixture:0-28\",\n            \"doc_id\": \"urn:doc:fixture:0001\",\n            \"section\": \"Intro\",\n            \"start_char\": 0,\n            \"end_char\": 28,\n            \"doctags_span\": {\"node_id\": \"n1\", \"start\": 0, \"end\": 28},\n            \"text\": \"Hello fixture text about LLMs\",\n            \"tokens\": 5,\n            \"created_at\": 0,\n        }\n    ]\n    dataset_root = writer.write(rows)\n    return dataset_root, len(rows)\n</code></pre>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_dense","title":"<code>t_write_fixture_dense(dense_root)</code>","text":"<p>Compute t write fixture dense.</p> <p>Carry out the t write fixture dense operation.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_dense--parameters","title":"Parameters","text":"<p>dense_root : str     Description for <code>dense_root</code>.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_dense--returns","title":"Returns","text":"<p>Tuple[str, int]     Description of return value.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_dense--examples","title":"Examples","text":"<p>from orchestration.fixture_flow import t_write_fixture_dense result = t_write_fixture_dense(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/orchestration/fixture_flow.py</code> <pre><code>@task\ndef t_write_fixture_dense(dense_root: str) -&gt; tuple[str, int]:\n    \"\"\"Compute t write fixture dense.\n\n    Carry out the t write fixture dense operation.\n\n    Parameters\n    ----------\n    dense_root : str\n        Description for ``dense_root``.\n\n    Returns\n    -------\n    Tuple[str, int]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from orchestration.fixture_flow import t_write_fixture_dense\n    &gt;&gt;&gt; result = t_write_fixture_dense(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    writer = ParquetVectorWriter(dense_root)\n    vector = [0.0] * 2560\n    out_root = writer.write_dense(\n        \"Qwen3-Embedding-4B\", \"fixture\", 2560, [(\"urn:chunk:fixture:0-28\", vector, 1.0)], shard=0\n    )\n    return out_root, 1\n</code></pre>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_splade","title":"<code>t_write_fixture_splade(sparse_root)</code>","text":"<p>Compute t write fixture splade.</p> <p>Carry out the t write fixture splade operation.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_splade--parameters","title":"Parameters","text":"<p>sparse_root : str     Description for <code>sparse_root</code>.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_splade--returns","title":"Returns","text":"<p>Tuple[str, int]     Description of return value.</p>"},{"location":"api/orchestration/fixture_flow/#orchestration.fixture_flow.t_write_fixture_splade--examples","title":"Examples","text":"<p>from orchestration.fixture_flow import t_write_fixture_splade result = t_write_fixture_splade(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/orchestration/fixture_flow.py</code> <pre><code>@task\ndef t_write_fixture_splade(sparse_root: str) -&gt; tuple[str, int]:\n    \"\"\"Compute t write fixture splade.\n\n    Carry out the t write fixture splade operation.\n\n    Parameters\n    ----------\n    sparse_root : str\n        Description for ``sparse_root``.\n\n    Returns\n    -------\n    Tuple[str, int]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from orchestration.fixture_flow import t_write_fixture_splade\n    &gt;&gt;&gt; result = t_write_fixture_splade(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    writer = ParquetVectorWriter(sparse_root)\n    out_root = writer.write_splade(\n        \"SPLADE-v3-distilbert\",\n        \"fixture\",\n        [(\"urn:chunk:fixture:0-28\", [1, 7, 42], [0.3, 0.2, 0.1])],\n        shard=0,\n    )\n    return out_root, 1\n</code></pre>"},{"location":"api/orchestration/flows/","title":"<code>orchestration.flows</code>","text":"<p>Flows utilities.</p>"},{"location":"api/orchestration/flows/#orchestration.flows.e2e_flow","title":"<code>e2e_flow()</code>","text":"<p>Compute e2e flow.</p> <p>Carry out the e2e flow operation.</p>"},{"location":"api/orchestration/flows/#orchestration.flows.e2e_flow--returns","title":"Returns","text":"<p>List[str]     Description of return value.</p>"},{"location":"api/orchestration/flows/#orchestration.flows.e2e_flow--examples","title":"Examples","text":"<p>from orchestration.flows import e2e_flow result = e2e_flow() result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/orchestration/flows.py</code> <pre><code>@flow(name=\"kgfoundry_e2e_skeleton\")\ndef e2e_flow() -&gt; list[str]:\n    \"\"\"Compute e2e flow.\n\n    Carry out the e2e flow operation.\n\n    Returns\n    -------\n    List[str]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from orchestration.flows import e2e_flow\n    &gt;&gt;&gt; result = e2e_flow()\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return [\n        t_echo.submit(x).result()\n        for x in [\n            \"harvest\",\n            \"doctags\",\n            \"chunk\",\n            \"embed_dense\",\n            \"encode_splade\",\n            \"bm25\",\n            \"faiss\",\n            \"ontology\",\n            \"concept_embed\",\n            \"linker\",\n            \"kg\",\n        ]\n    ]\n</code></pre>"},{"location":"api/orchestration/flows/#orchestration.flows.t_echo","title":"<code>t_echo(msg)</code>","text":"<p>Compute t echo.</p> <p>Carry out the t echo operation.</p>"},{"location":"api/orchestration/flows/#orchestration.flows.t_echo--parameters","title":"Parameters","text":"<p>msg : str     Description for <code>msg</code>.</p>"},{"location":"api/orchestration/flows/#orchestration.flows.t_echo--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/orchestration/flows/#orchestration.flows.t_echo--examples","title":"Examples","text":"<p>from orchestration.flows import t_echo result = t_echo(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/orchestration/flows.py</code> <pre><code>@task\ndef t_echo(msg: str) -&gt; str:\n    \"\"\"Compute t echo.\n\n    Carry out the t echo operation.\n\n    Parameters\n    ----------\n    msg : str\n        Description for ``msg``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from orchestration.flows import t_echo\n    &gt;&gt;&gt; result = t_echo(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return msg\n</code></pre>"},{"location":"api/registry/","title":"<code>registry</code>","text":"<p>Registry utilities.</p>"},{"location":"api/registry/api/","title":"<code>registry.api</code>","text":"<p>Api utilities.</p>"},{"location":"api/registry/api/#registry.api.Registry","title":"<code>Registry</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Describe Registry.</p> Source code in <code>src/registry/api.py</code> <pre><code>class Registry(Protocol):\n    \"\"\"Describe Registry.\"\"\"\n\n    def begin_dataset(self, kind: str, run_id: str) -&gt; str:\n        \"\"\"Compute begin dataset.\n\n        Carry out the begin dataset operation.\n\n        Parameters\n        ----------\n        kind : str\n            Description for ``kind``.\n        run_id : str\n            Description for ``run_id``.\n\n        Returns\n        -------\n        str\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.api import begin_dataset\n        &gt;&gt;&gt; result = begin_dataset(..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        ...\n\n    def commit_dataset(self, dataset_id: str, parquet_root: str, rows: int) -&gt; None:\n        \"\"\"Compute commit dataset.\n\n        Carry out the commit dataset operation.\n\n        Parameters\n        ----------\n        dataset_id : str\n            Description for ``dataset_id``.\n        parquet_root : str\n            Description for ``parquet_root``.\n        rows : int\n            Description for ``rows``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.api import commit_dataset\n        &gt;&gt;&gt; commit_dataset(..., ..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        ...\n\n    def rollback_dataset(self, dataset_id: str) -&gt; None:\n        \"\"\"Compute rollback dataset.\n\n        Carry out the rollback dataset operation.\n\n        Parameters\n        ----------\n        dataset_id : str\n            Description for ``dataset_id``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.api import rollback_dataset\n        &gt;&gt;&gt; rollback_dataset(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        ...\n\n    def insert_run(\n        self,\n        purpose: str,\n        model_id: str | None,\n        revision: str | None,\n        config: Mapping[str, object],\n    ) -&gt; str:\n        \"\"\"Compute insert run.\n\n        Carry out the insert run operation.\n\n        Parameters\n        ----------\n        purpose : str\n            Description for ``purpose``.\n        model_id : str | None\n            Description for ``model_id``.\n        revision : str | None\n            Description for ``revision``.\n        config : collections.abc.Mapping\n            Description for ``config``.\n\n        Returns\n        -------\n        str\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.api import insert_run\n        &gt;&gt;&gt; result = insert_run(..., ..., ..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        ...\n\n    def close_run(self, run_id: str, success: bool, notes: str | None = None) -&gt; None:\n        \"\"\"Compute close run.\n\n        Carry out the close run operation.\n\n        Parameters\n        ----------\n        run_id : str\n            Description for ``run_id``.\n        success : bool\n            Description for ``success``.\n        notes : str | None\n            Description for ``notes``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.api import close_run\n        &gt;&gt;&gt; close_run(..., ..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        ...\n\n    def register_documents(self, docs: list[Doc]) -&gt; None:\n        \"\"\"Compute register documents.\n\n        Carry out the register documents operation.\n\n        Parameters\n        ----------\n        docs : List[src.kgfoundry_common.models.Doc]\n            Description for ``docs``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.api import register_documents\n        &gt;&gt;&gt; register_documents(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        ...\n\n    def register_doctags(self, assets: list[DoctagsAsset]) -&gt; None:\n        \"\"\"Compute register doctags.\n\n        Carry out the register doctags operation.\n\n        Parameters\n        ----------\n        assets : List[src.kgfoundry_common.models.DoctagsAsset]\n            Description for ``assets``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.api import register_doctags\n        &gt;&gt;&gt; register_doctags(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        ...\n\n    def emit_event(self, event_name: str, subject_id: str, payload: Mapping[str, object]) -&gt; None:\n        \"\"\"Compute emit event.\n\n        Carry out the emit event operation.\n\n        Parameters\n        ----------\n        event_name : str\n            Description for ``event_name``.\n        subject_id : str\n            Description for ``subject_id``.\n        payload : collections.abc.Mapping\n            Description for ``payload``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.api import emit_event\n        &gt;&gt;&gt; emit_event(..., ..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        ...\n\n    def incident(self, event: str, subject_id: str, error_class: str, message: str) -&gt; None:\n        \"\"\"Compute incident.\n\n        Carry out the incident operation.\n\n        Parameters\n        ----------\n        event : str\n            Description for ``event``.\n        subject_id : str\n            Description for ``subject_id``.\n        error_class : str\n            Description for ``error_class``.\n        message : str\n            Description for ``message``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.api import incident\n        &gt;&gt;&gt; incident(..., ..., ..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.begin_dataset","title":"<code>begin_dataset(kind, run_id)</code>","text":"<p>Compute begin dataset.</p> <p>Carry out the begin dataset operation.</p>"},{"location":"api/registry/api/#registry.api.Registry.begin_dataset--parameters","title":"Parameters","text":"<p>kind : str     Description for <code>kind</code>. run_id : str     Description for <code>run_id</code>.</p>"},{"location":"api/registry/api/#registry.api.Registry.begin_dataset--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/registry/api/#registry.api.Registry.begin_dataset--examples","title":"Examples","text":"<p>from registry.api import begin_dataset result = begin_dataset(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/registry/api.py</code> <pre><code>def begin_dataset(self, kind: str, run_id: str) -&gt; str:\n    \"\"\"Compute begin dataset.\n\n    Carry out the begin dataset operation.\n\n    Parameters\n    ----------\n    kind : str\n        Description for ``kind``.\n    run_id : str\n        Description for ``run_id``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.api import begin_dataset\n    &gt;&gt;&gt; result = begin_dataset(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.close_run","title":"<code>close_run(run_id, success, notes=None)</code>","text":"<p>Compute close run.</p> <p>Carry out the close run operation.</p>"},{"location":"api/registry/api/#registry.api.Registry.close_run--parameters","title":"Parameters","text":"<p>run_id : str     Description for <code>run_id</code>. success : bool     Description for <code>success</code>. notes : str | None     Description for <code>notes</code>.</p>"},{"location":"api/registry/api/#registry.api.Registry.close_run--examples","title":"Examples","text":"<p>from registry.api import close_run close_run(..., ..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/api.py</code> <pre><code>def close_run(self, run_id: str, success: bool, notes: str | None = None) -&gt; None:\n    \"\"\"Compute close run.\n\n    Carry out the close run operation.\n\n    Parameters\n    ----------\n    run_id : str\n        Description for ``run_id``.\n    success : bool\n        Description for ``success``.\n    notes : str | None\n        Description for ``notes``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.api import close_run\n    &gt;&gt;&gt; close_run(..., ..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.commit_dataset","title":"<code>commit_dataset(dataset_id, parquet_root, rows)</code>","text":"<p>Compute commit dataset.</p> <p>Carry out the commit dataset operation.</p>"},{"location":"api/registry/api/#registry.api.Registry.commit_dataset--parameters","title":"Parameters","text":"<p>dataset_id : str     Description for <code>dataset_id</code>. parquet_root : str     Description for <code>parquet_root</code>. rows : int     Description for <code>rows</code>.</p>"},{"location":"api/registry/api/#registry.api.Registry.commit_dataset--examples","title":"Examples","text":"<p>from registry.api import commit_dataset commit_dataset(..., ..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/api.py</code> <pre><code>def commit_dataset(self, dataset_id: str, parquet_root: str, rows: int) -&gt; None:\n    \"\"\"Compute commit dataset.\n\n    Carry out the commit dataset operation.\n\n    Parameters\n    ----------\n    dataset_id : str\n        Description for ``dataset_id``.\n    parquet_root : str\n        Description for ``parquet_root``.\n    rows : int\n        Description for ``rows``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.api import commit_dataset\n    &gt;&gt;&gt; commit_dataset(..., ..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.emit_event","title":"<code>emit_event(event_name, subject_id, payload)</code>","text":"<p>Compute emit event.</p> <p>Carry out the emit event operation.</p>"},{"location":"api/registry/api/#registry.api.Registry.emit_event--parameters","title":"Parameters","text":"<p>event_name : str     Description for <code>event_name</code>. subject_id : str     Description for <code>subject_id</code>. payload : collections.abc.Mapping     Description for <code>payload</code>.</p>"},{"location":"api/registry/api/#registry.api.Registry.emit_event--examples","title":"Examples","text":"<p>from registry.api import emit_event emit_event(..., ..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/api.py</code> <pre><code>def emit_event(self, event_name: str, subject_id: str, payload: Mapping[str, object]) -&gt; None:\n    \"\"\"Compute emit event.\n\n    Carry out the emit event operation.\n\n    Parameters\n    ----------\n    event_name : str\n        Description for ``event_name``.\n    subject_id : str\n        Description for ``subject_id``.\n    payload : collections.abc.Mapping\n        Description for ``payload``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.api import emit_event\n    &gt;&gt;&gt; emit_event(..., ..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.incident","title":"<code>incident(event, subject_id, error_class, message)</code>","text":"<p>Compute incident.</p> <p>Carry out the incident operation.</p>"},{"location":"api/registry/api/#registry.api.Registry.incident--parameters","title":"Parameters","text":"<p>event : str     Description for <code>event</code>. subject_id : str     Description for <code>subject_id</code>. error_class : str     Description for <code>error_class</code>. message : str     Description for <code>message</code>.</p>"},{"location":"api/registry/api/#registry.api.Registry.incident--examples","title":"Examples","text":"<p>from registry.api import incident incident(..., ..., ..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/api.py</code> <pre><code>def incident(self, event: str, subject_id: str, error_class: str, message: str) -&gt; None:\n    \"\"\"Compute incident.\n\n    Carry out the incident operation.\n\n    Parameters\n    ----------\n    event : str\n        Description for ``event``.\n    subject_id : str\n        Description for ``subject_id``.\n    error_class : str\n        Description for ``error_class``.\n    message : str\n        Description for ``message``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.api import incident\n    &gt;&gt;&gt; incident(..., ..., ..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.insert_run","title":"<code>insert_run(purpose, model_id, revision, config)</code>","text":"<p>Compute insert run.</p> <p>Carry out the insert run operation.</p>"},{"location":"api/registry/api/#registry.api.Registry.insert_run--parameters","title":"Parameters","text":"<p>purpose : str     Description for <code>purpose</code>. model_id : str | None     Description for <code>model_id</code>. revision : str | None     Description for <code>revision</code>. config : collections.abc.Mapping     Description for <code>config</code>.</p>"},{"location":"api/registry/api/#registry.api.Registry.insert_run--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/registry/api/#registry.api.Registry.insert_run--examples","title":"Examples","text":"<p>from registry.api import insert_run result = insert_run(..., ..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/registry/api.py</code> <pre><code>def insert_run(\n    self,\n    purpose: str,\n    model_id: str | None,\n    revision: str | None,\n    config: Mapping[str, object],\n) -&gt; str:\n    \"\"\"Compute insert run.\n\n    Carry out the insert run operation.\n\n    Parameters\n    ----------\n    purpose : str\n        Description for ``purpose``.\n    model_id : str | None\n        Description for ``model_id``.\n    revision : str | None\n        Description for ``revision``.\n    config : collections.abc.Mapping\n        Description for ``config``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.api import insert_run\n    &gt;&gt;&gt; result = insert_run(..., ..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.register_doctags","title":"<code>register_doctags(assets)</code>","text":"<p>Compute register doctags.</p> <p>Carry out the register doctags operation.</p>"},{"location":"api/registry/api/#registry.api.Registry.register_doctags--parameters","title":"Parameters","text":"<p>assets : List[src.kgfoundry_common.models.DoctagsAsset]     Description for <code>assets</code>.</p>"},{"location":"api/registry/api/#registry.api.Registry.register_doctags--examples","title":"Examples","text":"<p>from registry.api import register_doctags register_doctags(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/api.py</code> <pre><code>def register_doctags(self, assets: list[DoctagsAsset]) -&gt; None:\n    \"\"\"Compute register doctags.\n\n    Carry out the register doctags operation.\n\n    Parameters\n    ----------\n    assets : List[src.kgfoundry_common.models.DoctagsAsset]\n        Description for ``assets``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.api import register_doctags\n    &gt;&gt;&gt; register_doctags(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.register_documents","title":"<code>register_documents(docs)</code>","text":"<p>Compute register documents.</p> <p>Carry out the register documents operation.</p>"},{"location":"api/registry/api/#registry.api.Registry.register_documents--parameters","title":"Parameters","text":"<p>docs : List[src.kgfoundry_common.models.Doc]     Description for <code>docs</code>.</p>"},{"location":"api/registry/api/#registry.api.Registry.register_documents--examples","title":"Examples","text":"<p>from registry.api import register_documents register_documents(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/api.py</code> <pre><code>def register_documents(self, docs: list[Doc]) -&gt; None:\n    \"\"\"Compute register documents.\n\n    Carry out the register documents operation.\n\n    Parameters\n    ----------\n    docs : List[src.kgfoundry_common.models.Doc]\n        Description for ``docs``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.api import register_documents\n    &gt;&gt;&gt; register_documents(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/registry/api/#registry.api.Registry.rollback_dataset","title":"<code>rollback_dataset(dataset_id)</code>","text":"<p>Compute rollback dataset.</p> <p>Carry out the rollback dataset operation.</p>"},{"location":"api/registry/api/#registry.api.Registry.rollback_dataset--parameters","title":"Parameters","text":"<p>dataset_id : str     Description for <code>dataset_id</code>.</p>"},{"location":"api/registry/api/#registry.api.Registry.rollback_dataset--examples","title":"Examples","text":"<p>from registry.api import rollback_dataset rollback_dataset(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/api.py</code> <pre><code>def rollback_dataset(self, dataset_id: str) -&gt; None:\n    \"\"\"Compute rollback dataset.\n\n    Carry out the rollback dataset operation.\n\n    Parameters\n    ----------\n    dataset_id : str\n        Description for ``dataset_id``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.api import rollback_dataset\n    &gt;&gt;&gt; rollback_dataset(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/registry/duckdb_registry/","title":"<code>registry.duckdb_registry</code>","text":"<p>Duckdb Registry utilities.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry","title":"<code>DuckDBRegistry</code>","text":"<p>Describe DuckDBRegistry.</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>class DuckDBRegistry:\n    \"\"\"Describe DuckDBRegistry.\"\"\"\n\n    def __init__(self, db_path: str) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        db_path : str\n            Description for ``db_path``.\n        \"\"\"\n\n\n        self.db_path = db_path\n        self.con = duckdb.connect(db_path, read_only=False)\n        self.con.execute(\"PRAGMA threads=14\")\n\n    def begin_dataset(self, kind: str, run_id: str) -&gt; str:\n        \"\"\"Compute begin dataset.\n\n        Carry out the begin dataset operation.\n\n        Parameters\n        ----------\n        kind : str\n            Description for ``kind``.\n        run_id : str\n            Description for ``run_id``.\n\n        Returns\n        -------\n        str\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.duckdb_registry import begin_dataset\n        &gt;&gt;&gt; result = begin_dataset(..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        dataset_id = str(uuid.uuid4())\n        self.con.execute(\n            (\n                \"INSERT INTO datasets(\"\n                \"dataset_id, kind, parquet_root, run_id, created_at\"\n                \") VALUES (?, ?, '', ?, now())\"\n            ),\n            [dataset_id, kind, run_id],\n        )\n        return dataset_id\n\n    def commit_dataset(self, dataset_id: str, parquet_root: str, rows: int) -&gt; None:\n        \"\"\"Compute commit dataset.\n\n        Carry out the commit dataset operation.\n\n        Parameters\n        ----------\n        dataset_id : str\n            Description for ``dataset_id``.\n        parquet_root : str\n            Description for ``parquet_root``.\n        rows : int\n            Description for ``rows``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.duckdb_registry import commit_dataset\n        &gt;&gt;&gt; commit_dataset(..., ..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        self.con.execute(\n            \"UPDATE datasets SET parquet_root=? WHERE dataset_id=?\", [parquet_root, dataset_id]\n        )\n\n    def rollback_dataset(self, dataset_id: str) -&gt; None:\n        \"\"\"Compute rollback dataset.\n\n        Carry out the rollback dataset operation.\n\n        Parameters\n        ----------\n        dataset_id : str\n            Description for ``dataset_id``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.duckdb_registry import rollback_dataset\n        &gt;&gt;&gt; rollback_dataset(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        self.con.execute(\"DELETE FROM datasets WHERE dataset_id=?\", [dataset_id])\n\n    def insert_run(\n        self,\n        purpose: str,\n        model_id: str | None,\n        revision: str | None,\n        config: Mapping[str, object],\n    ) -&gt; str:\n        \"\"\"Compute insert run.\n\n        Carry out the insert run operation.\n\n        Parameters\n        ----------\n        purpose : str\n            Description for ``purpose``.\n        model_id : str | None\n            Description for ``model_id``.\n        revision : str | None\n            Description for ``revision``.\n        config : collections.abc.Mapping\n            Description for ``config``.\n\n        Returns\n        -------\n        str\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.duckdb_registry import insert_run\n        &gt;&gt;&gt; result = insert_run(..., ..., ..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        run_id = str(uuid.uuid4())\n        self.con.execute(\n            (\n                \"INSERT INTO runs(\"\n                \"run_id, purpose, model_id, revision, started_at, config\"\n                \") VALUES (?, ?, ?, ?, now(), ?)\"\n            ),\n            [run_id, purpose, model_id, revision, json.dumps(config)],\n        )\n        return run_id\n\n    def close_run(self, run_id: str, success: bool, notes: str | None = None) -&gt; None:\n        \"\"\"Compute close run.\n\n        Carry out the close run operation.\n\n        Parameters\n        ----------\n        run_id : str\n            Description for ``run_id``.\n        success : bool\n            Description for ``success``.\n        notes : str | None\n            Description for ``notes``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.duckdb_registry import close_run\n        &gt;&gt;&gt; close_run(..., ..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        _ = success  # placeholder until success flag/notes are persisted\n        _ = notes\n        self.con.execute(\"UPDATE runs SET finished_at=now() WHERE run_id=?\", [run_id])\n\n    def register_documents(self, docs: list[Doc]) -&gt; None:\n        \"\"\"Compute register documents.\n\n        Carry out the register documents operation.\n\n        Parameters\n        ----------\n        docs : List[src.kgfoundry_common.models.Doc]\n            Description for ``docs``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.duckdb_registry import register_documents\n        &gt;&gt;&gt; register_documents(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        for doc in docs:\n            self.con.execute(\n                (\n                    \"INSERT OR REPLACE INTO documents(\"\n                    \"doc_id, openalex_id, doi, arxiv_id, pmcid, title, authors, \"\n                    \"pub_date, license, language, pdf_uri, source, content_hash, created_at\"\n                    \") VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, now())\"\n                ),\n                [\n                    doc.id,\n                    doc.openalex_id,\n                    doc.doi,\n                    doc.arxiv_id,\n                    doc.pmcid,\n                    doc.title,\n                    json.dumps(doc.authors),\n                    doc.pub_date,\n                    doc.license,\n                    doc.language,\n                    doc.pdf_uri,\n                    doc.source,\n                    doc.content_hash,\n                ],\n            )\n\n    def register_doctags(self, assets: list[DoctagsAsset]) -&gt; None:\n        \"\"\"Compute register doctags.\n\n        Carry out the register doctags operation.\n\n        Parameters\n        ----------\n        assets : List[src.kgfoundry_common.models.DoctagsAsset]\n            Description for ``assets``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.duckdb_registry import register_doctags\n        &gt;&gt;&gt; register_doctags(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        for asset in assets:\n            self.con.execute(\n                (\n                    \"INSERT OR REPLACE INTO doctags(\"\n                    \"doc_id, doctags_uri, pages, vlm_model, vlm_revision, avg_logprob, created_at\"\n                    \") VALUES (?, ?, ?, ?, ?, ?, now())\"\n                ),\n                [\n                    asset.doc_id,\n                    asset.doctags_uri,\n                    asset.pages,\n                    asset.vlm_model,\n                    asset.vlm_revision,\n                    asset.avg_logprob,\n                ],\n            )\n\n    def emit_event(self, event_name: str, subject_id: str, payload: Mapping[str, object]) -&gt; None:\n        \"\"\"Compute emit event.\n\n        Carry out the emit event operation.\n\n        Parameters\n        ----------\n        event_name : str\n            Description for ``event_name``.\n        subject_id : str\n            Description for ``subject_id``.\n        payload : collections.abc.Mapping\n            Description for ``payload``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.duckdb_registry import emit_event\n        &gt;&gt;&gt; emit_event(..., ..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        self.con.execute(\n            (\n                \"INSERT INTO pipeline_events(\"\n                \"event_id, event_name, subject_id, payload, created_at\"\n                \") VALUES (gen_random_uuid(), ?, ?, ?, now())\"\n            ),\n            [event_name, subject_id, json.dumps(payload)],\n        )\n\n    def incident(self, event: str, subject_id: str, error_class: str, message: str) -&gt; None:\n        \"\"\"Compute incident.\n\n        Carry out the incident operation.\n\n        Parameters\n        ----------\n        event : str\n            Description for ``event``.\n        subject_id : str\n            Description for ``subject_id``.\n        error_class : str\n            Description for ``error_class``.\n        message : str\n            Description for ``message``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.duckdb_registry import incident\n        &gt;&gt;&gt; incident(..., ..., ..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        self.con.execute(\n            (\n                \"INSERT INTO incidents(\"\n                \"id, event, subject_id, error_class, message, created_at\"\n                \") VALUES (gen_random_uuid(), ?, ?, ?, ?, now())\"\n            ),\n            [event, subject_id, error_class, message],\n        )\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.__init__","title":"<code>__init__(db_path)</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.__init__--parameters","title":"Parameters","text":"<p>db_path : str     Description for <code>db_path</code>.</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def __init__(self, db_path: str) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    db_path : str\n        Description for ``db_path``.\n    \"\"\"\n\n\n    self.db_path = db_path\n    self.con = duckdb.connect(db_path, read_only=False)\n    self.con.execute(\"PRAGMA threads=14\")\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.begin_dataset","title":"<code>begin_dataset(kind, run_id)</code>","text":"<p>Compute begin dataset.</p> <p>Carry out the begin dataset operation.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.begin_dataset--parameters","title":"Parameters","text":"<p>kind : str     Description for <code>kind</code>. run_id : str     Description for <code>run_id</code>.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.begin_dataset--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.begin_dataset--examples","title":"Examples","text":"<p>from registry.duckdb_registry import begin_dataset result = begin_dataset(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def begin_dataset(self, kind: str, run_id: str) -&gt; str:\n    \"\"\"Compute begin dataset.\n\n    Carry out the begin dataset operation.\n\n    Parameters\n    ----------\n    kind : str\n        Description for ``kind``.\n    run_id : str\n        Description for ``run_id``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.duckdb_registry import begin_dataset\n    &gt;&gt;&gt; result = begin_dataset(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    dataset_id = str(uuid.uuid4())\n    self.con.execute(\n        (\n            \"INSERT INTO datasets(\"\n            \"dataset_id, kind, parquet_root, run_id, created_at\"\n            \") VALUES (?, ?, '', ?, now())\"\n        ),\n        [dataset_id, kind, run_id],\n    )\n    return dataset_id\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.close_run","title":"<code>close_run(run_id, success, notes=None)</code>","text":"<p>Compute close run.</p> <p>Carry out the close run operation.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.close_run--parameters","title":"Parameters","text":"<p>run_id : str     Description for <code>run_id</code>. success : bool     Description for <code>success</code>. notes : str | None     Description for <code>notes</code>.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.close_run--examples","title":"Examples","text":"<p>from registry.duckdb_registry import close_run close_run(..., ..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def close_run(self, run_id: str, success: bool, notes: str | None = None) -&gt; None:\n    \"\"\"Compute close run.\n\n    Carry out the close run operation.\n\n    Parameters\n    ----------\n    run_id : str\n        Description for ``run_id``.\n    success : bool\n        Description for ``success``.\n    notes : str | None\n        Description for ``notes``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.duckdb_registry import close_run\n    &gt;&gt;&gt; close_run(..., ..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    _ = success  # placeholder until success flag/notes are persisted\n    _ = notes\n    self.con.execute(\"UPDATE runs SET finished_at=now() WHERE run_id=?\", [run_id])\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.commit_dataset","title":"<code>commit_dataset(dataset_id, parquet_root, rows)</code>","text":"<p>Compute commit dataset.</p> <p>Carry out the commit dataset operation.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.commit_dataset--parameters","title":"Parameters","text":"<p>dataset_id : str     Description for <code>dataset_id</code>. parquet_root : str     Description for <code>parquet_root</code>. rows : int     Description for <code>rows</code>.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.commit_dataset--examples","title":"Examples","text":"<p>from registry.duckdb_registry import commit_dataset commit_dataset(..., ..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def commit_dataset(self, dataset_id: str, parquet_root: str, rows: int) -&gt; None:\n    \"\"\"Compute commit dataset.\n\n    Carry out the commit dataset operation.\n\n    Parameters\n    ----------\n    dataset_id : str\n        Description for ``dataset_id``.\n    parquet_root : str\n        Description for ``parquet_root``.\n    rows : int\n        Description for ``rows``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.duckdb_registry import commit_dataset\n    &gt;&gt;&gt; commit_dataset(..., ..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    self.con.execute(\n        \"UPDATE datasets SET parquet_root=? WHERE dataset_id=?\", [parquet_root, dataset_id]\n    )\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.emit_event","title":"<code>emit_event(event_name, subject_id, payload)</code>","text":"<p>Compute emit event.</p> <p>Carry out the emit event operation.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.emit_event--parameters","title":"Parameters","text":"<p>event_name : str     Description for <code>event_name</code>. subject_id : str     Description for <code>subject_id</code>. payload : collections.abc.Mapping     Description for <code>payload</code>.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.emit_event--examples","title":"Examples","text":"<p>from registry.duckdb_registry import emit_event emit_event(..., ..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def emit_event(self, event_name: str, subject_id: str, payload: Mapping[str, object]) -&gt; None:\n    \"\"\"Compute emit event.\n\n    Carry out the emit event operation.\n\n    Parameters\n    ----------\n    event_name : str\n        Description for ``event_name``.\n    subject_id : str\n        Description for ``subject_id``.\n    payload : collections.abc.Mapping\n        Description for ``payload``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.duckdb_registry import emit_event\n    &gt;&gt;&gt; emit_event(..., ..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    self.con.execute(\n        (\n            \"INSERT INTO pipeline_events(\"\n            \"event_id, event_name, subject_id, payload, created_at\"\n            \") VALUES (gen_random_uuid(), ?, ?, ?, now())\"\n        ),\n        [event_name, subject_id, json.dumps(payload)],\n    )\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.incident","title":"<code>incident(event, subject_id, error_class, message)</code>","text":"<p>Compute incident.</p> <p>Carry out the incident operation.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.incident--parameters","title":"Parameters","text":"<p>event : str     Description for <code>event</code>. subject_id : str     Description for <code>subject_id</code>. error_class : str     Description for <code>error_class</code>. message : str     Description for <code>message</code>.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.incident--examples","title":"Examples","text":"<p>from registry.duckdb_registry import incident incident(..., ..., ..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def incident(self, event: str, subject_id: str, error_class: str, message: str) -&gt; None:\n    \"\"\"Compute incident.\n\n    Carry out the incident operation.\n\n    Parameters\n    ----------\n    event : str\n        Description for ``event``.\n    subject_id : str\n        Description for ``subject_id``.\n    error_class : str\n        Description for ``error_class``.\n    message : str\n        Description for ``message``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.duckdb_registry import incident\n    &gt;&gt;&gt; incident(..., ..., ..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    self.con.execute(\n        (\n            \"INSERT INTO incidents(\"\n            \"id, event, subject_id, error_class, message, created_at\"\n            \") VALUES (gen_random_uuid(), ?, ?, ?, ?, now())\"\n        ),\n        [event, subject_id, error_class, message],\n    )\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.insert_run","title":"<code>insert_run(purpose, model_id, revision, config)</code>","text":"<p>Compute insert run.</p> <p>Carry out the insert run operation.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.insert_run--parameters","title":"Parameters","text":"<p>purpose : str     Description for <code>purpose</code>. model_id : str | None     Description for <code>model_id</code>. revision : str | None     Description for <code>revision</code>. config : collections.abc.Mapping     Description for <code>config</code>.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.insert_run--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.insert_run--examples","title":"Examples","text":"<p>from registry.duckdb_registry import insert_run result = insert_run(..., ..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def insert_run(\n    self,\n    purpose: str,\n    model_id: str | None,\n    revision: str | None,\n    config: Mapping[str, object],\n) -&gt; str:\n    \"\"\"Compute insert run.\n\n    Carry out the insert run operation.\n\n    Parameters\n    ----------\n    purpose : str\n        Description for ``purpose``.\n    model_id : str | None\n        Description for ``model_id``.\n    revision : str | None\n        Description for ``revision``.\n    config : collections.abc.Mapping\n        Description for ``config``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.duckdb_registry import insert_run\n    &gt;&gt;&gt; result = insert_run(..., ..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    run_id = str(uuid.uuid4())\n    self.con.execute(\n        (\n            \"INSERT INTO runs(\"\n            \"run_id, purpose, model_id, revision, started_at, config\"\n            \") VALUES (?, ?, ?, ?, now(), ?)\"\n        ),\n        [run_id, purpose, model_id, revision, json.dumps(config)],\n    )\n    return run_id\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.register_doctags","title":"<code>register_doctags(assets)</code>","text":"<p>Compute register doctags.</p> <p>Carry out the register doctags operation.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.register_doctags--parameters","title":"Parameters","text":"<p>assets : List[src.kgfoundry_common.models.DoctagsAsset]     Description for <code>assets</code>.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.register_doctags--examples","title":"Examples","text":"<p>from registry.duckdb_registry import register_doctags register_doctags(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def register_doctags(self, assets: list[DoctagsAsset]) -&gt; None:\n    \"\"\"Compute register doctags.\n\n    Carry out the register doctags operation.\n\n    Parameters\n    ----------\n    assets : List[src.kgfoundry_common.models.DoctagsAsset]\n        Description for ``assets``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.duckdb_registry import register_doctags\n    &gt;&gt;&gt; register_doctags(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    for asset in assets:\n        self.con.execute(\n            (\n                \"INSERT OR REPLACE INTO doctags(\"\n                \"doc_id, doctags_uri, pages, vlm_model, vlm_revision, avg_logprob, created_at\"\n                \") VALUES (?, ?, ?, ?, ?, ?, now())\"\n            ),\n            [\n                asset.doc_id,\n                asset.doctags_uri,\n                asset.pages,\n                asset.vlm_model,\n                asset.vlm_revision,\n                asset.avg_logprob,\n            ],\n        )\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.register_documents","title":"<code>register_documents(docs)</code>","text":"<p>Compute register documents.</p> <p>Carry out the register documents operation.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.register_documents--parameters","title":"Parameters","text":"<p>docs : List[src.kgfoundry_common.models.Doc]     Description for <code>docs</code>.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.register_documents--examples","title":"Examples","text":"<p>from registry.duckdb_registry import register_documents register_documents(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def register_documents(self, docs: list[Doc]) -&gt; None:\n    \"\"\"Compute register documents.\n\n    Carry out the register documents operation.\n\n    Parameters\n    ----------\n    docs : List[src.kgfoundry_common.models.Doc]\n        Description for ``docs``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.duckdb_registry import register_documents\n    &gt;&gt;&gt; register_documents(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    for doc in docs:\n        self.con.execute(\n            (\n                \"INSERT OR REPLACE INTO documents(\"\n                \"doc_id, openalex_id, doi, arxiv_id, pmcid, title, authors, \"\n                \"pub_date, license, language, pdf_uri, source, content_hash, created_at\"\n                \") VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, now())\"\n            ),\n            [\n                doc.id,\n                doc.openalex_id,\n                doc.doi,\n                doc.arxiv_id,\n                doc.pmcid,\n                doc.title,\n                json.dumps(doc.authors),\n                doc.pub_date,\n                doc.license,\n                doc.language,\n                doc.pdf_uri,\n                doc.source,\n                doc.content_hash,\n            ],\n        )\n</code></pre>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.rollback_dataset","title":"<code>rollback_dataset(dataset_id)</code>","text":"<p>Compute rollback dataset.</p> <p>Carry out the rollback dataset operation.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.rollback_dataset--parameters","title":"Parameters","text":"<p>dataset_id : str     Description for <code>dataset_id</code>.</p>"},{"location":"api/registry/duckdb_registry/#registry.duckdb_registry.DuckDBRegistry.rollback_dataset--examples","title":"Examples","text":"<p>from registry.duckdb_registry import rollback_dataset rollback_dataset(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/duckdb_registry.py</code> <pre><code>def rollback_dataset(self, dataset_id: str) -&gt; None:\n    \"\"\"Compute rollback dataset.\n\n    Carry out the rollback dataset operation.\n\n    Parameters\n    ----------\n    dataset_id : str\n        Description for ``dataset_id``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.duckdb_registry import rollback_dataset\n    &gt;&gt;&gt; rollback_dataset(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    self.con.execute(\"DELETE FROM datasets WHERE dataset_id=?\", [dataset_id])\n</code></pre>"},{"location":"api/registry/helper/","title":"<code>registry.helper</code>","text":"<p>Helper utilities.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper","title":"<code>DuckDBRegistryHelper</code>","text":"<p>Describe DuckDBRegistryHelper.</p> Source code in <code>src/registry/helper.py</code> <pre><code>class DuckDBRegistryHelper:\n    \"\"\"Describe DuckDBRegistryHelper.\"\"\"\n\n    def __init__(self, db_path: str) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        db_path : str\n            Description for ``db_path``.\n        \"\"\"\n\n\n        self.db_path = db_path\n\n    def _con(self) -&gt; duckdb.DuckDBPyConnection:\n        \"\"\"Compute con.\n\n        Carry out the con operation.\n\n        Returns\n        -------\n        duckdb.DuckDBPyConnection\n            Description of return value.\n        \"\"\"\n        return duckdb.connect(self.db_path)\n\n    def new_run(\n        self,\n        purpose: str,\n        model_id: str | None,\n        revision: str | None,\n        config: Mapping[str, object],\n    ) -&gt; str:\n        \"\"\"Compute new run.\n\n        Carry out the new run operation.\n\n        Parameters\n        ----------\n        purpose : str\n            Description for ``purpose``.\n        model_id : str | None\n            Description for ``model_id``.\n        revision : str | None\n            Description for ``revision``.\n        config : collections.abc.Mapping\n            Description for ``config``.\n\n        Returns\n        -------\n        str\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.helper import new_run\n        &gt;&gt;&gt; result = new_run(..., ..., ..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        run_id = str(uuid.uuid4())\n        con = self._con()\n        con.execute(\n            (\n                \"INSERT INTO runs \"\n                \"(run_id,purpose,model_id,revision,started_at,config) \"\n                \"VALUES (?,?,?,?,CURRENT_TIMESTAMP,?)\"\n            ),\n            [run_id, purpose, model_id, revision, json.dumps(config)],\n        )\n        con.close()\n        return run_id\n\n    def close_run(self, run_id: str, success: bool, notes: str | None = None) -&gt; None:\n        \"\"\"Compute close run.\n\n        Carry out the close run operation.\n\n        Parameters\n        ----------\n        run_id : str\n            Description for ``run_id``.\n        success : bool\n            Description for ``success``.\n        notes : str | None\n            Description for ``notes``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.helper import close_run\n        &gt;&gt;&gt; close_run(..., ..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        con = self._con()\n        con.execute(\"UPDATE runs SET finished_at=CURRENT_TIMESTAMP WHERE run_id=?\", [run_id])\n        con.execute(\n            \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n            [\n                str(uuid.uuid4()),\n                \"RunClosed\",\n                run_id,\n                json.dumps({\"success\": success, \"notes\": notes or \"\"}),\n            ],\n        )\n        con.close()\n\n    def begin_dataset(self, kind: str, run_id: str) -&gt; str:\n        \"\"\"Compute begin dataset.\n\n        Carry out the begin dataset operation.\n\n        Parameters\n        ----------\n        kind : str\n            Description for ``kind``.\n        run_id : str\n            Description for ``run_id``.\n\n        Returns\n        -------\n        str\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.helper import begin_dataset\n        &gt;&gt;&gt; result = begin_dataset(..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        dataset_id = str(uuid.uuid4())\n        con = self._con()\n        con.execute(\n            (\n                \"INSERT INTO datasets \"\n                \"(dataset_id,kind,parquet_root,run_id,created_at) \"\n                \"VALUES (?,?,?,?,CURRENT_TIMESTAMP)\"\n            ),\n            [dataset_id, kind, \"\", run_id],\n        )\n        con.close()\n        return dataset_id\n\n    def commit_dataset(self, dataset_id: str, parquet_root: str, rows: int) -&gt; None:\n        \"\"\"Compute commit dataset.\n\n        Carry out the commit dataset operation.\n\n        Parameters\n        ----------\n        dataset_id : str\n            Description for ``dataset_id``.\n        parquet_root : str\n            Description for ``parquet_root``.\n        rows : int\n            Description for ``rows``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.helper import commit_dataset\n        &gt;&gt;&gt; commit_dataset(..., ..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        con = self._con()\n        con.execute(\n            \"UPDATE datasets SET parquet_root=? WHERE dataset_id=?\", [parquet_root, dataset_id]\n        )\n        con.execute(\n            \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n            [\n                str(uuid.uuid4()),\n                \"DatasetCommitted\",\n                dataset_id,\n                json.dumps({\"rows\": rows, \"root\": parquet_root}),\n            ],\n        )\n        con.close()\n\n    def rollback_dataset(self, dataset_id: str) -&gt; None:\n        \"\"\"Compute rollback dataset.\n\n        Carry out the rollback dataset operation.\n\n        Parameters\n        ----------\n        dataset_id : str\n            Description for ``dataset_id``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.helper import rollback_dataset\n        &gt;&gt;&gt; rollback_dataset(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        con = self._con()\n        con.execute(\"DELETE FROM datasets WHERE dataset_id=?\", [dataset_id])\n        con.execute(\n            \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n            [str(uuid.uuid4()), \"DatasetRolledBack\", dataset_id, \"{}\"],\n        )\n        con.close()\n\n    def register_documents(self, docs: list[Doc]) -&gt; None:\n        \"\"\"Compute register documents.\n\n        Carry out the register documents operation.\n\n        Parameters\n        ----------\n        docs : List[src.kgfoundry_common.models.Doc]\n            Description for ``docs``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.helper import register_documents\n        &gt;&gt;&gt; register_documents(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        con = self._con()\n        for doc in docs:\n            con.execute(\n                \"\"\"INSERT OR REPLACE INTO documents\n                (doc_id, openalex_id, doi, arxiv_id, pmcid, title, authors, pub_date, license,\n                 language, pdf_uri, source, content_hash, created_at)\n                VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,CURRENT_TIMESTAMP)\"\"\",\n                [\n                    doc.id,\n                    doc.openalex_id,\n                    doc.doi,\n                    doc.arxiv_id,\n                    doc.pmcid,\n                    doc.title,\n                    json.dumps(doc.authors),\n                    doc.pub_date,\n                    doc.license,\n                    doc.language,\n                    doc.pdf_uri,\n                    doc.source,\n                    doc.content_hash or \"\",\n                ],\n            )\n        con.close()\n\n    def register_doctags(self, assets: list[DoctagsAsset]) -&gt; None:\n        \"\"\"Compute register doctags.\n\n        Carry out the register doctags operation.\n\n        Parameters\n        ----------\n        assets : List[src.kgfoundry_common.models.DoctagsAsset]\n            Description for ``assets``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.helper import register_doctags\n        &gt;&gt;&gt; register_doctags(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        con = self._con()\n        for asset in assets:\n            con.execute(\n                \"INSERT OR REPLACE INTO doctags VALUES (?,?,?,?,?,?,CURRENT_TIMESTAMP)\",\n                [\n                    asset.doc_id,\n                    asset.doctags_uri,\n                    asset.pages,\n                    asset.vlm_model,\n                    asset.vlm_revision,\n                    asset.avg_logprob,\n                ],\n            )\n        con.close()\n\n    def emit_event(self, event_name: str, subject_id: str, payload: Mapping[str, object]) -&gt; None:\n        \"\"\"Compute emit event.\n\n        Carry out the emit event operation.\n\n        Parameters\n        ----------\n        event_name : str\n            Description for ``event_name``.\n        subject_id : str\n            Description for ``subject_id``.\n        payload : collections.abc.Mapping\n            Description for ``payload``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from registry.helper import emit_event\n        &gt;&gt;&gt; emit_event(..., ..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        con = self._con()\n        con.execute(\n            \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n            [str(uuid.uuid4()), event_name, subject_id, json.dumps(payload)],\n        )\n        con.close()\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.__init__","title":"<code>__init__(db_path)</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.__init__--parameters","title":"Parameters","text":"<p>db_path : str     Description for <code>db_path</code>.</p> Source code in <code>src/registry/helper.py</code> <pre><code>def __init__(self, db_path: str) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    db_path : str\n        Description for ``db_path``.\n    \"\"\"\n\n\n    self.db_path = db_path\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.begin_dataset","title":"<code>begin_dataset(kind, run_id)</code>","text":"<p>Compute begin dataset.</p> <p>Carry out the begin dataset operation.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.begin_dataset--parameters","title":"Parameters","text":"<p>kind : str     Description for <code>kind</code>. run_id : str     Description for <code>run_id</code>.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.begin_dataset--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.begin_dataset--examples","title":"Examples","text":"<p>from registry.helper import begin_dataset result = begin_dataset(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/registry/helper.py</code> <pre><code>def begin_dataset(self, kind: str, run_id: str) -&gt; str:\n    \"\"\"Compute begin dataset.\n\n    Carry out the begin dataset operation.\n\n    Parameters\n    ----------\n    kind : str\n        Description for ``kind``.\n    run_id : str\n        Description for ``run_id``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.helper import begin_dataset\n    &gt;&gt;&gt; result = begin_dataset(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    dataset_id = str(uuid.uuid4())\n    con = self._con()\n    con.execute(\n        (\n            \"INSERT INTO datasets \"\n            \"(dataset_id,kind,parquet_root,run_id,created_at) \"\n            \"VALUES (?,?,?,?,CURRENT_TIMESTAMP)\"\n        ),\n        [dataset_id, kind, \"\", run_id],\n    )\n    con.close()\n    return dataset_id\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.close_run","title":"<code>close_run(run_id, success, notes=None)</code>","text":"<p>Compute close run.</p> <p>Carry out the close run operation.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.close_run--parameters","title":"Parameters","text":"<p>run_id : str     Description for <code>run_id</code>. success : bool     Description for <code>success</code>. notes : str | None     Description for <code>notes</code>.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.close_run--examples","title":"Examples","text":"<p>from registry.helper import close_run close_run(..., ..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/helper.py</code> <pre><code>def close_run(self, run_id: str, success: bool, notes: str | None = None) -&gt; None:\n    \"\"\"Compute close run.\n\n    Carry out the close run operation.\n\n    Parameters\n    ----------\n    run_id : str\n        Description for ``run_id``.\n    success : bool\n        Description for ``success``.\n    notes : str | None\n        Description for ``notes``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.helper import close_run\n    &gt;&gt;&gt; close_run(..., ..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    con = self._con()\n    con.execute(\"UPDATE runs SET finished_at=CURRENT_TIMESTAMP WHERE run_id=?\", [run_id])\n    con.execute(\n        \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n        [\n            str(uuid.uuid4()),\n            \"RunClosed\",\n            run_id,\n            json.dumps({\"success\": success, \"notes\": notes or \"\"}),\n        ],\n    )\n    con.close()\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.commit_dataset","title":"<code>commit_dataset(dataset_id, parquet_root, rows)</code>","text":"<p>Compute commit dataset.</p> <p>Carry out the commit dataset operation.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.commit_dataset--parameters","title":"Parameters","text":"<p>dataset_id : str     Description for <code>dataset_id</code>. parquet_root : str     Description for <code>parquet_root</code>. rows : int     Description for <code>rows</code>.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.commit_dataset--examples","title":"Examples","text":"<p>from registry.helper import commit_dataset commit_dataset(..., ..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/helper.py</code> <pre><code>def commit_dataset(self, dataset_id: str, parquet_root: str, rows: int) -&gt; None:\n    \"\"\"Compute commit dataset.\n\n    Carry out the commit dataset operation.\n\n    Parameters\n    ----------\n    dataset_id : str\n        Description for ``dataset_id``.\n    parquet_root : str\n        Description for ``parquet_root``.\n    rows : int\n        Description for ``rows``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.helper import commit_dataset\n    &gt;&gt;&gt; commit_dataset(..., ..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    con = self._con()\n    con.execute(\n        \"UPDATE datasets SET parquet_root=? WHERE dataset_id=?\", [parquet_root, dataset_id]\n    )\n    con.execute(\n        \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n        [\n            str(uuid.uuid4()),\n            \"DatasetCommitted\",\n            dataset_id,\n            json.dumps({\"rows\": rows, \"root\": parquet_root}),\n        ],\n    )\n    con.close()\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.emit_event","title":"<code>emit_event(event_name, subject_id, payload)</code>","text":"<p>Compute emit event.</p> <p>Carry out the emit event operation.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.emit_event--parameters","title":"Parameters","text":"<p>event_name : str     Description for <code>event_name</code>. subject_id : str     Description for <code>subject_id</code>. payload : collections.abc.Mapping     Description for <code>payload</code>.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.emit_event--examples","title":"Examples","text":"<p>from registry.helper import emit_event emit_event(..., ..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/helper.py</code> <pre><code>def emit_event(self, event_name: str, subject_id: str, payload: Mapping[str, object]) -&gt; None:\n    \"\"\"Compute emit event.\n\n    Carry out the emit event operation.\n\n    Parameters\n    ----------\n    event_name : str\n        Description for ``event_name``.\n    subject_id : str\n        Description for ``subject_id``.\n    payload : collections.abc.Mapping\n        Description for ``payload``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.helper import emit_event\n    &gt;&gt;&gt; emit_event(..., ..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    con = self._con()\n    con.execute(\n        \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n        [str(uuid.uuid4()), event_name, subject_id, json.dumps(payload)],\n    )\n    con.close()\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.new_run","title":"<code>new_run(purpose, model_id, revision, config)</code>","text":"<p>Compute new run.</p> <p>Carry out the new run operation.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.new_run--parameters","title":"Parameters","text":"<p>purpose : str     Description for <code>purpose</code>. model_id : str | None     Description for <code>model_id</code>. revision : str | None     Description for <code>revision</code>. config : collections.abc.Mapping     Description for <code>config</code>.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.new_run--returns","title":"Returns","text":"<p>str     Description of return value.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.new_run--examples","title":"Examples","text":"<p>from registry.helper import new_run result = new_run(..., ..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/registry/helper.py</code> <pre><code>def new_run(\n    self,\n    purpose: str,\n    model_id: str | None,\n    revision: str | None,\n    config: Mapping[str, object],\n) -&gt; str:\n    \"\"\"Compute new run.\n\n    Carry out the new run operation.\n\n    Parameters\n    ----------\n    purpose : str\n        Description for ``purpose``.\n    model_id : str | None\n        Description for ``model_id``.\n    revision : str | None\n        Description for ``revision``.\n    config : collections.abc.Mapping\n        Description for ``config``.\n\n    Returns\n    -------\n    str\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.helper import new_run\n    &gt;&gt;&gt; result = new_run(..., ..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    run_id = str(uuid.uuid4())\n    con = self._con()\n    con.execute(\n        (\n            \"INSERT INTO runs \"\n            \"(run_id,purpose,model_id,revision,started_at,config) \"\n            \"VALUES (?,?,?,?,CURRENT_TIMESTAMP,?)\"\n        ),\n        [run_id, purpose, model_id, revision, json.dumps(config)],\n    )\n    con.close()\n    return run_id\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.register_doctags","title":"<code>register_doctags(assets)</code>","text":"<p>Compute register doctags.</p> <p>Carry out the register doctags operation.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.register_doctags--parameters","title":"Parameters","text":"<p>assets : List[src.kgfoundry_common.models.DoctagsAsset]     Description for <code>assets</code>.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.register_doctags--examples","title":"Examples","text":"<p>from registry.helper import register_doctags register_doctags(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/helper.py</code> <pre><code>def register_doctags(self, assets: list[DoctagsAsset]) -&gt; None:\n    \"\"\"Compute register doctags.\n\n    Carry out the register doctags operation.\n\n    Parameters\n    ----------\n    assets : List[src.kgfoundry_common.models.DoctagsAsset]\n        Description for ``assets``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.helper import register_doctags\n    &gt;&gt;&gt; register_doctags(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    con = self._con()\n    for asset in assets:\n        con.execute(\n            \"INSERT OR REPLACE INTO doctags VALUES (?,?,?,?,?,?,CURRENT_TIMESTAMP)\",\n            [\n                asset.doc_id,\n                asset.doctags_uri,\n                asset.pages,\n                asset.vlm_model,\n                asset.vlm_revision,\n                asset.avg_logprob,\n            ],\n        )\n    con.close()\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.register_documents","title":"<code>register_documents(docs)</code>","text":"<p>Compute register documents.</p> <p>Carry out the register documents operation.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.register_documents--parameters","title":"Parameters","text":"<p>docs : List[src.kgfoundry_common.models.Doc]     Description for <code>docs</code>.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.register_documents--examples","title":"Examples","text":"<p>from registry.helper import register_documents register_documents(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/helper.py</code> <pre><code>def register_documents(self, docs: list[Doc]) -&gt; None:\n    \"\"\"Compute register documents.\n\n    Carry out the register documents operation.\n\n    Parameters\n    ----------\n    docs : List[src.kgfoundry_common.models.Doc]\n        Description for ``docs``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.helper import register_documents\n    &gt;&gt;&gt; register_documents(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    con = self._con()\n    for doc in docs:\n        con.execute(\n            \"\"\"INSERT OR REPLACE INTO documents\n            (doc_id, openalex_id, doi, arxiv_id, pmcid, title, authors, pub_date, license,\n             language, pdf_uri, source, content_hash, created_at)\n            VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,CURRENT_TIMESTAMP)\"\"\",\n            [\n                doc.id,\n                doc.openalex_id,\n                doc.doi,\n                doc.arxiv_id,\n                doc.pmcid,\n                doc.title,\n                json.dumps(doc.authors),\n                doc.pub_date,\n                doc.license,\n                doc.language,\n                doc.pdf_uri,\n                doc.source,\n                doc.content_hash or \"\",\n            ],\n        )\n    con.close()\n</code></pre>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.rollback_dataset","title":"<code>rollback_dataset(dataset_id)</code>","text":"<p>Compute rollback dataset.</p> <p>Carry out the rollback dataset operation.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.rollback_dataset--parameters","title":"Parameters","text":"<p>dataset_id : str     Description for <code>dataset_id</code>.</p>"},{"location":"api/registry/helper/#registry.helper.DuckDBRegistryHelper.rollback_dataset--examples","title":"Examples","text":"<p>from registry.helper import rollback_dataset rollback_dataset(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/helper.py</code> <pre><code>def rollback_dataset(self, dataset_id: str) -&gt; None:\n    \"\"\"Compute rollback dataset.\n\n    Carry out the rollback dataset operation.\n\n    Parameters\n    ----------\n    dataset_id : str\n        Description for ``dataset_id``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.helper import rollback_dataset\n    &gt;&gt;&gt; rollback_dataset(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    con = self._con()\n    con.execute(\"DELETE FROM datasets WHERE dataset_id=?\", [dataset_id])\n    con.execute(\n        \"INSERT INTO pipeline_events VALUES (?,?,?,?,CURRENT_TIMESTAMP)\",\n        [str(uuid.uuid4()), \"DatasetRolledBack\", dataset_id, \"{}\"],\n    )\n    con.close()\n</code></pre>"},{"location":"api/registry/migrate/","title":"<code>registry.migrate</code>","text":"<p>Migrate utilities.</p>"},{"location":"api/registry/migrate/#registry.migrate.apply","title":"<code>apply(db, migrations_dir)</code>","text":"<p>Compute apply.</p> <p>Carry out the apply operation.</p>"},{"location":"api/registry/migrate/#registry.migrate.apply--parameters","title":"Parameters","text":"<p>db : str     Description for <code>db</code>. migrations_dir : str     Description for <code>migrations_dir</code>.</p>"},{"location":"api/registry/migrate/#registry.migrate.apply--examples","title":"Examples","text":"<p>from registry.migrate import apply apply(..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/migrate.py</code> <pre><code>def apply(db: str, migrations_dir: str) -&gt; None:\n    \"\"\"Compute apply.\n\n    Carry out the apply operation.\n\n    Parameters\n    ----------\n    db : str\n        Description for ``db``.\n    migrations_dir : str\n        Description for ``migrations_dir``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.migrate import apply\n    &gt;&gt;&gt; apply(..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    con = duckdb.connect(db)\n    for p in sorted(pathlib.Path(migrations_dir).glob(\"*.sql\")):\n        con.execute(p.read_text())\n    con.close()\n</code></pre>"},{"location":"api/registry/migrate/#registry.migrate.main","title":"<code>main()</code>","text":"<p>Compute main.</p> <p>Carry out the main operation.</p>"},{"location":"api/registry/migrate/#registry.migrate.main--examples","title":"Examples","text":"<p>from registry.migrate import main main()  # doctest: +ELLIPSIS</p> Source code in <code>src/registry/migrate.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Compute main.\n\n    Carry out the main operation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from registry.migrate import main\n    &gt;&gt;&gt; main()  # doctest: +ELLIPSIS\n    \"\"\"\n    ap = argparse.ArgumentParser()\n    sp = ap.add_subparsers(dest=\"cmd\", required=True)\n    a = sp.add_parser(\"apply\")\n    a.add_argument(\"--db\", required=True)\n    a.add_argument(\"--migrations\", required=True)\n    ns = ap.parse_args()\n    if ns.cmd == \"apply\":\n        apply(ns.db, ns.migrations)\n</code></pre>"},{"location":"api/search_api/","title":"<code>search_api</code>","text":"<p>Search Api utilities.</p>"},{"location":"api/search_api/app/","title":"<code>search_api.app</code>","text":"<p>App utilities.</p>"},{"location":"api/search_api/app/#search_api.app.apply_kg_boosts","title":"<code>apply_kg_boosts(cands, query, direct=0.08, one_hop=0.04)</code>","text":"<p>Compute apply kg boosts.</p> <p>Carry out the apply kg boosts operation.</p>"},{"location":"api/search_api/app/#search_api.app.apply_kg_boosts--parameters","title":"Parameters","text":"<p>cands : collections.abc.Mapping     Description for <code>cands</code>. query : str     Description for <code>query</code>. direct : float | None     Description for <code>direct</code>. one_hop : float | None     Description for <code>one_hop</code>.</p>"},{"location":"api/search_api/app/#search_api.app.apply_kg_boosts--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/search_api/app/#search_api.app.apply_kg_boosts--examples","title":"Examples","text":"<p>from search_api.app import apply_kg_boosts result = apply_kg_boosts(..., ..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/app.py</code> <pre><code>def apply_kg_boosts(\n    cands: dict[str, float],\n    query: str,\n    direct: float = 0.08,\n    one_hop: float = 0.04,\n) -&gt; dict[str, float]:\n    \"\"\"Compute apply kg boosts.\n\n    Carry out the apply kg boosts operation.\n\n    Parameters\n    ----------\n    cands : collections.abc.Mapping\n        Description for ``cands``.\n    query : str\n        Description for ``query``.\n    direct : float | None\n        Description for ``direct``.\n    one_hop : float | None\n        Description for ``one_hop``.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.app import apply_kg_boosts\n    &gt;&gt;&gt; result = apply_kg_boosts(..., ..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    q_concepts = set()\n    for w in query.lower().split():\n        if w.startswith(\"concept\"):\n            q_concepts.add(f\"C:{w.replace('concept', '')}\")\n    out = dict(cands)\n    for chunk_id, base in cands.items():\n        linked = set(kg.linked_concepts(chunk_id))\n        boost = 0.0\n        if linked &amp; q_concepts:\n            boost += direct\n        else:\n            for c in linked:\n                if set(kg.one_hop(c)) &amp; q_concepts:\n                    boost += one_hop\n                    break\n        out[chunk_id] = base + boost\n    return out\n</code></pre>"},{"location":"api/search_api/app/#search_api.app.auth","title":"<code>auth(authorization=Header(default=None))</code>","text":"<p>Compute auth.</p> <p>Carry out the auth operation.</p>"},{"location":"api/search_api/app/#search_api.app.auth--parameters","title":"Parameters","text":"<p>authorization : str | None     Description for <code>authorization</code>.</p>"},{"location":"api/search_api/app/#search_api.app.auth--raises","title":"Raises","text":"<p>HTTPException     Raised when validation fails.</p>"},{"location":"api/search_api/app/#search_api.app.auth--examples","title":"Examples","text":"<p>from search_api.app import auth auth(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/search_api/app.py</code> <pre><code>def auth(authorization: str | None = Header(default=None)) -&gt; None:\n    \"\"\"Compute auth.\n\n    Carry out the auth operation.\n\n    Parameters\n    ----------\n    authorization : str | None\n        Description for ``authorization``.\n\n    Raises\n    ------\n    HTTPException\n        Raised when validation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.app import auth\n    &gt;&gt;&gt; auth(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    if not API_KEYS:\n        return  # disabled in skeleton\n    if not authorization or not authorization.startswith(\"Bearer \"):\n        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n    token = authorization.split(\" \", 1)[1]\n    if token not in API_KEYS:\n        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n</code></pre>"},{"location":"api/search_api/app/#search_api.app.graph_concepts","title":"<code>graph_concepts(body, _=Depends(auth))</code>","text":"<p>Compute graph concepts.</p> <p>Carry out the graph concepts operation.</p>"},{"location":"api/search_api/app/#search_api.app.graph_concepts--parameters","title":"Parameters","text":"<p>body : collections.abc.Mapping     Description for <code>body</code>. _ : None | None     Description for <code>_</code>.</p>"},{"location":"api/search_api/app/#search_api.app.graph_concepts--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/search_api/app/#search_api.app.graph_concepts--examples","title":"Examples","text":"<p>from search_api.app import graph_concepts result = graph_concepts(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/app.py</code> <pre><code>@app.post(\"/graph/concepts\", response_model=dict)\ndef graph_concepts(body: Mapping[str, Any], _: None = Depends(auth)) -&gt; dict[str, Any]:\n    \"\"\"Compute graph concepts.\n\n    Carry out the graph concepts operation.\n\n    Parameters\n    ----------\n    body : collections.abc.Mapping\n        Description for ``body``.\n    _ : None | None\n        Description for ``_``.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.app import graph_concepts\n    &gt;&gt;&gt; result = graph_concepts(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    q = (body or {}).get(\"q\", \"\").lower()\n    # toy: return nodes that contain the query substring\n    concepts = [\n        {\"concept_id\": c, \"label\": c}\n        for c in sorted({c for cs in kg.chunk2concepts.values() for c in cs})\n        if q in c.lower()\n    ][: body.get(\"limit\", 50)]\n    return {\"concepts\": concepts}\n</code></pre>"},{"location":"api/search_api/app/#search_api.app.healthz","title":"<code>healthz()</code>","text":"<p>Compute healthz.</p> <p>Carry out the healthz operation.</p>"},{"location":"api/search_api/app/#search_api.app.healthz--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/search_api/app/#search_api.app.healthz--examples","title":"Examples","text":"<p>from search_api.app import healthz result = healthz() result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/app.py</code> <pre><code>@app.get(\"/healthz\")\ndef healthz() -&gt; dict[str, Any]:\n    \"\"\"Compute healthz.\n\n    Carry out the healthz operation.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.app import healthz\n    &gt;&gt;&gt; result = healthz()\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return {\n        \"status\": \"ok\",\n        \"components\": {\n            \"faiss\": (\"loaded\" if faiss is not None else \"missing\"),\n            \"bm25\": type(bm25).__name__,\n            \"splade\": type(splade).__name__,\n            \"vllm_embeddings\": \"mocked\",\n            \"neo4j\": \"mocked\",\n        },\n    }\n</code></pre>"},{"location":"api/search_api/app/#search_api.app.rrf_fuse","title":"<code>rrf_fuse(lists, k_rrf)</code>","text":"<p>Compute rrf fuse.</p> <p>Carry out the rrf fuse operation.</p>"},{"location":"api/search_api/app/#search_api.app.rrf_fuse--parameters","title":"Parameters","text":"<p>lists : List[List[Tuple[str, float]]]     Description for <code>lists</code>. k_rrf : int     Description for <code>k_rrf</code>.</p>"},{"location":"api/search_api/app/#search_api.app.rrf_fuse--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/search_api/app/#search_api.app.rrf_fuse--examples","title":"Examples","text":"<p>from search_api.app import rrf_fuse result = rrf_fuse(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/app.py</code> <pre><code>def rrf_fuse(lists: list[list[tuple[str, float]]], k_rrf: int) -&gt; dict[str, float]:\n    \"\"\"Compute rrf fuse.\n\n    Carry out the rrf fuse operation.\n\n    Parameters\n    ----------\n    lists : List[List[Tuple[str, float]]]\n        Description for ``lists``.\n    k_rrf : int\n        Description for ``k_rrf``.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.app import rrf_fuse\n    &gt;&gt;&gt; result = rrf_fuse(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    scores: dict[str, float] = {}\n    for hits in lists:\n        for rank, (doc_id, _score) in enumerate(hits, start=1):\n            scores[doc_id] = scores.get(doc_id, 0.0) + 1.0 / (k_rrf + rank)\n    return scores\n</code></pre>"},{"location":"api/search_api/app/#search_api.app.search","title":"<code>search(req, _=Depends(auth))</code>","text":"<p>Compute search.</p> <p>Carry out the search operation.</p>"},{"location":"api/search_api/app/#search_api.app.search--parameters","title":"Parameters","text":"<p>req : src.search_api.schemas.SearchRequest     Description for <code>req</code>. _ : None | None     Description for <code>_</code>.</p>"},{"location":"api/search_api/app/#search_api.app.search--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/search_api/app/#search_api.app.search--examples","title":"Examples","text":"<p>from search_api.app import search result = search(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/app.py</code> <pre><code>@app.post(\"/search\", response_model=dict)\ndef search(req: SearchRequest, _: None = Depends(auth)) -&gt; dict[str, Any]:\n    \"\"\"Compute search.\n\n    Carry out the search operation.\n\n    Parameters\n    ----------\n    req : src.search_api.schemas.SearchRequest\n        Description for ``req``.\n    _ : None | None\n        Description for ``_``.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.app import search\n    &gt;&gt;&gt; result = search(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    # Retrieve from each channel\n    # We don't have a query embedder here; fallback to empty or demo vector\n    dense_hits: list[tuple[str, float]] = []\n    # sparse via BM25 (preferred) and SPLADE\n    bm25_hits: list[tuple[str, float]] = []\n    if bm25:\n        try:\n            bm25_hits = bm25.search(req.query, k=CFG[\"search\"][\"sparse_candidates\"])\n        except Exception as exc:  # pragma: no cover - defensive fallback for missing indices\n            logger.warning(\"BM25 search failed, falling back to empty results: %s\", exc)\n            bm25_hits = []\n    try:\n        splade_hits = (\n            splade.search(req.query, k=CFG[\"search\"][\"sparse_candidates\"]) if splade else []\n        )\n    except Exception:\n        splade_hits = []\n\n    # RRF fusion\n    fused = rrf_fuse([dense_hits, bm25_hits, splade_hits], k_rrf=int(CFG[\"search\"][\"rrf_k\"]))\n    # KG boosts\n    boosted = apply_kg_boosts(\n        fused,\n        req.query,\n        direct=CFG[\"search\"][\"kg_boosts\"][\"direct\"],\n        one_hop=CFG[\"search\"][\"kg_boosts\"][\"one_hop\"],\n    )\n    # Rank and craft results\n    top = sorted(boosted.items(), key=lambda x: x[1], reverse=True)[: req.k]\n    results: list[dict[str, Any]] = []\n    for chunk_id, score in top:\n        # In real system we'd hydrate title/section via DuckDB; here we echo ids\n        results.append(\n            SearchResult(\n                doc_id=f\"doc-of-{chunk_id}\",\n                chunk_id=chunk_id,\n                title=f\"Title for {chunk_id}\",\n                section=\"Methods\",\n                score=float(score),\n                signals={\n                    \"rrf\": float(fused.get(chunk_id, 0.0)),\n                    \"kg_boost\": float(boosted[chunk_id] - fused.get(chunk_id, 0.0)),\n                },\n                spans={\"start_char\": 0, \"end_char\": 50},\n                concepts=[\n                    {\n                        \"concept_id\": c,\n                        \"label\": c,\n                        \"match\": (\"direct\" if c in req.query else \"nearby\"),\n                    }\n                    for c in kg.linked_concepts(chunk_id)\n                ],\n            ).model_dump()\n        )\n    return {\"results\": results}\n</code></pre>"},{"location":"api/search_api/bm25_index/","title":"<code>search_api.bm25_index</code>","text":"<p>Bm25 Index utilities.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Doc","title":"<code>BM25Doc</code>  <code>dataclass</code>","text":"<p>Describe BM25Doc.</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>@dataclass\nclass BM25Doc:\n    \"\"\"Describe BM25Doc.\"\"\"\n\n    chunk_id: str\n    doc_id: str\n    title: str\n    section: str\n    tf: dict[str, float]\n    dl: float\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index","title":"<code>BM25Index</code>","text":"<p>Describe BM25Index.</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>class BM25Index:\n    \"\"\"Describe BM25Index.\"\"\"\n\n    def __init__(self, k1: float = 0.9, b: float = 0.4) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        k1 : float | None\n            Description for ``k1``.\n        b : float | None\n            Description for ``b``.\n        \"\"\"\n\n\n        self.k1 = k1\n        self.b = b\n        self.docs: list[BM25Doc] = []\n        self.df: dict[str, int] = {}\n        self.N = 0\n        self.avgdl = 0.0\n\n    @classmethod\n    def build_from_duckdb(cls, db_path: str) -&gt; BM25Index:\n        \"\"\"Compute build from duckdb.\n\n        Carry out the build from duckdb operation.\n\n        Parameters\n        ----------\n        db_path : str\n            Description for ``db_path``.\n\n        Returns\n        -------\n        BM25Index\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_api.bm25_index import build_from_duckdb\n        &gt;&gt;&gt; result = build_from_duckdb(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        index = cls()\n        con = duckdb.connect(db_path)\n        try:\n            dataset = con.execute(\n                \"SELECT parquet_root FROM datasets \"\n                \"WHERE kind='chunks' ORDER BY created_at DESC LIMIT 1\"\n            ).fetchone()\n            if not dataset:\n                return index\n            root = dataset[0]\n            rows = con.execute(\n                f\"\"\"\n                SELECT c.chunk_id, c.doc_id, coalesce(c.section,''), c.text, coalesce(d.title,'')\n                FROM read_parquet('{root}/*/*.parquet', union_by_name=true) AS c\n                LEFT JOIN documents d ON c.doc_id = d.doc_id\n            \"\"\"\n            ).fetchall()\n        finally:\n            con.close()\n        index._build(rows)\n        return index\n\n    def _build(self, rows: Iterable[tuple[str, str, str, str, str]]) -&gt; None:\n        \"\"\"Compute build.\n\n        Carry out the build operation.\n\n        Parameters\n        ----------\n        rows : Iterable[Tuple[str, str, str, str, str]]\n            Description for ``rows``.\n        \"\"\"\n        self.docs.clear()\n        self.df.clear()\n        dl_sum = 0.0\n        for chunk_id, doc_id, section, body, title in rows:\n            tf: dict[str, float] = {}\n            for term in toks(body or \"\"):\n                tf[term] = tf.get(term, 0.0) + 1.0\n            for term in toks(title or \"\"):\n                tf[term] = tf.get(term, 0.0) + 2.0\n            for term in toks(section or \"\"):\n                tf[term] = tf.get(term, 0.0) + 1.2\n            dl = sum(tf.values())\n            self.docs.append(\n                BM25Doc(\n                    chunk_id=chunk_id,\n                    doc_id=doc_id or \"urn:doc:fixture\",\n                    title=title or \"Fixture\",\n                    section=section or \"\",\n                    tf=tf,\n                    dl=dl,\n                )\n            )\n            dl_sum += dl\n            for term in set(tf.keys()):\n                self.df[term] = self.df.get(term, 0) + 1\n        self.N = len(self.docs)\n        self.avgdl = (dl_sum / self.N) if self.N &gt; 0 else 0.0\n\n    def save(self, path: str) -&gt; None:\n        \"\"\"Compute save.\n\n        Carry out the save operation.\n\n        Parameters\n        ----------\n        path : str\n            Description for ``path``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_api.bm25_index import save\n        &gt;&gt;&gt; save(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        Path(path).parent.mkdir(parents=True, exist_ok=True)\n        with open(path, \"wb\") as handle:\n            pickle.dump(\n                {\n                    \"k1\": self.k1,\n                    \"b\": self.b,\n                    \"N\": self.N,\n                    \"avgdl\": self.avgdl,\n                    \"df\": self.df,\n                    \"docs\": self.docs,\n                },\n                handle,\n            )\n\n    @classmethod\n    def load(cls, path: str) -&gt; BM25Index:\n        \"\"\"Compute load.\n\n        Carry out the load operation.\n\n        Parameters\n        ----------\n        path : str\n            Description for ``path``.\n\n        Returns\n        -------\n        BM25Index\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_api.bm25_index import load\n        &gt;&gt;&gt; result = load(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        with open(path, \"rb\") as handle:\n            payload = pickle.load(handle)\n        index = cls(payload.get(\"k1\", 0.9), payload.get(\"b\", 0.4))\n        index.N = payload[\"N\"]\n        index.avgdl = payload[\"avgdl\"]\n        index.df = payload[\"df\"]\n        index.docs = payload[\"docs\"]\n        return index\n\n    def _idf(self, term: str) -&gt; float:\n        \"\"\"Compute idf.\n\n        Carry out the idf operation.\n\n        Parameters\n        ----------\n        term : str\n            Description for ``term``.\n\n        Returns\n        -------\n        float\n            Description of return value.\n        \"\"\"\n        df = self.df.get(term, 0)\n        if self.N == 0 or df == 0:\n            return 0.0\n        return math.log((self.N - df + 0.5) / (df + 0.5) + 1.0)\n\n    def search(self, query: str, k: int = 10) -&gt; list[tuple[int, float]]:\n        \"\"\"Compute search.\n\n        Carry out the search operation.\n\n        Parameters\n        ----------\n        query : str\n            Description for ``query``.\n        k : int | None\n            Description for ``k``.\n\n        Returns\n        -------\n        List[Tuple[int, float]]\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_api.bm25_index import search\n        &gt;&gt;&gt; result = search(..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        if self.N == 0:\n            return []\n        terms = toks(query)\n        scores = [0.0] * self.N\n        for i, doc in enumerate(self.docs):\n            score = 0.0\n            for term in terms:\n                tf = doc.tf.get(term, 0.0)\n                if tf &lt;= 0.0:\n                    continue\n                idf = self._idf(term)\n                denom = tf + self.k1 * (1.0 - self.b + self.b * (doc.dl / (self.avgdl or 1.0)))\n                score += idf * ((tf * (self.k1 + 1.0)) / denom)\n            scores[i] = score\n        ranked = sorted(enumerate(scores), key=lambda item: item[1], reverse=True)\n        return [(index, score) for index, score in ranked[:k] if score &gt; 0.0]\n\n    def doc(self, index: int) -&gt; BM25Doc:\n        \"\"\"Compute doc.\n\n        Carry out the doc operation.\n\n        Parameters\n        ----------\n        index : int\n            Description for ``index``.\n\n        Returns\n        -------\n        BM25Doc\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_api.bm25_index import doc\n        &gt;&gt;&gt; result = doc(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        return self.docs[index]\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.__init__","title":"<code>__init__(k1=0.9, b=0.4)</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.__init__--parameters","title":"Parameters","text":"<p>k1 : float | None     Description for <code>k1</code>. b : float | None     Description for <code>b</code>.</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>def __init__(self, k1: float = 0.9, b: float = 0.4) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    k1 : float | None\n        Description for ``k1``.\n    b : float | None\n        Description for ``b``.\n    \"\"\"\n\n\n    self.k1 = k1\n    self.b = b\n    self.docs: list[BM25Doc] = []\n    self.df: dict[str, int] = {}\n    self.N = 0\n    self.avgdl = 0.0\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.build_from_duckdb","title":"<code>build_from_duckdb(db_path)</code>  <code>classmethod</code>","text":"<p>Compute build from duckdb.</p> <p>Carry out the build from duckdb operation.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.build_from_duckdb--parameters","title":"Parameters","text":"<p>db_path : str     Description for <code>db_path</code>.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.build_from_duckdb--returns","title":"Returns","text":"<p>BM25Index     Description of return value.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.build_from_duckdb--examples","title":"Examples","text":"<p>from search_api.bm25_index import build_from_duckdb result = build_from_duckdb(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>@classmethod\ndef build_from_duckdb(cls, db_path: str) -&gt; BM25Index:\n    \"\"\"Compute build from duckdb.\n\n    Carry out the build from duckdb operation.\n\n    Parameters\n    ----------\n    db_path : str\n        Description for ``db_path``.\n\n    Returns\n    -------\n    BM25Index\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.bm25_index import build_from_duckdb\n    &gt;&gt;&gt; result = build_from_duckdb(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    index = cls()\n    con = duckdb.connect(db_path)\n    try:\n        dataset = con.execute(\n            \"SELECT parquet_root FROM datasets \"\n            \"WHERE kind='chunks' ORDER BY created_at DESC LIMIT 1\"\n        ).fetchone()\n        if not dataset:\n            return index\n        root = dataset[0]\n        rows = con.execute(\n            f\"\"\"\n            SELECT c.chunk_id, c.doc_id, coalesce(c.section,''), c.text, coalesce(d.title,'')\n            FROM read_parquet('{root}/*/*.parquet', union_by_name=true) AS c\n            LEFT JOIN documents d ON c.doc_id = d.doc_id\n        \"\"\"\n        ).fetchall()\n    finally:\n        con.close()\n    index._build(rows)\n    return index\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.doc","title":"<code>doc(index)</code>","text":"<p>Compute doc.</p> <p>Carry out the doc operation.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.doc--parameters","title":"Parameters","text":"<p>index : int     Description for <code>index</code>.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.doc--returns","title":"Returns","text":"<p>BM25Doc     Description of return value.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.doc--examples","title":"Examples","text":"<p>from search_api.bm25_index import doc result = doc(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>def doc(self, index: int) -&gt; BM25Doc:\n    \"\"\"Compute doc.\n\n    Carry out the doc operation.\n\n    Parameters\n    ----------\n    index : int\n        Description for ``index``.\n\n    Returns\n    -------\n    BM25Doc\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.bm25_index import doc\n    &gt;&gt;&gt; result = doc(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return self.docs[index]\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Compute load.</p> <p>Carry out the load operation.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.load--parameters","title":"Parameters","text":"<p>path : str     Description for <code>path</code>.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.load--returns","title":"Returns","text":"<p>BM25Index     Description of return value.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.load--examples","title":"Examples","text":"<p>from search_api.bm25_index import load result = load(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>@classmethod\ndef load(cls, path: str) -&gt; BM25Index:\n    \"\"\"Compute load.\n\n    Carry out the load operation.\n\n    Parameters\n    ----------\n    path : str\n        Description for ``path``.\n\n    Returns\n    -------\n    BM25Index\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.bm25_index import load\n    &gt;&gt;&gt; result = load(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    with open(path, \"rb\") as handle:\n        payload = pickle.load(handle)\n    index = cls(payload.get(\"k1\", 0.9), payload.get(\"b\", 0.4))\n    index.N = payload[\"N\"]\n    index.avgdl = payload[\"avgdl\"]\n    index.df = payload[\"df\"]\n    index.docs = payload[\"docs\"]\n    return index\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.save","title":"<code>save(path)</code>","text":"<p>Compute save.</p> <p>Carry out the save operation.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.save--parameters","title":"Parameters","text":"<p>path : str     Description for <code>path</code>.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.save--examples","title":"Examples","text":"<p>from search_api.bm25_index import save save(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"Compute save.\n\n    Carry out the save operation.\n\n    Parameters\n    ----------\n    path : str\n        Description for ``path``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.bm25_index import save\n    &gt;&gt;&gt; save(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    Path(path).parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"wb\") as handle:\n        pickle.dump(\n            {\n                \"k1\": self.k1,\n                \"b\": self.b,\n                \"N\": self.N,\n                \"avgdl\": self.avgdl,\n                \"df\": self.df,\n                \"docs\": self.docs,\n            },\n            handle,\n        )\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.search","title":"<code>search(query, k=10)</code>","text":"<p>Compute search.</p> <p>Carry out the search operation.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.search--parameters","title":"Parameters","text":"<p>query : str     Description for <code>query</code>. k : int | None     Description for <code>k</code>.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.search--returns","title":"Returns","text":"<p>List[Tuple[int, float]]     Description of return value.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.BM25Index.search--examples","title":"Examples","text":"<p>from search_api.bm25_index import search result = search(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>def search(self, query: str, k: int = 10) -&gt; list[tuple[int, float]]:\n    \"\"\"Compute search.\n\n    Carry out the search operation.\n\n    Parameters\n    ----------\n    query : str\n        Description for ``query``.\n    k : int | None\n        Description for ``k``.\n\n    Returns\n    -------\n    List[Tuple[int, float]]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.bm25_index import search\n    &gt;&gt;&gt; result = search(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    if self.N == 0:\n        return []\n    terms = toks(query)\n    scores = [0.0] * self.N\n    for i, doc in enumerate(self.docs):\n        score = 0.0\n        for term in terms:\n            tf = doc.tf.get(term, 0.0)\n            if tf &lt;= 0.0:\n                continue\n            idf = self._idf(term)\n            denom = tf + self.k1 * (1.0 - self.b + self.b * (doc.dl / (self.avgdl or 1.0)))\n            score += idf * ((tf * (self.k1 + 1.0)) / denom)\n        scores[i] = score\n    ranked = sorted(enumerate(scores), key=lambda item: item[1], reverse=True)\n    return [(index, score) for index, score in ranked[:k] if score &gt; 0.0]\n</code></pre>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.toks","title":"<code>toks(text)</code>","text":"<p>Compute toks.</p> <p>Carry out the toks operation.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.toks--parameters","title":"Parameters","text":"<p>text : str     Description for <code>text</code>.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.toks--returns","title":"Returns","text":"<p>List[str]     Description of return value.</p>"},{"location":"api/search_api/bm25_index/#search_api.bm25_index.toks--examples","title":"Examples","text":"<p>from search_api.bm25_index import toks result = toks(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/bm25_index.py</code> <pre><code>def toks(text: str) -&gt; list[str]:\n    \"\"\"Compute toks.\n\n    Carry out the toks operation.\n\n    Parameters\n    ----------\n    text : str\n        Description for ``text``.\n\n    Returns\n    -------\n    List[str]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.bm25_index import toks\n    &gt;&gt;&gt; result = toks(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return [token.lower() for token in TOKEN_RE.findall(text or \"\")]\n</code></pre>"},{"location":"api/search_api/faiss_adapter/","title":"<code>search_api.faiss_adapter</code>","text":"<p>Faiss Adapter utilities.</p>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.DenseVecs","title":"<code>DenseVecs</code>  <code>dataclass</code>","text":"<p>Describe DenseVecs.</p> Source code in <code>src/search_api/faiss_adapter.py</code> <pre><code>@dataclass\nclass DenseVecs:\n    \"\"\"Describe DenseVecs.\"\"\"\n\n    ids: list[str]\n    mat: VecArray\n</code></pre>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter","title":"<code>FaissAdapter</code>","text":"<p>Describe FaissAdapter.</p> Source code in <code>src/search_api/faiss_adapter.py</code> <pre><code>class FaissAdapter:\n    \"\"\"Describe FaissAdapter.\"\"\"\n\n    def __init__(\n        self,\n        db_path: str,\n        factory: str = \"OPQ64,IVF8192,PQ64\",\n        metric: str = \"ip\",\n    ) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        db_path : str\n            Description for ``db_path``.\n        factory : str | None\n            Description for ``factory``.\n        metric : str | None\n            Description for ``metric``.\n        \"\"\"\n\n\n        self.db_path = db_path\n        self.factory = factory\n        self.metric = metric\n        self.index: Any | None = None\n        self.idmap: list[str] | None = None\n        self.vecs: DenseVecs | None = None\n\n    def _load_dense_parquet(self) -&gt; DenseVecs:\n        \"\"\"Compute load dense parquet.\n\n        Carry out the load dense parquet operation.\n\n        Returns\n        -------\n        DenseVecs\n            Description of return value.\n\n        Raises\n        ------\n        RuntimeError\n            Raised when validation fails.\n        \"\"\"\n        if not Path(self.db_path).exists():\n            message = \"DuckDB registry not found\"\n            raise RuntimeError(message)\n        con = duckdb.connect(self.db_path)\n        try:\n            dense_run = con.execute(\n                \"SELECT parquet_root, dim FROM dense_runs ORDER BY created_at DESC LIMIT 1\"\n            ).fetchone()\n            if not dense_run:\n                message = \"No dense_runs found\"\n                raise RuntimeError(message)\n            root = dense_run[0]\n            rows = con.execute(\n                f\"\"\"\n                SELECT chunk_id, vector\n                FROM read_parquet('{root}/*/*.parquet', union_by_name=true)\n            \"\"\"\n            ).fetchall()\n        finally:\n            con.close()\n        ids = [row[0] for row in rows]\n        mat = cast(VecArray, np.stack([np.asarray(row[1], dtype=np.float32) for row in rows]))\n        norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-9\n        normalized = cast(VecArray, mat / norms)\n        return DenseVecs(ids=ids, mat=normalized)\n\n    def build(self) -&gt; None:\n        \"\"\"Compute build.\n\n        Carry out the build operation.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_api.faiss_adapter import build\n        &gt;&gt;&gt; build()  # doctest: +ELLIPSIS\n        \"\"\"\n        vectors = self._load_dense_parquet()\n        self.vecs = vectors\n        if not HAVE_FAISS:\n            return\n        dimension = vectors.mat.shape[1]\n        metric_type = faiss.METRIC_INNER_PRODUCT if self.metric == \"ip\" else faiss.METRIC_L2\n        cpu = faiss.index_factory(dimension, self.factory, metric_type)\n        cpu = faiss.IndexIDMap2(cpu)\n        train = vectors.mat[: min(100000, vectors.mat.shape[0])].copy()\n        faiss.normalize_L2(train)\n        cpu.train(train)\n        ids64 = cast(IndexArray, np.arange(vectors.mat.shape[0], dtype=np.int64))\n        cpu.add_with_ids(vectors.mat, ids64)\n        resources = faiss.StandardGpuResources()\n        options = faiss.GpuClonerOptions()\n        options.use_cuvs = True\n        try:\n            self.index = faiss.index_cpu_to_gpu(resources, 0, cpu, options)\n        except Exception:  # pragma: no cover - fallback path without cuVS\n            options.use_cuvs = False\n            self.index = faiss.index_cpu_to_gpu(resources, 0, cpu, options)\n        self.idmap = vectors.ids\n\n    def load_or_build(self, cpu_index_path: str | None = None) -&gt; None:\n        \"\"\"Compute load or build.\n\n        Carry out the load or build operation.\n\n        Parameters\n        ----------\n        cpu_index_path : str | None\n            Description for ``cpu_index_path``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_api.faiss_adapter import load_or_build\n        &gt;&gt;&gt; load_or_build(...)  # doctest: +ELLIPSIS\n        \"\"\"\n        try:\n            if HAVE_FAISS and cpu_index_path and Path(cpu_index_path).exists():\n                cpu = faiss.read_index(cpu_index_path)\n                resources = faiss.StandardGpuResources()\n                options = faiss.GpuClonerOptions()\n                options.use_cuvs = True\n                try:\n                    self.index = faiss.index_cpu_to_gpu(resources, 0, cpu, options)\n                except Exception:  # pragma: no cover - fallback path without cuVS\n                    options.use_cuvs = False\n                    self.index = faiss.index_cpu_to_gpu(resources, 0, cpu, options)\n                dense = self._load_dense_parquet()\n                self.vecs = dense\n                self.idmap = dense.ids\n                return\n        except Exception:\n            pass\n        self.build()\n\n    def search(self, qvec: VecArray, k: int = 10) -&gt; list[tuple[str, float]]:\n        \"\"\"Compute search.\n\n        Carry out the search operation.\n\n        Parameters\n        ----------\n        qvec : src.search_api.faiss_adapter.VecArray\n            Description for ``qvec``.\n        k : int | None\n            Description for ``k``.\n\n        Returns\n        -------\n        List[Tuple[str, float]]\n            Description of return value.\n\n        Raises\n        ------\n        RuntimeError\n            Raised when validation fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_api.faiss_adapter import search\n        &gt;&gt;&gt; result = search(..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        if self.vecs is None and self.index is None:\n            return []\n        if HAVE_FAISS and self.index is not None:\n            if self.idmap is None:\n                message = \"ID mapping not loaded for FAISS index\"\n                raise RuntimeError(message)\n            query = cast(VecArray, np.asarray(qvec[None, :], dtype=np.float32, order=\"C\"))\n            distances, indices = self.index.search(query, k)\n            results: list[tuple[str, float]] = []\n            for idx, score in zip(indices[0], distances[0], strict=False):\n                if idx &lt; 0:\n                    continue\n                results.append((self.idmap[int(idx)], float(score)))\n            return results\n        if self.vecs is None:\n            message = \"Dense vectors not loaded\"\n            raise RuntimeError(message)\n        matrix = self.vecs.mat\n        query = cast(VecArray, np.asarray(qvec, dtype=np.float32, order=\"C\"))\n        query /= np.linalg.norm(query) + 1e-9\n        sims = matrix @ query\n        topk = np.argsort(-sims)[:k]\n        ids = self.vecs.ids\n        return [(ids[i], float(sims[i])) for i in topk]\n</code></pre>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.__init__","title":"<code>__init__(db_path, factory='OPQ64,IVF8192,PQ64', metric='ip')</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.__init__--parameters","title":"Parameters","text":"<p>db_path : str     Description for <code>db_path</code>. factory : str | None     Description for <code>factory</code>. metric : str | None     Description for <code>metric</code>.</p> Source code in <code>src/search_api/faiss_adapter.py</code> <pre><code>def __init__(\n    self,\n    db_path: str,\n    factory: str = \"OPQ64,IVF8192,PQ64\",\n    metric: str = \"ip\",\n) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    db_path : str\n        Description for ``db_path``.\n    factory : str | None\n        Description for ``factory``.\n    metric : str | None\n        Description for ``metric``.\n    \"\"\"\n\n\n    self.db_path = db_path\n    self.factory = factory\n    self.metric = metric\n    self.index: Any | None = None\n    self.idmap: list[str] | None = None\n    self.vecs: DenseVecs | None = None\n</code></pre>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.build","title":"<code>build()</code>","text":"<p>Compute build.</p> <p>Carry out the build operation.</p>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.build--examples","title":"Examples","text":"<p>from search_api.faiss_adapter import build build()  # doctest: +ELLIPSIS</p> Source code in <code>src/search_api/faiss_adapter.py</code> <pre><code>def build(self) -&gt; None:\n    \"\"\"Compute build.\n\n    Carry out the build operation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.faiss_adapter import build\n    &gt;&gt;&gt; build()  # doctest: +ELLIPSIS\n    \"\"\"\n    vectors = self._load_dense_parquet()\n    self.vecs = vectors\n    if not HAVE_FAISS:\n        return\n    dimension = vectors.mat.shape[1]\n    metric_type = faiss.METRIC_INNER_PRODUCT if self.metric == \"ip\" else faiss.METRIC_L2\n    cpu = faiss.index_factory(dimension, self.factory, metric_type)\n    cpu = faiss.IndexIDMap2(cpu)\n    train = vectors.mat[: min(100000, vectors.mat.shape[0])].copy()\n    faiss.normalize_L2(train)\n    cpu.train(train)\n    ids64 = cast(IndexArray, np.arange(vectors.mat.shape[0], dtype=np.int64))\n    cpu.add_with_ids(vectors.mat, ids64)\n    resources = faiss.StandardGpuResources()\n    options = faiss.GpuClonerOptions()\n    options.use_cuvs = True\n    try:\n        self.index = faiss.index_cpu_to_gpu(resources, 0, cpu, options)\n    except Exception:  # pragma: no cover - fallback path without cuVS\n        options.use_cuvs = False\n        self.index = faiss.index_cpu_to_gpu(resources, 0, cpu, options)\n    self.idmap = vectors.ids\n</code></pre>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.load_or_build","title":"<code>load_or_build(cpu_index_path=None)</code>","text":"<p>Compute load or build.</p> <p>Carry out the load or build operation.</p>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.load_or_build--parameters","title":"Parameters","text":"<p>cpu_index_path : str | None     Description for <code>cpu_index_path</code>.</p>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.load_or_build--examples","title":"Examples","text":"<p>from search_api.faiss_adapter import load_or_build load_or_build(...)  # doctest: +ELLIPSIS</p> Source code in <code>src/search_api/faiss_adapter.py</code> <pre><code>def load_or_build(self, cpu_index_path: str | None = None) -&gt; None:\n    \"\"\"Compute load or build.\n\n    Carry out the load or build operation.\n\n    Parameters\n    ----------\n    cpu_index_path : str | None\n        Description for ``cpu_index_path``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.faiss_adapter import load_or_build\n    &gt;&gt;&gt; load_or_build(...)  # doctest: +ELLIPSIS\n    \"\"\"\n    try:\n        if HAVE_FAISS and cpu_index_path and Path(cpu_index_path).exists():\n            cpu = faiss.read_index(cpu_index_path)\n            resources = faiss.StandardGpuResources()\n            options = faiss.GpuClonerOptions()\n            options.use_cuvs = True\n            try:\n                self.index = faiss.index_cpu_to_gpu(resources, 0, cpu, options)\n            except Exception:  # pragma: no cover - fallback path without cuVS\n                options.use_cuvs = False\n                self.index = faiss.index_cpu_to_gpu(resources, 0, cpu, options)\n            dense = self._load_dense_parquet()\n            self.vecs = dense\n            self.idmap = dense.ids\n            return\n    except Exception:\n        pass\n    self.build()\n</code></pre>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.search","title":"<code>search(qvec, k=10)</code>","text":"<p>Compute search.</p> <p>Carry out the search operation.</p>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.search--parameters","title":"Parameters","text":"<p>qvec : src.search_api.faiss_adapter.VecArray     Description for <code>qvec</code>. k : int | None     Description for <code>k</code>.</p>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.search--returns","title":"Returns","text":"<p>List[Tuple[str, float]]     Description of return value.</p>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.search--raises","title":"Raises","text":"<p>RuntimeError     Raised when validation fails.</p>"},{"location":"api/search_api/faiss_adapter/#search_api.faiss_adapter.FaissAdapter.search--examples","title":"Examples","text":"<p>from search_api.faiss_adapter import search result = search(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/faiss_adapter.py</code> <pre><code>def search(self, qvec: VecArray, k: int = 10) -&gt; list[tuple[str, float]]:\n    \"\"\"Compute search.\n\n    Carry out the search operation.\n\n    Parameters\n    ----------\n    qvec : src.search_api.faiss_adapter.VecArray\n        Description for ``qvec``.\n    k : int | None\n        Description for ``k``.\n\n    Returns\n    -------\n    List[Tuple[str, float]]\n        Description of return value.\n\n    Raises\n    ------\n    RuntimeError\n        Raised when validation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.faiss_adapter import search\n    &gt;&gt;&gt; result = search(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    if self.vecs is None and self.index is None:\n        return []\n    if HAVE_FAISS and self.index is not None:\n        if self.idmap is None:\n            message = \"ID mapping not loaded for FAISS index\"\n            raise RuntimeError(message)\n        query = cast(VecArray, np.asarray(qvec[None, :], dtype=np.float32, order=\"C\"))\n        distances, indices = self.index.search(query, k)\n        results: list[tuple[str, float]] = []\n        for idx, score in zip(indices[0], distances[0], strict=False):\n            if idx &lt; 0:\n                continue\n            results.append((self.idmap[int(idx)], float(score)))\n        return results\n    if self.vecs is None:\n        message = \"Dense vectors not loaded\"\n        raise RuntimeError(message)\n    matrix = self.vecs.mat\n    query = cast(VecArray, np.asarray(qvec, dtype=np.float32, order=\"C\"))\n    query /= np.linalg.norm(query) + 1e-9\n    sims = matrix @ query\n    topk = np.argsort(-sims)[:k]\n    ids = self.vecs.ids\n    return [(ids[i], float(sims[i])) for i in topk]\n</code></pre>"},{"location":"api/search_api/fixture_index/","title":"<code>search_api.fixture_index</code>","text":"<p>Fixture Index utilities.</p>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureDoc","title":"<code>FixtureDoc</code>  <code>dataclass</code>","text":"<p>Describe FixtureDoc.</p> Source code in <code>src/search_api/fixture_index.py</code> <pre><code>@dataclass\nclass FixtureDoc:\n    \"\"\"Describe FixtureDoc.\"\"\"\n\n    chunk_id: str\n    doc_id: str\n    title: str\n    section: str\n    text: str\n</code></pre>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex","title":"<code>FixtureIndex</code>","text":"<p>Describe FixtureIndex.</p> Source code in <code>src/search_api/fixture_index.py</code> <pre><code>class FixtureIndex:\n    \"\"\"Describe FixtureIndex.\"\"\"\n\n    def __init__(self, root: str = \"/data\", db_path: str = \"/data/catalog/catalog.duckdb\") -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        root : str | None\n            Description for ``root``.\n        db_path : str | None\n            Description for ``db_path``.\n        \"\"\"\n\n\n        self.root = Path(root)\n        self.db_path = db_path\n        self.docs: list[FixtureDoc] = []\n        self.df: dict[str, int] = {}\n        self.tf: list[dict[str, int]] = []\n        self._load_from_duckdb()\n\n    def _load_from_duckdb(self) -&gt; None:\n        \"\"\"Compute load from duckdb.\n\n        Carry out the load from duckdb operation.\n        \"\"\"\n        if not Path(self.db_path).exists():\n            return\n        con = duckdb.connect(self.db_path)\n        try:\n            dataset = con.execute(\n                \"\"\"\n              SELECT parquet_root FROM datasets\n              WHERE kind='chunks'\n              ORDER BY created_at DESC\n              LIMIT 1\n            \"\"\"\n            ).fetchone()\n            if not dataset:\n                return\n            root = dataset[0]\n            rows = con.execute(\n                f\"\"\"\n                SELECT c.chunk_id, c.doc_id, coalesce(c.section,''), c.text,\n                       coalesce(d.title,'') AS title\n                FROM read_parquet('{root}/*/*.parquet', union_by_name=true) AS c\n                LEFT JOIN documents d ON c.doc_id = d.doc_id\n            \"\"\"\n            ).fetchall()\n        finally:\n            con.close()\n\n        for chunk_id, doc_id, section, text, title in rows:\n            self.docs.append(\n                FixtureDoc(\n                    chunk_id=chunk_id,\n                    doc_id=doc_id or \"urn:doc:fixture\",\n                    title=title or \"Fixture\",\n                    section=section or \"\",\n                    text=text or \"\",\n                )\n            )\n\n        self._build_lex()\n\n    def _build_lex(self) -&gt; None:\n        \"\"\"Compute build lex.\n\n        Carry out the build lex operation.\n        \"\"\"\n        self.tf.clear()\n        self.df.clear()\n        for doc in self.docs:\n            tokens = tokenize(doc.text)\n            tf_counts: dict[str, int] = {}\n            for token in tokens:\n                tf_counts[token] = tf_counts.get(token, 0) + 1\n            self.tf.append(tf_counts)\n            for token in set(tokens):\n                self.df[token] = self.df.get(token, 0) + 1\n        self.N = len(self.docs)\n\n    def search(self, query: str, k: int = 10) -&gt; list[tuple[int, float]]:\n        \"\"\"Compute search.\n\n        Carry out the search operation.\n\n        Parameters\n        ----------\n        query : str\n            Description for ``query``.\n        k : int | None\n            Description for ``k``.\n\n        Returns\n        -------\n        List[Tuple[int, float]]\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_api.fixture_index import search\n        &gt;&gt;&gt; result = search(..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        if getattr(self, \"N\", 0) == 0:\n            return []\n        qtoks = tokenize(query)\n        if not qtoks:\n            return []\n        scores = [0.0] * self.N\n        for i, tf in enumerate(self.tf):\n            score = 0.0\n            for token in qtoks:\n                if token not in self.df:\n                    continue\n                idf = math.log((self.N + 1) / (self.df[token] + 0.5) + 1.0)\n                score += idf * tf.get(token, 0)\n            scores[i] = score\n        ranked = sorted(enumerate(scores), key=lambda item: item[1], reverse=True)\n        return [(index, score) for index, score in ranked[:k] if score &gt; 0.0]\n\n    def doc(self, index: int) -&gt; FixtureDoc:\n        \"\"\"Compute doc.\n\n        Carry out the doc operation.\n\n        Parameters\n        ----------\n        index : int\n            Description for ``index``.\n\n        Returns\n        -------\n        src.search_api.fixture_index.FixtureDoc\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_api.fixture_index import doc\n        &gt;&gt;&gt; result = doc(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        return self.docs[index]\n</code></pre>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex.__init__","title":"<code>__init__(root='/data', db_path='/data/catalog/catalog.duckdb')</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex.__init__--parameters","title":"Parameters","text":"<p>root : str | None     Description for <code>root</code>. db_path : str | None     Description for <code>db_path</code>.</p> Source code in <code>src/search_api/fixture_index.py</code> <pre><code>def __init__(self, root: str = \"/data\", db_path: str = \"/data/catalog/catalog.duckdb\") -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    root : str | None\n        Description for ``root``.\n    db_path : str | None\n        Description for ``db_path``.\n    \"\"\"\n\n\n    self.root = Path(root)\n    self.db_path = db_path\n    self.docs: list[FixtureDoc] = []\n    self.df: dict[str, int] = {}\n    self.tf: list[dict[str, int]] = []\n    self._load_from_duckdb()\n</code></pre>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex.doc","title":"<code>doc(index)</code>","text":"<p>Compute doc.</p> <p>Carry out the doc operation.</p>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex.doc--parameters","title":"Parameters","text":"<p>index : int     Description for <code>index</code>.</p>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex.doc--returns","title":"Returns","text":"<p>src.search_api.fixture_index.FixtureDoc     Description of return value.</p>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex.doc--examples","title":"Examples","text":"<p>from search_api.fixture_index import doc result = doc(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/fixture_index.py</code> <pre><code>def doc(self, index: int) -&gt; FixtureDoc:\n    \"\"\"Compute doc.\n\n    Carry out the doc operation.\n\n    Parameters\n    ----------\n    index : int\n        Description for ``index``.\n\n    Returns\n    -------\n    src.search_api.fixture_index.FixtureDoc\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.fixture_index import doc\n    &gt;&gt;&gt; result = doc(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return self.docs[index]\n</code></pre>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex.search","title":"<code>search(query, k=10)</code>","text":"<p>Compute search.</p> <p>Carry out the search operation.</p>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex.search--parameters","title":"Parameters","text":"<p>query : str     Description for <code>query</code>. k : int | None     Description for <code>k</code>.</p>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex.search--returns","title":"Returns","text":"<p>List[Tuple[int, float]]     Description of return value.</p>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.FixtureIndex.search--examples","title":"Examples","text":"<p>from search_api.fixture_index import search result = search(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/fixture_index.py</code> <pre><code>def search(self, query: str, k: int = 10) -&gt; list[tuple[int, float]]:\n    \"\"\"Compute search.\n\n    Carry out the search operation.\n\n    Parameters\n    ----------\n    query : str\n        Description for ``query``.\n    k : int | None\n        Description for ``k``.\n\n    Returns\n    -------\n    List[Tuple[int, float]]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.fixture_index import search\n    &gt;&gt;&gt; result = search(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    if getattr(self, \"N\", 0) == 0:\n        return []\n    qtoks = tokenize(query)\n    if not qtoks:\n        return []\n    scores = [0.0] * self.N\n    for i, tf in enumerate(self.tf):\n        score = 0.0\n        for token in qtoks:\n            if token not in self.df:\n                continue\n            idf = math.log((self.N + 1) / (self.df[token] + 0.5) + 1.0)\n            score += idf * tf.get(token, 0)\n        scores[i] = score\n    ranked = sorted(enumerate(scores), key=lambda item: item[1], reverse=True)\n    return [(index, score) for index, score in ranked[:k] if score &gt; 0.0]\n</code></pre>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.tokenize","title":"<code>tokenize(text)</code>","text":"<p>Compute tokenize.</p> <p>Carry out the tokenize operation.</p>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.tokenize--parameters","title":"Parameters","text":"<p>text : str     Description for <code>text</code>.</p>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.tokenize--returns","title":"Returns","text":"<p>List[str]     Description of return value.</p>"},{"location":"api/search_api/fixture_index/#search_api.fixture_index.tokenize--examples","title":"Examples","text":"<p>from search_api.fixture_index import tokenize result = tokenize(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/fixture_index.py</code> <pre><code>def tokenize(text: str) -&gt; list[str]:\n    \"\"\"Compute tokenize.\n\n    Carry out the tokenize operation.\n\n    Parameters\n    ----------\n    text : str\n        Description for ``text``.\n\n    Returns\n    -------\n    List[str]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.fixture_index import tokenize\n    &gt;&gt;&gt; result = tokenize(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return [token.lower() for token in TOKEN_RE.findall(text or \"\")]\n</code></pre>"},{"location":"api/search_api/fusion/","title":"<code>search_api.fusion</code>","text":"<p>Fusion utilities.</p>"},{"location":"api/search_api/fusion/#search_api.fusion.rrf_fuse","title":"<code>rrf_fuse(rankers, k=60)</code>","text":"<p>Compute rrf fuse.</p> <p>Carry out the rrf fuse operation.</p>"},{"location":"api/search_api/fusion/#search_api.fusion.rrf_fuse--parameters","title":"Parameters","text":"<p>rankers : List[List[Tuple[str, float]]]     Description for <code>rankers</code>. k : int | None     Description for <code>k</code>.</p>"},{"location":"api/search_api/fusion/#search_api.fusion.rrf_fuse--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/search_api/fusion/#search_api.fusion.rrf_fuse--examples","title":"Examples","text":"<p>from search_api.fusion import rrf_fuse result = rrf_fuse(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/fusion.py</code> <pre><code>def rrf_fuse(rankers: list[list[tuple[str, float]]], k: int = 60) -&gt; dict[str, float]:\n    \"\"\"Compute rrf fuse.\n\n    Carry out the rrf fuse operation.\n\n    Parameters\n    ----------\n    rankers : List[List[Tuple[str, float]]]\n        Description for ``rankers``.\n    k : int | None\n        Description for ``k``.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.fusion import rrf_fuse\n    &gt;&gt;&gt; result = rrf_fuse(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    agg: dict[str, float] = {}\n    for ranked in rankers:\n        for r, (key, _score) in enumerate(ranked, start=1):\n            agg[key] = agg.get(key, 0.0) + 1.0 / (k + r)\n    return agg\n</code></pre>"},{"location":"api/search_api/kg_mock/","title":"<code>search_api.kg_mock</code>","text":"<p>Kg Mock utilities.</p>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.ConceptMeta","title":"<code>ConceptMeta</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Describe ConceptMeta.</p> Source code in <code>src/search_api/kg_mock.py</code> <pre><code>class ConceptMeta(TypedDict):\n    \"\"\"Describe ConceptMeta.\"\"\"\n\n    label: str\n    keywords: list[str]\n</code></pre>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.detect_query_concepts","title":"<code>detect_query_concepts(query)</code>","text":"<p>Compute detect query concepts.</p> <p>Carry out the detect query concepts operation.</p>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.detect_query_concepts--parameters","title":"Parameters","text":"<p>query : str     Description for <code>query</code>.</p>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.detect_query_concepts--returns","title":"Returns","text":"<p>collections.abc.Set     Description of return value.</p>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.detect_query_concepts--examples","title":"Examples","text":"<p>from search_api.kg_mock import detect_query_concepts result = detect_query_concepts(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/kg_mock.py</code> <pre><code>def detect_query_concepts(query: str) -&gt; set[str]:\n    \"\"\"Compute detect query concepts.\n\n    Carry out the detect query concepts operation.\n\n    Parameters\n    ----------\n    query : str\n        Description for ``query``.\n\n    Returns\n    -------\n    collections.abc.Set\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.kg_mock import detect_query_concepts\n    &gt;&gt;&gt; result = detect_query_concepts(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    lowered = query.lower()\n    hits: set[str] = set()\n    for concept_id, meta in CONCEPTS.items():\n        if any(keyword in lowered for keyword in meta[\"keywords\"]):\n            hits.add(concept_id)\n    return hits\n</code></pre>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.kg_boost","title":"<code>kg_boost(query_concepts, chunk_concepts, direct=0.08, one_hop=0.04)</code>","text":"<p>Compute kg boost.</p> <p>Carry out the kg boost operation.</p>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.kg_boost--parameters","title":"Parameters","text":"<p>query_concepts : List[str]     Description for <code>query_concepts</code>. chunk_concepts : List[str]     Description for <code>chunk_concepts</code>. direct : float | None     Description for <code>direct</code>. one_hop : float | None     Description for <code>one_hop</code>.</p>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.kg_boost--returns","title":"Returns","text":"<p>float     Description of return value.</p>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.kg_boost--examples","title":"Examples","text":"<p>from search_api.kg_mock import kg_boost result = kg_boost(..., ..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/kg_mock.py</code> <pre><code>def kg_boost(\n    query_concepts: list[str],\n    chunk_concepts: list[str],\n    direct: float = 0.08,\n    one_hop: float = 0.04,\n) -&gt; float:\n    \"\"\"Compute kg boost.\n\n    Carry out the kg boost operation.\n\n    Parameters\n    ----------\n    query_concepts : List[str]\n        Description for ``query_concepts``.\n    chunk_concepts : List[str]\n        Description for ``chunk_concepts``.\n    direct : float | None\n        Description for ``direct``.\n    one_hop : float | None\n        Description for ``one_hop``.\n\n    Returns\n    -------\n    float\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.kg_mock import kg_boost\n    &gt;&gt;&gt; result = kg_boost(..., ..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    _ = one_hop  # placeholder for future graph traversal heuristics\n    return direct if set(query_concepts) &amp; set(chunk_concepts) else 0.0\n</code></pre>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.linked_concepts_for_text","title":"<code>linked_concepts_for_text(text)</code>","text":"<p>Compute linked concepts for text.</p> <p>Carry out the linked concepts for text operation.</p>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.linked_concepts_for_text--parameters","title":"Parameters","text":"<p>text : str     Description for <code>text</code>.</p>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.linked_concepts_for_text--returns","title":"Returns","text":"<p>List[str]     Description of return value.</p>"},{"location":"api/search_api/kg_mock/#search_api.kg_mock.linked_concepts_for_text--examples","title":"Examples","text":"<p>from search_api.kg_mock import linked_concepts_for_text result = linked_concepts_for_text(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/kg_mock.py</code> <pre><code>def linked_concepts_for_text(text: str) -&gt; list[str]:\n    \"\"\"Compute linked concepts for text.\n\n    Carry out the linked concepts for text operation.\n\n    Parameters\n    ----------\n    text : str\n        Description for ``text``.\n\n    Returns\n    -------\n    List[str]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.kg_mock import linked_concepts_for_text\n    &gt;&gt;&gt; result = linked_concepts_for_text(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    lowered = text.lower()\n    hits = []\n    for concept_id, meta in CONCEPTS.items():\n        if any(keyword in lowered for keyword in meta[\"keywords\"]):\n            hits.append(concept_id)\n    return hits\n</code></pre>"},{"location":"api/search_api/schemas/","title":"<code>search_api.schemas</code>","text":"<p>Schemas utilities.</p>"},{"location":"api/search_api/schemas/#search_api.schemas.SearchRequest","title":"<code>SearchRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Describe SearchRequest.</p> Source code in <code>src/search_api/schemas.py</code> <pre><code>class SearchRequest(BaseModel):\n    \"\"\"Describe SearchRequest.\"\"\"\n\n    query: str = Field(min_length=1)\n    k: int = 10\n    filters: dict[str, object] | None = None\n    explain: bool = False\n</code></pre>"},{"location":"api/search_api/schemas/#search_api.schemas.SearchResult","title":"<code>SearchResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Describe SearchResult.</p> Source code in <code>src/search_api/schemas.py</code> <pre><code>class SearchResult(BaseModel):\n    \"\"\"Describe SearchResult.\"\"\"\n\n    doc_id: str\n    chunk_id: str\n    title: str\n    section: str\n    score: float\n    signals: dict[str, float] = {}\n    spans: dict[str, int] = {}\n    concepts: list[dict[str, str]] = []\n</code></pre>"},{"location":"api/search_api/service/","title":"<code>search_api.service</code>","text":"<p>Service utilities.</p>"},{"location":"api/search_api/service/#search_api.service.apply_kg_boosts","title":"<code>apply_kg_boosts(fused, query)</code>","text":"<p>Compute apply kg boosts.</p> <p>Carry out the apply kg boosts operation.</p>"},{"location":"api/search_api/service/#search_api.service.apply_kg_boosts--parameters","title":"Parameters","text":"<p>fused : List[Tuple[str, float]]     Description for <code>fused</code>. query : str     Description for <code>query</code>.</p>"},{"location":"api/search_api/service/#search_api.service.apply_kg_boosts--returns","title":"Returns","text":"<p>List[Tuple[str, float]]     Description of return value.</p>"},{"location":"api/search_api/service/#search_api.service.apply_kg_boosts--examples","title":"Examples","text":"<p>from search_api.service import apply_kg_boosts result = apply_kg_boosts(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/service.py</code> <pre><code>def apply_kg_boosts(fused: list[tuple[str, float]], query: str) -&gt; list[tuple[str, float]]:\n    \"\"\"Compute apply kg boosts.\n\n    Carry out the apply kg boosts operation.\n\n    Parameters\n    ----------\n    fused : List[Tuple[str, float]]\n        Description for ``fused``.\n    query : str\n        Description for ``query``.\n\n    Returns\n    -------\n    List[Tuple[str, float]]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.service import apply_kg_boosts\n    &gt;&gt;&gt; result = apply_kg_boosts(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    # NOTE: apply boosts for direct &amp; one-hop concept matches once KG signals exist\n    return fused\n</code></pre>"},{"location":"api/search_api/service/#search_api.service.mmr_deduplicate","title":"<code>mmr_deduplicate(results, lambda_=0.7)</code>","text":"<p>Compute mmr deduplicate.</p> <p>Carry out the mmr deduplicate operation.</p>"},{"location":"api/search_api/service/#search_api.service.mmr_deduplicate--parameters","title":"Parameters","text":"<p>results : List[Tuple[str, float]]     Description for <code>results</code>. lambda_ : float | None     Description for <code>lambda_</code>.</p>"},{"location":"api/search_api/service/#search_api.service.mmr_deduplicate--returns","title":"Returns","text":"<p>List[Tuple[str, float]]     Description of return value.</p>"},{"location":"api/search_api/service/#search_api.service.mmr_deduplicate--examples","title":"Examples","text":"<p>from search_api.service import mmr_deduplicate result = mmr_deduplicate(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/service.py</code> <pre><code>def mmr_deduplicate(\n    results: list[tuple[str, float]], lambda_: float = 0.7\n) -&gt; list[tuple[str, float]]:\n    \"\"\"Compute mmr deduplicate.\n\n    Carry out the mmr deduplicate operation.\n\n    Parameters\n    ----------\n    results : List[Tuple[str, float]]\n        Description for ``results``.\n    lambda_ : float | None\n        Description for ``lambda_``.\n\n    Returns\n    -------\n    List[Tuple[str, float]]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.service import mmr_deduplicate\n    &gt;&gt;&gt; result = mmr_deduplicate(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    # NOTE: add doc-level diversity via MMR when result scoring is available\n    return results\n</code></pre>"},{"location":"api/search_api/service/#search_api.service.rrf_fuse","title":"<code>rrf_fuse(dense, sparse, k=60)</code>","text":"<p>Compute rrf fuse.</p> <p>Carry out the rrf fuse operation.</p>"},{"location":"api/search_api/service/#search_api.service.rrf_fuse--parameters","title":"Parameters","text":"<p>dense : List[Tuple[str, float]]     Description for <code>dense</code>. sparse : List[Tuple[str, float]]     Description for <code>sparse</code>. k : int | None     Description for <code>k</code>.</p>"},{"location":"api/search_api/service/#search_api.service.rrf_fuse--returns","title":"Returns","text":"<p>List[Tuple[str, float]]     Description of return value.</p>"},{"location":"api/search_api/service/#search_api.service.rrf_fuse--examples","title":"Examples","text":"<p>from search_api.service import rrf_fuse result = rrf_fuse(..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/service.py</code> <pre><code>def rrf_fuse(\n    dense: list[tuple[str, float]], sparse: list[tuple[str, float]], k: int = 60\n) -&gt; list[tuple[str, float]]:\n    \"\"\"Compute rrf fuse.\n\n    Carry out the rrf fuse operation.\n\n    Parameters\n    ----------\n    dense : List[Tuple[str, float]]\n        Description for ``dense``.\n    sparse : List[Tuple[str, float]]\n        Description for ``sparse``.\n    k : int | None\n        Description for ``k``.\n\n    Returns\n    -------\n    List[Tuple[str, float]]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.service import rrf_fuse\n    &gt;&gt;&gt; result = rrf_fuse(..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    # NOTE: implement stable RRF across rankers when ranker outputs are wired\n    return []\n</code></pre>"},{"location":"api/search_api/splade_index/","title":"<code>search_api.splade_index</code>","text":"<p>Splade Index utilities.</p>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeDoc","title":"<code>SpladeDoc</code>  <code>dataclass</code>","text":"<p>Describe SpladeDoc.</p> Source code in <code>src/search_api/splade_index.py</code> <pre><code>@dataclass\nclass SpladeDoc:\n    \"\"\"Describe SpladeDoc.\"\"\"\n\n    chunk_id: str\n    doc_id: str\n    section: str\n    text: str\n</code></pre>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex","title":"<code>SpladeIndex</code>","text":"<p>Describe SpladeIndex.</p> Source code in <code>src/search_api/splade_index.py</code> <pre><code>class SpladeIndex:\n    \"\"\"Describe SpladeIndex.\"\"\"\n\n    def __init__(\n        self,\n        db_path: str,\n        chunks_dataset_root: str | None = None,\n        sparse_root: str | None = None,\n    ) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        db_path : str\n            Description for ``db_path``.\n        chunks_dataset_root : str | None\n            Description for ``chunks_dataset_root``.\n        sparse_root : str | None\n            Description for ``sparse_root``.\n        \"\"\"\n\n\n        _ = sparse_root  # retained for interface compatibility\n        self.db_path = db_path\n        self.docs: list[SpladeDoc] = []\n        self.df: dict[str, int] = {}\n        self.N = 0\n        self._load(chunks_dataset_root)\n\n    def _load(self, chunks_root: str | None) -&gt; None:\n        \"\"\"Compute load.\n\n        Carry out the load operation.\n\n        Parameters\n        ----------\n        chunks_root : str | None\n            Description for ``chunks_root``.\n        \"\"\"\n        _ = chunks_root  # optional override currently unused\n        if not Path(self.db_path).exists():\n            return\n        con = duckdb.connect(self.db_path)\n        try:\n            dataset = con.execute(\n                \"SELECT parquet_root FROM datasets \"\n                \"WHERE kind='chunks' ORDER BY created_at DESC LIMIT 1\"\n            ).fetchone()\n            if dataset:\n                rows = con.execute(\n                    \"SELECT c.chunk_id, c.doc_id, coalesce(c.section,''), c.text \"\n                    f\"FROM read_parquet('{dataset[0]}/*/*.parquet', union_by_name=true) AS c\"\n                ).fetchall()\n                for chunk_id, doc_id, section, text in rows:\n                    self.docs.append(\n                        SpladeDoc(\n                            chunk_id=chunk_id,\n                            doc_id=doc_id or \"urn:doc:fixture\",\n                            section=section,\n                            text=text or \"\",\n                        )\n                    )\n        finally:\n            con.close()\n        self.N = len(self.docs)\n        for doc in self.docs:\n            for term in set(tok(doc.text)):\n                self.df[term] = self.df.get(term, 0) + 1\n\n    def search(self, query: str, k: int = 10) -&gt; list[tuple[int, float]]:\n        \"\"\"Compute search.\n\n        Carry out the search operation.\n\n        Parameters\n        ----------\n        query : str\n            Description for ``query``.\n        k : int | None\n            Description for ``k``.\n\n        Returns\n        -------\n        List[Tuple[int, float]]\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_api.splade_index import search\n        &gt;&gt;&gt; result = search(..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        if self.N == 0:\n            return []\n        terms = tok(query)\n        if not terms:\n            return []\n        scores = [0.0] * self.N\n        for index, doc in enumerate(self.docs):\n            term_freq: dict[str, int] = {}\n            for term in tok(doc.text):\n                term_freq[term] = term_freq.get(term, 0) + 1\n            score = 0.0\n            for term in terms:\n                if term in self.df:\n                    idf = (self.N + 1) / (self.df[term] + 0.5)\n                    score += term_freq.get(term, 0) * idf\n            scores[index] = score\n        ranked = sorted(enumerate(scores), key=lambda item: item[1], reverse=True)\n        return [(idx, value) for idx, value in ranked[:k] if value &gt; 0.0]\n\n    def doc(self, index: int) -&gt; SpladeDoc:\n        \"\"\"Compute doc.\n\n        Carry out the doc operation.\n\n        Parameters\n        ----------\n        index : int\n            Description for ``index``.\n\n        Returns\n        -------\n        src.search_api.splade_index.SpladeDoc\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_api.splade_index import doc\n        &gt;&gt;&gt; result = doc(...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        return self.docs[index]\n</code></pre>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex.__init__","title":"<code>__init__(db_path, chunks_dataset_root=None, sparse_root=None)</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex.__init__--parameters","title":"Parameters","text":"<p>db_path : str     Description for <code>db_path</code>. chunks_dataset_root : str | None     Description for <code>chunks_dataset_root</code>. sparse_root : str | None     Description for <code>sparse_root</code>.</p> Source code in <code>src/search_api/splade_index.py</code> <pre><code>def __init__(\n    self,\n    db_path: str,\n    chunks_dataset_root: str | None = None,\n    sparse_root: str | None = None,\n) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    db_path : str\n        Description for ``db_path``.\n    chunks_dataset_root : str | None\n        Description for ``chunks_dataset_root``.\n    sparse_root : str | None\n        Description for ``sparse_root``.\n    \"\"\"\n\n\n    _ = sparse_root  # retained for interface compatibility\n    self.db_path = db_path\n    self.docs: list[SpladeDoc] = []\n    self.df: dict[str, int] = {}\n    self.N = 0\n    self._load(chunks_dataset_root)\n</code></pre>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex.doc","title":"<code>doc(index)</code>","text":"<p>Compute doc.</p> <p>Carry out the doc operation.</p>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex.doc--parameters","title":"Parameters","text":"<p>index : int     Description for <code>index</code>.</p>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex.doc--returns","title":"Returns","text":"<p>src.search_api.splade_index.SpladeDoc     Description of return value.</p>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex.doc--examples","title":"Examples","text":"<p>from search_api.splade_index import doc result = doc(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/splade_index.py</code> <pre><code>def doc(self, index: int) -&gt; SpladeDoc:\n    \"\"\"Compute doc.\n\n    Carry out the doc operation.\n\n    Parameters\n    ----------\n    index : int\n        Description for ``index``.\n\n    Returns\n    -------\n    src.search_api.splade_index.SpladeDoc\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.splade_index import doc\n    &gt;&gt;&gt; result = doc(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return self.docs[index]\n</code></pre>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex.search","title":"<code>search(query, k=10)</code>","text":"<p>Compute search.</p> <p>Carry out the search operation.</p>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex.search--parameters","title":"Parameters","text":"<p>query : str     Description for <code>query</code>. k : int | None     Description for <code>k</code>.</p>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex.search--returns","title":"Returns","text":"<p>List[Tuple[int, float]]     Description of return value.</p>"},{"location":"api/search_api/splade_index/#search_api.splade_index.SpladeIndex.search--examples","title":"Examples","text":"<p>from search_api.splade_index import search result = search(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/splade_index.py</code> <pre><code>def search(self, query: str, k: int = 10) -&gt; list[tuple[int, float]]:\n    \"\"\"Compute search.\n\n    Carry out the search operation.\n\n    Parameters\n    ----------\n    query : str\n        Description for ``query``.\n    k : int | None\n        Description for ``k``.\n\n    Returns\n    -------\n    List[Tuple[int, float]]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.splade_index import search\n    &gt;&gt;&gt; result = search(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    if self.N == 0:\n        return []\n    terms = tok(query)\n    if not terms:\n        return []\n    scores = [0.0] * self.N\n    for index, doc in enumerate(self.docs):\n        term_freq: dict[str, int] = {}\n        for term in tok(doc.text):\n            term_freq[term] = term_freq.get(term, 0) + 1\n        score = 0.0\n        for term in terms:\n            if term in self.df:\n                idf = (self.N + 1) / (self.df[term] + 0.5)\n                score += term_freq.get(term, 0) * idf\n        scores[index] = score\n    ranked = sorted(enumerate(scores), key=lambda item: item[1], reverse=True)\n    return [(idx, value) for idx, value in ranked[:k] if value &gt; 0.0]\n</code></pre>"},{"location":"api/search_api/splade_index/#search_api.splade_index.tok","title":"<code>tok(text)</code>","text":"<p>Compute tok.</p> <p>Carry out the tok operation.</p>"},{"location":"api/search_api/splade_index/#search_api.splade_index.tok--parameters","title":"Parameters","text":"<p>text : str     Description for <code>text</code>.</p>"},{"location":"api/search_api/splade_index/#search_api.splade_index.tok--returns","title":"Returns","text":"<p>List[str]     Description of return value.</p>"},{"location":"api/search_api/splade_index/#search_api.splade_index.tok--examples","title":"Examples","text":"<p>from search_api.splade_index import tok result = tok(...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_api/splade_index.py</code> <pre><code>def tok(text: str) -&gt; list[str]:\n    \"\"\"Compute tok.\n\n    Carry out the tok operation.\n\n    Parameters\n    ----------\n    text : str\n        Description for ``text``.\n\n    Returns\n    -------\n    List[str]\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_api.splade_index import tok\n    &gt;&gt;&gt; result = tok(...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    return [token.lower() for token in TOKEN.findall(text or \"\")]\n</code></pre>"},{"location":"api/search_client/","title":"<code>search_client</code>","text":"<p>Search Client utilities.</p>"},{"location":"api/search_client/client/","title":"<code>search_client.client</code>","text":"<p>Client utilities.</p>"},{"location":"api/search_client/client/#search_client.client.KGFoundryClient","title":"<code>KGFoundryClient</code>","text":"<p>Describe KGFoundryClient.</p> Source code in <code>src/search_client/client.py</code> <pre><code>class KGFoundryClient:\n    \"\"\"Describe KGFoundryClient.\"\"\"\n\n    def __init__(\n        self,\n        base_url: str = \"http://localhost:8080\",\n        api_key: str | None = None,\n        timeout: float = 30.0,\n        http: _SupportsHttp | None = None,\n    ) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        base_url : str | None\n            Description for ``base_url``.\n        api_key : str | None\n            Description for ``api_key``.\n        timeout : float | None\n            Description for ``timeout``.\n        http : _SupportsHttp | None\n            Description for ``http``.\n        \"\"\"\n\n\n        self.base_url = base_url.rstrip(\"/\")\n        self.api_key = api_key\n        self.timeout = timeout\n        self._http: _SupportsHttp = http or requests\n\n    def _headers(self) -&gt; dict[str, str]:\n        \"\"\"Compute headers.\n\n        Carry out the headers operation.\n\n        Returns\n        -------\n        Mapping[str, str]\n            Description of return value.\n        \"\"\"\n        h = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            h[\"Authorization\"] = f\"Bearer {self.api_key}\"\n        return h\n\n    def healthz(self) -&gt; dict[str, Any]:\n        \"\"\"Compute healthz.\n\n        Carry out the healthz operation.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_client.client import healthz\n        &gt;&gt;&gt; result = healthz()\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        r = self._http.get(f\"{self.base_url}/healthz\", timeout=self.timeout)\n        r.raise_for_status()\n        return r.json()\n\n    def search(\n        self,\n        query: str,\n        k: int = 10,\n        filters: dict[str, Any] | None = None,\n        explain: bool = False,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Compute search.\n\n        Carry out the search operation.\n\n        Parameters\n        ----------\n        query : str\n            Description for ``query``.\n        k : int | None\n            Description for ``k``.\n        filters : Mapping[str, Any] | None\n            Description for ``filters``.\n        explain : bool | None\n            Description for ``explain``.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_client.client import search\n        &gt;&gt;&gt; result = search(..., ..., ..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        payload = {\"query\": query, \"k\": k, \"filters\": filters or {}, \"explain\": explain}\n        r = self._http.post(\n            f\"{self.base_url}/search\", json=payload, headers=self._headers(), timeout=self.timeout\n        )\n        r.raise_for_status()\n        return r.json()\n\n    def concepts(self, q: str, limit: int = 50) -&gt; dict[str, Any]:\n        \"\"\"Compute concepts.\n\n        Carry out the concepts operation.\n\n        Parameters\n        ----------\n        q : str\n            Description for ``q``.\n        limit : int | None\n            Description for ``limit``.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Description of return value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from search_client.client import concepts\n        &gt;&gt;&gt; result = concepts(..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        r = self._http.post(\n            f\"{self.base_url}/graph/concepts\",\n            json={\"q\": q, \"limit\": limit},\n            headers=self._headers(),\n            timeout=self.timeout,\n        )\n        r.raise_for_status()\n        return r.json()\n</code></pre>"},{"location":"api/search_client/client/#search_client.client.KGFoundryClient.__init__","title":"<code>__init__(base_url='http://localhost:8080', api_key=None, timeout=30.0, http=None)</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/search_client/client/#search_client.client.KGFoundryClient.__init__--parameters","title":"Parameters","text":"<p>base_url : str | None     Description for <code>base_url</code>. api_key : str | None     Description for <code>api_key</code>. timeout : float | None     Description for <code>timeout</code>. http : _SupportsHttp | None     Description for <code>http</code>.</p> Source code in <code>src/search_client/client.py</code> <pre><code>def __init__(\n    self,\n    base_url: str = \"http://localhost:8080\",\n    api_key: str | None = None,\n    timeout: float = 30.0,\n    http: _SupportsHttp | None = None,\n) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    base_url : str | None\n        Description for ``base_url``.\n    api_key : str | None\n        Description for ``api_key``.\n    timeout : float | None\n        Description for ``timeout``.\n    http : _SupportsHttp | None\n        Description for ``http``.\n    \"\"\"\n\n\n    self.base_url = base_url.rstrip(\"/\")\n    self.api_key = api_key\n    self.timeout = timeout\n    self._http: _SupportsHttp = http or requests\n</code></pre>"},{"location":"api/search_client/client/#search_client.client.KGFoundryClient.concepts","title":"<code>concepts(q, limit=50)</code>","text":"<p>Compute concepts.</p> <p>Carry out the concepts operation.</p>"},{"location":"api/search_client/client/#search_client.client.KGFoundryClient.concepts--parameters","title":"Parameters","text":"<p>q : str     Description for <code>q</code>. limit : int | None     Description for <code>limit</code>.</p>"},{"location":"api/search_client/client/#search_client.client.KGFoundryClient.concepts--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/search_client/client/#search_client.client.KGFoundryClient.concepts--examples","title":"Examples","text":"<p>from search_client.client import concepts result = concepts(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_client/client.py</code> <pre><code>def concepts(self, q: str, limit: int = 50) -&gt; dict[str, Any]:\n    \"\"\"Compute concepts.\n\n    Carry out the concepts operation.\n\n    Parameters\n    ----------\n    q : str\n        Description for ``q``.\n    limit : int | None\n        Description for ``limit``.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_client.client import concepts\n    &gt;&gt;&gt; result = concepts(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    r = self._http.post(\n        f\"{self.base_url}/graph/concepts\",\n        json={\"q\": q, \"limit\": limit},\n        headers=self._headers(),\n        timeout=self.timeout,\n    )\n    r.raise_for_status()\n    return r.json()\n</code></pre>"},{"location":"api/search_client/client/#search_client.client.KGFoundryClient.healthz","title":"<code>healthz()</code>","text":"<p>Compute healthz.</p> <p>Carry out the healthz operation.</p>"},{"location":"api/search_client/client/#search_client.client.KGFoundryClient.healthz--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/search_client/client/#search_client.client.KGFoundryClient.healthz--examples","title":"Examples","text":"<p>from search_client.client import healthz result = healthz() result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_client/client.py</code> <pre><code>def healthz(self) -&gt; dict[str, Any]:\n    \"\"\"Compute healthz.\n\n    Carry out the healthz operation.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_client.client import healthz\n    &gt;&gt;&gt; result = healthz()\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    r = self._http.get(f\"{self.base_url}/healthz\", timeout=self.timeout)\n    r.raise_for_status()\n    return r.json()\n</code></pre>"},{"location":"api/search_client/client/#search_client.client.KGFoundryClient.search","title":"<code>search(query, k=10, filters=None, explain=False)</code>","text":"<p>Compute search.</p> <p>Carry out the search operation.</p>"},{"location":"api/search_client/client/#search_client.client.KGFoundryClient.search--parameters","title":"Parameters","text":"<p>query : str     Description for <code>query</code>. k : int | None     Description for <code>k</code>. filters : Mapping[str, Any] | None     Description for <code>filters</code>. explain : bool | None     Description for <code>explain</code>.</p>"},{"location":"api/search_client/client/#search_client.client.KGFoundryClient.search--returns","title":"Returns","text":"<p>collections.abc.Mapping     Description of return value.</p>"},{"location":"api/search_client/client/#search_client.client.KGFoundryClient.search--examples","title":"Examples","text":"<p>from search_client.client import search result = search(..., ..., ..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/search_client/client.py</code> <pre><code>def search(\n    self,\n    query: str,\n    k: int = 10,\n    filters: dict[str, Any] | None = None,\n    explain: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Compute search.\n\n    Carry out the search operation.\n\n    Parameters\n    ----------\n    query : str\n        Description for ``query``.\n    k : int | None\n        Description for ``k``.\n    filters : Mapping[str, Any] | None\n        Description for ``filters``.\n    explain : bool | None\n        Description for ``explain``.\n\n    Returns\n    -------\n    collections.abc.Mapping\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from search_client.client import search\n    &gt;&gt;&gt; result = search(..., ..., ..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    payload = {\"query\": query, \"k\": k, \"filters\": filters or {}, \"explain\": explain}\n    r = self._http.post(\n        f\"{self.base_url}/search\", json=payload, headers=self._headers(), timeout=self.timeout\n    )\n    r.raise_for_status()\n    return r.json()\n</code></pre>"},{"location":"api/vectorstore_faiss/","title":"<code>vectorstore_faiss</code>","text":"<p>Vectorstore Faiss utilities.</p>"},{"location":"api/vectorstore_faiss/gpu/","title":"<code>vectorstore_faiss.gpu</code>","text":"<p>Gpu utilities.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex","title":"<code>FaissGpuIndex</code>","text":"<p>Describe FaissGpuIndex.</p> Source code in <code>src/vectorstore_faiss/gpu.py</code> <pre><code>class FaissGpuIndex:\n    \"\"\"Describe FaissGpuIndex.\"\"\"\n\n    def __init__(\n        self,\n        factory: str = \"OPQ64,IVF8192,PQ64\",\n        nprobe: int = 64,\n        gpu: bool = True,\n        cuvs: bool = True,\n    ) -&gt; None:\n        \"\"\"Compute init.\n\n        Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n        Parameters\n        ----------\n        factory : str | None\n            Description for ``factory``.\n        nprobe : int | None\n            Description for ``nprobe``.\n        gpu : bool | None\n            Description for ``gpu``.\n        cuvs : bool | None\n            Description for ``cuvs``.\n        \"\"\"\n\n\n        self.factory = factory\n        self.nprobe = nprobe\n        self.gpu = gpu\n        self.cuvs = cuvs\n        self._faiss: Any | None = None\n        self._res: Any | None = None\n        self._index: Any | None = None\n        self._idmap: StrArray | None = None\n        self._xb: FloatArray | None = None\n        try:\n            import faiss\n\n            self._faiss = faiss\n        except Exception:  # pragma: no cover - environment without FAISS\n            self._faiss = None\n\n    def _ensure_resources(self) -&gt; None:\n        \"\"\"Compute ensure resources.\n\n        Carry out the ensure resources operation.\n        \"\"\"\n        if not self._faiss or not self.gpu:\n            return\n        if self._res is None:\n            faiss = self._faiss\n            self._res = faiss.StandardGpuResources()\n\n    def train(self, train_vectors: FloatArray, *, seed: int = 42) -&gt; None:\n        \"\"\"Compute train.\n\n        Carry out the train operation.\n\n        Parameters\n        ----------\n        train_vectors : src.vectorstore_faiss.gpu.FloatArray\n            Description for ``train_vectors``.\n        seed : int | None\n            Description for ``seed``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from vectorstore_faiss.gpu import train\n        &gt;&gt;&gt; train(..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        if self._faiss is None:\n            return\n        train_mat = cast(FloatArray, np.asarray(train_vectors, dtype=np.float32, order=\"C\"))\n        faiss = self._faiss\n        dimension = train_mat.shape[1]\n        cpu_index = faiss.index_factory(dimension, self.factory, faiss.METRIC_INNER_PRODUCT)\n        faiss.normalize_L2(train_mat)\n        cpu_index.train(train_mat)\n        self._ensure_resources()\n        if self.gpu:\n            options = faiss.GpuClonerOptions()\n            if hasattr(options, \"use_cuvs\"):\n                options.use_cuvs = bool(self.cuvs)\n            try:\n                self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index, options)\n            except Exception:  # pragma: no cover - fallback without cuVS\n                self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index)\n        else:\n            self._index = cpu_index\n        try:\n            params = faiss.GpuParameterSpace() if self.gpu else faiss.ParameterSpace()\n            params.set_index_parameter(self._index, \"nprobe\", self.nprobe)\n        except Exception:\n            pass\n\n    def add(self, keys: list[str], vectors: FloatArray) -&gt; None:\n        \"\"\"Compute add.\n\n        Carry out the add operation.\n\n        Parameters\n        ----------\n        keys : List[str]\n            Description for ``keys``.\n        vectors : src.vectorstore_faiss.gpu.FloatArray\n            Description for ``vectors``.\n\n        Raises\n        ------\n        RuntimeError\n            Raised when validation fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from vectorstore_faiss.gpu import add\n        &gt;&gt;&gt; add(..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        vec_array = cast(FloatArray, np.asarray(vectors, dtype=np.float32, order=\"C\"))\n        if self._faiss is None:\n            self._xb = cast(FloatArray, np.array(vec_array, copy=True))\n            self._idmap = cast(StrArray, np.asarray(keys, dtype=str))\n            norms = np.linalg.norm(self._xb, axis=1, keepdims=True) + 1e-12\n            self._xb /= norms\n            return\n        faiss = self._faiss\n        if self._index is None:\n            message = \"FAISS index not initialized; call train() before add().\"\n            raise RuntimeError(message)\n        faiss.normalize_L2(vec_array)\n        if isinstance(self._index, faiss.IndexIDMap2):\n            idmap_array = cast(IntArray, np.asarray(keys, dtype=\"int64\"))\n            self._index.add_with_ids(vec_array, idmap_array)\n        elif hasattr(faiss, \"IndexIDMap2\"):\n            idmap = faiss.IndexIDMap2(self._index)\n            idmap_array = cast(IntArray, np.asarray(keys, dtype=\"int64\"))\n            idmap.add_with_ids(vec_array, idmap_array)\n            self._index = idmap\n        else:\n            self._index.add(vec_array)\n\n    def search(self, query: FloatArray, k: int) -&gt; list[tuple[str, float]]:\n        \"\"\"Compute search.\n\n        Carry out the search operation.\n\n        Parameters\n        ----------\n        query : src.vectorstore_faiss.gpu.FloatArray\n            Description for ``query``.\n        k : int\n            Description for ``k``.\n\n        Returns\n        -------\n        List[Tuple[str, float]]\n            Description of return value.\n\n        Raises\n        ------\n        RuntimeError\n            Raised when validation fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from vectorstore_faiss.gpu import search\n        &gt;&gt;&gt; result = search(..., ...)\n        &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n        ...\n        \"\"\"\n        q = cast(FloatArray, np.asarray(query, dtype=np.float32, order=\"C\"))\n        q /= np.linalg.norm(q, axis=-1, keepdims=True) + 1e-12\n        if self._faiss is None or self._index is None:\n            if self._xb is None or self._idmap is None:\n                return []\n            sims_matrix = self._xb @ q.T\n            sims = np.asarray(sims_matrix, dtype=np.float32).squeeze()\n            indices = np.argsort(-sims)[:k]\n            return [(str(self._idmap[i]), float(sims[i])) for i in indices.tolist()]\n        if self._idmap is None:\n            message = \"ID map not loaded; cannot resolve FAISS results.\"\n            raise RuntimeError(message)\n        distances, indices = self._index.search(q.reshape(1, -1), k)\n        ids = indices[0]\n        scores = distances[0]\n        return [(str(ids[i]), float(scores[i])) for i in range(len(ids)) if ids[i] != -1]\n\n    def save(self, index_uri: str, idmap_uri: str | None = None) -&gt; None:\n        \"\"\"Compute save.\n\n        Carry out the save operation.\n\n        Parameters\n        ----------\n        index_uri : str\n            Description for ``index_uri``.\n        idmap_uri : str | None\n            Description for ``idmap_uri``.\n\n        Raises\n        ------\n        RuntimeError\n            Raised when validation fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from vectorstore_faiss.gpu import save\n        &gt;&gt;&gt; save(..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        if self._faiss is None or self._index is None:\n            if self._xb is not None and self._idmap is not None:\n                np.savez(index_uri, xb=self._xb, ids=self._idmap)\n            return\n        faiss = self._faiss\n        if faiss is None:\n            message = \"FAISS not available\"\n            raise RuntimeError(message)\n        target_index = faiss.index_gpu_to_cpu(self._index) if self.gpu else self._index\n        faiss.write_index(target_index, index_uri)\n\n    def load(self, index_uri: str, idmap_uri: str | None = None) -&gt; None:\n        \"\"\"Compute load.\n\n        Carry out the load operation.\n\n        Parameters\n        ----------\n        index_uri : str\n            Description for ``index_uri``.\n        idmap_uri : str | None\n            Description for ``idmap_uri``.\n\n        Raises\n        ------\n        RuntimeError\n            Raised when validation fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from vectorstore_faiss.gpu import load\n        &gt;&gt;&gt; load(..., ...)  # doctest: +ELLIPSIS\n        \"\"\"\n        if self._faiss is None:\n            if os.path.exists(index_uri + \".npz\"):\n                data = np.load(index_uri + \".npz\", allow_pickle=True)\n                self._xb = data[\"xb\"]\n                self._idmap = data[\"ids\"]\n            return\n        faiss = self._faiss\n        if faiss is None:\n            message = \"FAISS not available\"\n            raise RuntimeError(message)\n        cpu_index = faiss.read_index(index_uri)\n        self._ensure_resources()\n        if self.gpu:\n            options = faiss.GpuClonerOptions()\n            if hasattr(options, \"use_cuvs\"):\n                options.use_cuvs = bool(self.cuvs)\n            try:\n                self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index, options)\n            except Exception:\n                self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index)\n        else:\n            self._index = cpu_index\n</code></pre>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.__init__","title":"<code>__init__(factory='OPQ64,IVF8192,PQ64', nprobe=64, gpu=True, cuvs=True)</code>","text":"<p>Compute init.</p> <p>Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call <code>super().__init__</code> to keep validation and defaults intact.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.__init__--parameters","title":"Parameters","text":"<p>factory : str | None     Description for <code>factory</code>. nprobe : int | None     Description for <code>nprobe</code>. gpu : bool | None     Description for <code>gpu</code>. cuvs : bool | None     Description for <code>cuvs</code>.</p> Source code in <code>src/vectorstore_faiss/gpu.py</code> <pre><code>def __init__(\n    self,\n    factory: str = \"OPQ64,IVF8192,PQ64\",\n    nprobe: int = 64,\n    gpu: bool = True,\n    cuvs: bool = True,\n) -&gt; None:\n    \"\"\"Compute init.\n\n    Initialise a new instance with validated parameters. The constructor prepares internal state and coordinates any setup required by the class. Subclasses should call ``super().__init__`` to keep validation and defaults intact.\n\n    Parameters\n    ----------\n    factory : str | None\n        Description for ``factory``.\n    nprobe : int | None\n        Description for ``nprobe``.\n    gpu : bool | None\n        Description for ``gpu``.\n    cuvs : bool | None\n        Description for ``cuvs``.\n    \"\"\"\n\n\n    self.factory = factory\n    self.nprobe = nprobe\n    self.gpu = gpu\n    self.cuvs = cuvs\n    self._faiss: Any | None = None\n    self._res: Any | None = None\n    self._index: Any | None = None\n    self._idmap: StrArray | None = None\n    self._xb: FloatArray | None = None\n    try:\n        import faiss\n\n        self._faiss = faiss\n    except Exception:  # pragma: no cover - environment without FAISS\n        self._faiss = None\n</code></pre>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.add","title":"<code>add(keys, vectors)</code>","text":"<p>Compute add.</p> <p>Carry out the add operation.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.add--parameters","title":"Parameters","text":"<p>keys : List[str]     Description for <code>keys</code>. vectors : src.vectorstore_faiss.gpu.FloatArray     Description for <code>vectors</code>.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.add--raises","title":"Raises","text":"<p>RuntimeError     Raised when validation fails.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.add--examples","title":"Examples","text":"<p>from vectorstore_faiss.gpu import add add(..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/vectorstore_faiss/gpu.py</code> <pre><code>def add(self, keys: list[str], vectors: FloatArray) -&gt; None:\n    \"\"\"Compute add.\n\n    Carry out the add operation.\n\n    Parameters\n    ----------\n    keys : List[str]\n        Description for ``keys``.\n    vectors : src.vectorstore_faiss.gpu.FloatArray\n        Description for ``vectors``.\n\n    Raises\n    ------\n    RuntimeError\n        Raised when validation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from vectorstore_faiss.gpu import add\n    &gt;&gt;&gt; add(..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    vec_array = cast(FloatArray, np.asarray(vectors, dtype=np.float32, order=\"C\"))\n    if self._faiss is None:\n        self._xb = cast(FloatArray, np.array(vec_array, copy=True))\n        self._idmap = cast(StrArray, np.asarray(keys, dtype=str))\n        norms = np.linalg.norm(self._xb, axis=1, keepdims=True) + 1e-12\n        self._xb /= norms\n        return\n    faiss = self._faiss\n    if self._index is None:\n        message = \"FAISS index not initialized; call train() before add().\"\n        raise RuntimeError(message)\n    faiss.normalize_L2(vec_array)\n    if isinstance(self._index, faiss.IndexIDMap2):\n        idmap_array = cast(IntArray, np.asarray(keys, dtype=\"int64\"))\n        self._index.add_with_ids(vec_array, idmap_array)\n    elif hasattr(faiss, \"IndexIDMap2\"):\n        idmap = faiss.IndexIDMap2(self._index)\n        idmap_array = cast(IntArray, np.asarray(keys, dtype=\"int64\"))\n        idmap.add_with_ids(vec_array, idmap_array)\n        self._index = idmap\n    else:\n        self._index.add(vec_array)\n</code></pre>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.load","title":"<code>load(index_uri, idmap_uri=None)</code>","text":"<p>Compute load.</p> <p>Carry out the load operation.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.load--parameters","title":"Parameters","text":"<p>index_uri : str     Description for <code>index_uri</code>. idmap_uri : str | None     Description for <code>idmap_uri</code>.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.load--raises","title":"Raises","text":"<p>RuntimeError     Raised when validation fails.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.load--examples","title":"Examples","text":"<p>from vectorstore_faiss.gpu import load load(..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/vectorstore_faiss/gpu.py</code> <pre><code>def load(self, index_uri: str, idmap_uri: str | None = None) -&gt; None:\n    \"\"\"Compute load.\n\n    Carry out the load operation.\n\n    Parameters\n    ----------\n    index_uri : str\n        Description for ``index_uri``.\n    idmap_uri : str | None\n        Description for ``idmap_uri``.\n\n    Raises\n    ------\n    RuntimeError\n        Raised when validation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from vectorstore_faiss.gpu import load\n    &gt;&gt;&gt; load(..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    if self._faiss is None:\n        if os.path.exists(index_uri + \".npz\"):\n            data = np.load(index_uri + \".npz\", allow_pickle=True)\n            self._xb = data[\"xb\"]\n            self._idmap = data[\"ids\"]\n        return\n    faiss = self._faiss\n    if faiss is None:\n        message = \"FAISS not available\"\n        raise RuntimeError(message)\n    cpu_index = faiss.read_index(index_uri)\n    self._ensure_resources()\n    if self.gpu:\n        options = faiss.GpuClonerOptions()\n        if hasattr(options, \"use_cuvs\"):\n            options.use_cuvs = bool(self.cuvs)\n        try:\n            self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index, options)\n        except Exception:\n            self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index)\n    else:\n        self._index = cpu_index\n</code></pre>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.save","title":"<code>save(index_uri, idmap_uri=None)</code>","text":"<p>Compute save.</p> <p>Carry out the save operation.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.save--parameters","title":"Parameters","text":"<p>index_uri : str     Description for <code>index_uri</code>. idmap_uri : str | None     Description for <code>idmap_uri</code>.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.save--raises","title":"Raises","text":"<p>RuntimeError     Raised when validation fails.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.save--examples","title":"Examples","text":"<p>from vectorstore_faiss.gpu import save save(..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/vectorstore_faiss/gpu.py</code> <pre><code>def save(self, index_uri: str, idmap_uri: str | None = None) -&gt; None:\n    \"\"\"Compute save.\n\n    Carry out the save operation.\n\n    Parameters\n    ----------\n    index_uri : str\n        Description for ``index_uri``.\n    idmap_uri : str | None\n        Description for ``idmap_uri``.\n\n    Raises\n    ------\n    RuntimeError\n        Raised when validation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from vectorstore_faiss.gpu import save\n    &gt;&gt;&gt; save(..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    if self._faiss is None or self._index is None:\n        if self._xb is not None and self._idmap is not None:\n            np.savez(index_uri, xb=self._xb, ids=self._idmap)\n        return\n    faiss = self._faiss\n    if faiss is None:\n        message = \"FAISS not available\"\n        raise RuntimeError(message)\n    target_index = faiss.index_gpu_to_cpu(self._index) if self.gpu else self._index\n    faiss.write_index(target_index, index_uri)\n</code></pre>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.search","title":"<code>search(query, k)</code>","text":"<p>Compute search.</p> <p>Carry out the search operation.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.search--parameters","title":"Parameters","text":"<p>query : src.vectorstore_faiss.gpu.FloatArray     Description for <code>query</code>. k : int     Description for <code>k</code>.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.search--returns","title":"Returns","text":"<p>List[Tuple[str, float]]     Description of return value.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.search--raises","title":"Raises","text":"<p>RuntimeError     Raised when validation fails.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.search--examples","title":"Examples","text":"<p>from vectorstore_faiss.gpu import search result = search(..., ...) result  # doctest: +ELLIPSIS ...</p> Source code in <code>src/vectorstore_faiss/gpu.py</code> <pre><code>def search(self, query: FloatArray, k: int) -&gt; list[tuple[str, float]]:\n    \"\"\"Compute search.\n\n    Carry out the search operation.\n\n    Parameters\n    ----------\n    query : src.vectorstore_faiss.gpu.FloatArray\n        Description for ``query``.\n    k : int\n        Description for ``k``.\n\n    Returns\n    -------\n    List[Tuple[str, float]]\n        Description of return value.\n\n    Raises\n    ------\n    RuntimeError\n        Raised when validation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from vectorstore_faiss.gpu import search\n    &gt;&gt;&gt; result = search(..., ...)\n    &gt;&gt;&gt; result  # doctest: +ELLIPSIS\n    ...\n    \"\"\"\n    q = cast(FloatArray, np.asarray(query, dtype=np.float32, order=\"C\"))\n    q /= np.linalg.norm(q, axis=-1, keepdims=True) + 1e-12\n    if self._faiss is None or self._index is None:\n        if self._xb is None or self._idmap is None:\n            return []\n        sims_matrix = self._xb @ q.T\n        sims = np.asarray(sims_matrix, dtype=np.float32).squeeze()\n        indices = np.argsort(-sims)[:k]\n        return [(str(self._idmap[i]), float(sims[i])) for i in indices.tolist()]\n    if self._idmap is None:\n        message = \"ID map not loaded; cannot resolve FAISS results.\"\n        raise RuntimeError(message)\n    distances, indices = self._index.search(q.reshape(1, -1), k)\n    ids = indices[0]\n    scores = distances[0]\n    return [(str(ids[i]), float(scores[i])) for i in range(len(ids)) if ids[i] != -1]\n</code></pre>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.train","title":"<code>train(train_vectors, *, seed=42)</code>","text":"<p>Compute train.</p> <p>Carry out the train operation.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.train--parameters","title":"Parameters","text":"<p>train_vectors : src.vectorstore_faiss.gpu.FloatArray     Description for <code>train_vectors</code>. seed : int | None     Description for <code>seed</code>.</p>"},{"location":"api/vectorstore_faiss/gpu/#vectorstore_faiss.gpu.FaissGpuIndex.train--examples","title":"Examples","text":"<p>from vectorstore_faiss.gpu import train train(..., ...)  # doctest: +ELLIPSIS</p> Source code in <code>src/vectorstore_faiss/gpu.py</code> <pre><code>def train(self, train_vectors: FloatArray, *, seed: int = 42) -&gt; None:\n    \"\"\"Compute train.\n\n    Carry out the train operation.\n\n    Parameters\n    ----------\n    train_vectors : src.vectorstore_faiss.gpu.FloatArray\n        Description for ``train_vectors``.\n    seed : int | None\n        Description for ``seed``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from vectorstore_faiss.gpu import train\n    &gt;&gt;&gt; train(..., ...)  # doctest: +ELLIPSIS\n    \"\"\"\n    if self._faiss is None:\n        return\n    train_mat = cast(FloatArray, np.asarray(train_vectors, dtype=np.float32, order=\"C\"))\n    faiss = self._faiss\n    dimension = train_mat.shape[1]\n    cpu_index = faiss.index_factory(dimension, self.factory, faiss.METRIC_INNER_PRODUCT)\n    faiss.normalize_L2(train_mat)\n    cpu_index.train(train_mat)\n    self._ensure_resources()\n    if self.gpu:\n        options = faiss.GpuClonerOptions()\n        if hasattr(options, \"use_cuvs\"):\n            options.use_cuvs = bool(self.cuvs)\n        try:\n            self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index, options)\n        except Exception:  # pragma: no cover - fallback without cuVS\n            self._index = faiss.index_cpu_to_gpu(self._res, 0, cpu_index)\n    else:\n        self._index = cpu_index\n    try:\n        params = faiss.GpuParameterSpace() if self.gpu else faiss.ParameterSpace()\n        params.set_index_parameter(self._index, \"nprobe\", self.nprobe)\n    except Exception:\n        pass\n</code></pre>"},{"location":"architecture/","title":"Architecture","text":"<p>Architecture decision records and diagrams that govern kgfoundry.</p> <pre><code>:maxdepth: 1\nadr/0001-record-architecture-decisions\nadr/0002-hexagonal-architecture\n</code></pre> <p>Add diagram markdown files under <code>docs/architecture/diagrams/</code> to surface them here.</p>"},{"location":"architecture/adr/0001-record-architecture-decisions/","title":"0001 \u2013 Record architecture decisions","text":"<p>Date: 2025-10-26</p>"},{"location":"architecture/adr/0001-record-architecture-decisions/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/adr/0001-record-architecture-decisions/#context","title":"Context","text":"<p>We need persistent, reviewable decisions for the architecture (layers, boundaries, invariants).</p>"},{"location":"architecture/adr/0001-record-architecture-decisions/#decision","title":"Decision","text":"<p>Use ADRs (Markdown files in <code>docs/architecture/adr/</code>) to capture decisions. Use Import Linter to encode layering rules; fail CI on violations. Use C4 diagrams (PlantUML/Mermaid) to visualize context and containers.</p>"},{"location":"architecture/adr/0001-record-architecture-decisions/#consequences","title":"Consequences","text":"<ul> <li>New contributors and AI agents understand why choices were made.</li> <li>Changes across layers require updating rules and ADRs to stay aligned.</li> </ul>"},{"location":"architecture/adr/0002-hexagonal-architecture/","title":"0002 \u2013 Adopt a Hexagonal Architecture","text":"<p>Date: 2025-10-25</p>"},{"location":"architecture/adr/0002-hexagonal-architecture/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/adr/0002-hexagonal-architecture/#context","title":"Context","text":"<p>We are delivering a single-machine, high-performance hybrid search system that stitches together DocTags, Qwen3 embeddings, SPLADE, FAISS GPU/cuVS, and DuckDB as the registry. We must keep interfaces stable while enabling rapid iteration.</p>"},{"location":"architecture/adr/0002-hexagonal-architecture/#decision","title":"Decision","text":"<p>Adopt a hexagonal architecture with strict inbound/outbound ports. Persist vectors in Parquet to keep storage self-describing and tooling-friendly.</p>"},{"location":"architecture/adr/0002-hexagonal-architecture/#consequences","title":"Consequences","text":"<ul> <li>Independent workstreams can progress without coordination friction.</li> <li>Ports/adapters isolate infrastructure churn from core domain logic.</li> <li>Testing and reproducibility improve because IO remains deterministic.</li> </ul>"},{"location":"explanations/","title":"Explanations","text":"<p>Deep dives into kgfoundry architecture and design decisions.</p> <pre><code>:maxdepth: 1\n251025_HighLevelArchitecture\n251025_FAISS_whl_overview\nnumpy-docstring-migration\n</code></pre>"},{"location":"explanations/251025_FAISS_whl_overview/","title":"Updated cuVS Addendum","text":"<p>Below is an updated, implementation\u2011grade addendum that reflects your new FAISS wheel with cuVS integration compiled for: Flat, IVF\u2011Flat, IVF\u2011PQ, Graph CAGRA, and Binary\u2011CAGRA, as indicated by the build artifacts:</p> <pre><code>CuvsFlatIndex.cu.o\nCuvsIVFFlat.cu.o\nCuvsIVFPQ.cu.o\nCuvsCagra.cu.o\nBinaryCuvsCagra.cu.o\nutils/CuvsFilterConvert.cu.o\nutils/CuvsUtils.cu.o\n</code></pre> <p>Key consequence: In this wheel, FAISS GPU indexes can dispatch to cuVS kernels for the families above when you enable cuVS at construction/cloning time and the cuVS/RAPIDS libraries are present in the process. The runtime loader &amp; resource rules for cuVS remain the same (preload <code>libcuvs</code>, <code>librmm</code>, logging; use a shared <code>Resources</code> handle if calling cuVS directly).  </p>"},{"location":"explanations/251025_FAISS_whl_overview/#1-revised-capability-matrix-this-wheel","title":"1) Revised capability matrix (this wheel)","text":"Family / class CPU GPU (FAISS kernels) cuVS through FAISS (this wheel) cuVS direct (Python) Flat (exact) \u2705 <code>IndexFlat{L2,IP}</code> \u2705 <code>GpuIndexFlat{L2,IP}</code> \u2705 via index\u2011level cuVS (<code>GpuIndexFlat{L2,IP}</code> with <code>use_cuvs</code>) and via bfKNN (<code>knn_gpu/bfKnn</code> with <code>use_cuvs</code>) \u2705 <code>cuvs.neighbors.brute_force</code> IVF\u2011Flat \u2705 \u2705 <code>GpuIndexIVFFlat</code> \u2705 index\u2011level cuVS (<code>GpuIndexIVFFlat</code> with <code>use_cuvs</code>) \u2705 <code>cuvs.neighbors.ivf_flat</code> IVF\u2011PQ \u2705 \u2705 <code>GpuIndexIVFPQ</code> \u2705 index\u2011level cuVS (<code>GpuIndexIVFPQ</code> with <code>use_cuvs</code>) \u2705 <code>cuvs.neighbors.ivf_pq</code> IVF\u2011SQ \u2705 \u2705 <code>GpuIndexIVFScalarQuantizer</code> \u2013 (no cuVS kernel here) \u2013 Graph (HNSW) \u2705 \u2013 \u2013 \u2705 <code>cuvs.neighbors.hnsw</code> Graph (CAGRA) \u2705 <code>IndexHNSWCagra</code> \u2705 <code>GpuIndexCagra</code> \u2705 index\u2011level cuVS (<code>GpuIndexCagra</code> with <code>use_cuvs</code>) \u2705 <code>cuvs.neighbors.cagra</code> (+ multi\u2011GPU) Binary \u2705 \u2705 <code>GpuIndexBinaryFlat</code>, <code>GpuIndexBinaryCagra</code> \u2705 Binary\u2011CAGRA via index\u2011level cuVS (<code>GpuIndexBinaryCagra</code>) \u2013 Direct bfKNN \u2013 \u2705 <code>bfKnn(...)</code> / <code>knn_gpu(...)</code> \u2705 set <code>use_cuvs=True</code> (guard with <code>should_use_cuvs(...)</code>) \u2705 <code>cuvs.distance</code> / brute_force <p>What changed: In earlier guidance, IVF\u2011Flat/IVF\u2011PQ/CAGRA/Binary\u2011CAGRA were marked \u201cnot via FAISS route\u201d. In this wheel, those families are cuVS\u2011enabled inside FAISS when you opt in via the index configs or GPU cloner options described below (and cuVS libs are present). The bfKNN cuVS path remains available. The cuVS loader &amp; environment remain as documented. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#2-how-to-enable-cuvs-reliable-patterns","title":"2) How to enable cuVS (reliable patterns)","text":""},{"location":"explanations/251025_FAISS_whl_overview/#21-oneline-preload-cuvsrapids-libs","title":"2.1. One\u2011line preload (cuVS/RAPIDS libs)","text":"<p>Call this before importing/initializing FAISS to ensure the shared libraries are resident:</p> <pre><code>from libcuvs import load_library\nload_library()  # loads libcuvs, librmm, rapids_logger; idempotent\n</code></pre> <p>This follows the integration guidance in your loader reference and prevents <code>OSError: libcuvs_c.so not found</code> surprises. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#22-enabling-cuvs-via-gpu-cloner-options-recommended","title":"2.2. Enabling cuVS via GPU Cloner Options (recommended)","text":"<p>The simplest and most portable way to engage cuVS for Flat / IVF\u2011Flat / IVF\u2011PQ / CAGRA / Binary\u2011CAGRA is to build the index on CPU, then clone to GPU with a cuVS\u2011aware cloner:</p> <pre><code>import faiss\n\n# 1) Build a CPU index (examples below)\ncpu_index = faiss.index_factory(d, \"OPQ64,IVF8192,PQ64\", faiss.METRIC_INNER_PRODUCT)\ncpu_index.train(train_vectors)\n\n# 2) Prepare GPU resources\nres = faiss.StandardGpuResources()\n\n# 3) Clone with cuVS enabled (guarded)\nco = faiss.GpuClonerOptions()\nco.use_cuvs = True  # ask for cuVS kernels if available\n\ntry:\n    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)\nexcept RuntimeError:\n    # Fallback if cuVS cannot be used for this combo\n    co.use_cuvs = False\n    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)\n</code></pre> <ul> <li><code>GpuClonerOptions.use_cuvs</code> is exposed in this wheel and directs FAISS to build the cuVS\u2011backed GPU index when possible. </li> <li>This pattern works uniformly across Flat / IVF\u2011Flat / IVF\u2011PQ and also covers CAGRA if cloning from <code>IndexHNSWCagra</code> to <code>GpuIndexCagra</code> is desired. (You can also build <code>GpuIndexCagra</code> directly; see 2.3.)</li> <li>Keep your existing memory tuning (<code>setTempMemory</code>, <code>setPinnedMemory</code>) for throughput.</li> </ul> <p>You can also probe <code>faiss.should_use_cuvs(...)</code> before setting the flag; however, the try/except is the most robust guard when combining drivers, cards, and dims. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#23-enabling-cuvs-via-index-configs-direct-gpu-constructors","title":"2.3. Enabling cuVS via index configs (direct GPU constructors)","text":"<p>When you construct GPU indexes directly, pass a config whose base type is <code>GpuIndexConfig</code> and set <code>use_cuvs</code> there (the property is inherited by specific configs):</p> <pre><code># Flat\nflat_cfg = faiss.GpuIndexFlatConfig()\nflat_cfg.use_cuvs = True\ngpu_flat = faiss.GpuIndexFlatIP(res, d, flat_cfg)  # or GpuIndexFlatL2\n\n# IVF-FLAT / IVF-PQ\nivf_cfg = faiss.GpuIndexIVFConfig()\nivf_cfg.use_cuvs = True\n# ... pass ivf_cfg to the appropriate GpuIndexIVF* constructor\n\n# CAGRA\ncagra_cfg = faiss.GpuIndexCagraConfig()\ncagra_cfg.use_cuvs = True\ngpu_cagra = faiss.GpuIndexCagra(res, d, cagra_cfg)\n\n# Binary-CAGRA (if you build it directly)\n# (config class may not be separate in the API; prefer the cloner path if unsure)\n</code></pre> <p>These configs inherit <code>use_cuvs</code> through <code>GpuIndexConfig</code> in this wheel. Use the cloner option (2.2) when you want one code path that works across all index types. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#24-enabling-cuvs-for-direct-gpu-bruteforce-knn","title":"2.4. Enabling cuVS for direct GPU brute\u2011force kNN","text":"<p>The bfKNN path can independently dispatch to cuVS kernels:</p> <pre><code>from faiss import StandardGpuResources, METRIC_INNER_PRODUCT, GpuDistanceParams, knn_gpu\n\nres = StandardGpuResources()\nparams = GpuDistanceParams(); params.metric = METRIC_INNER_PRODUCT; params.k = 10; params.dims = d\n\n# guard with should_use_cuvs; fallback is automatic inside knn_gpu if you pass False\nuse_cuvs = True  # or: bool(faiss.should_use_cuvs(params))\nD, I = knn_gpu(res, xq.astype('float32'), xb.astype('float32'),\n               k=10, metric=METRIC_INNER_PRODUCT, use_cuvs=use_cuvs)\n</code></pre> <p>This remains a great baseline even when your main serving index is IVF\u2011PQ/CAGRA. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#3-minimal-correct-by-construction-recipes-per-family","title":"3) Minimal \u201ccorrect by construction\u201d recipes per family","text":"<p>These patterns train on CPU (deterministic), then clone to GPU with cuVS. You can switch to direct GPU construction later; the recall/latency behavior is identical once cuVS is engaged.</p>"},{"location":"explanations/251025_FAISS_whl_overview/#31-ivfpq-opq64ivf8192pq64-with-cuvs","title":"3.1. IVF\u2011PQ (OPQ64,IVF8192,PQ64) with cuVS","text":"<pre><code>import faiss, numpy as np\n\nd = 2560\ncpu = faiss.index_factory(d, \"OPQ64,IVF8192,PQ64\", faiss.METRIC_INNER_PRODUCT)\n\ntrain = np.random.randn(10_000_000, d).astype('float32')\nfaiss.normalize_L2(train)\ncpu.train(train)\n\nres = faiss.StandardGpuResources()\nco = faiss.GpuClonerOptions(); co.use_cuvs = True\ngpu = faiss.index_cpu_to_gpu(res, 0, cpu, co)  # cuVS-backed IVFPQ\n\n# add/search as usual\n</code></pre> <ul> <li>Keeps your default \u201cbest\u2011in\u2011class\u201d factory and determinism.</li> <li>The cloner chooses the cuVS implementation for the data path compiled in this wheel. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#32-ivfflat-with-cuvs","title":"3.2. IVF\u2011Flat with cuVS","text":"<pre><code>cpu = faiss.index_factory(d, \"IVF8192,Flat\", faiss.METRIC_INNER_PRODUCT)\ncpu.train(train)\n\nco = faiss.GpuClonerOptions(); co.use_cuvs = True\ngpu = faiss.index_cpu_to_gpu(res, 0, cpu, co)  # cuVS-backed IVFFlat\n</code></pre>"},{"location":"explanations/251025_FAISS_whl_overview/#33-flat-exact-with-cuvs","title":"3.3. Flat (exact) with cuVS","text":"<ul> <li>Index route: <code>GpuIndexFlat{L2,IP}</code> with <code>GpuIndexFlatConfig.use_cuvs = True</code>.</li> <li>Direct KNN: <code>knn_gpu(..., use_cuvs=True)</code> (guarded). </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#34-cagra-with-cuvs","title":"3.4. CAGRA with cuVS","text":"<ul> <li>Direct GPU build (graph lives on GPU):</li> </ul> <p><code>python   cfg = faiss.GpuIndexCagraConfig(); cfg.use_cuvs = True   gpu_cagra = faiss.GpuIndexCagra(res, d, cfg)   # gpu_cagra.add(n, xb) builds the graph; search as usual</code></p> <ul> <li>CPU\u2194GPU interop: <code>IndexHNSWCagra</code> exists on CPU for moving graph structure if needed (e.g., copy base level). Keep primary serving on GPU to avoid host &lt;-&gt; device hops.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#35-binarycagra-with-cuvs","title":"3.5. Binary\u2011CAGRA with cuVS","text":"<ul> <li>Prefer the cloner path when coming from CPU binary indexes; if you instantiate <code>GpuIndexBinaryCagra</code> directly, keep the same principle: construct with a config that enables cuVS (or clone with <code>GpuClonerOptions.use_cuvs=True</code>) and fall back on error.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#4-filters-selectors-what-those-cuvsfilter-objects-mean-for-you","title":"4) Filters &amp; selectors (what those <code>CuvsFilter*</code> objects mean for you)","text":"<p>The presence of <code>utils/CuvsFilterConvert.cu.o</code> indicates FAISS performs selector/bitset conversion into cuVS\u2011compatible filter representations when a filter is passed at search time (e.g., via <code>SearchParametersIVF.sel</code> / ID selectors). You do not need to change your Python calls:</p> <ul> <li>Keep using FAISS\u2019 search parameters (e.g., <code>SearchParametersIVF</code>\u2019s selector).</li> <li>When the index was constructed/cloned with <code>use_cuvs=True</code>, the FAISS runtime converts the filter to cuVS\u2019 internal format and applies it during list scans.</li> </ul> <p>This is transparent but worth noting for filtered search scenarios. (The general cuVS run\u2011time and loader expectations remain exactly as previously documented.) </p>"},{"location":"explanations/251025_FAISS_whl_overview/#5-operational-reminders-unchanged-but-critical","title":"5) Operational reminders (unchanged but critical)","text":"<ul> <li>Preload cuVS once per process (<code>libcuvs.load_library()</code>), so RMM and logging are initialized and all <code>.so</code> dependencies are resident. </li> <li>Stick to row\u2011major <code>float32</code> contiguous arrays for FAISS; normalize vectors for cosine (IP).</li> <li>Keep GPU working set \u2264 ~80% VRAM; pre\u2011allocate temp and pinned memory on <code>StandardGpuResources</code>.</li> <li>For cuVS direct experimentation (IVF\u2011PQ, IVF\u2011Flat, CAGRA, HNSW, multi\u2011GPU), continue to use the Python surfaces under <code>cuvs.neighbors.*</code> with a shared <code>Resources</code> handle and device arrays, as documented in your cuVS reference. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#6-quick-am-i-using-cuvs-right-now-probes","title":"6) Quick \u201cam I using cuVS right now?\u201d probes","text":"<pre><code># After calling libcuvs.load_library() and constructing your GPU index:\n\nimport faiss\n\n# 1) For bfKNN:\np = faiss.GpuDistanceParams(); p.k=10; p.dims=d; p.metric=faiss.METRIC_INNER_PRODUCT\nprint(\"bfKNN should_use_cuvs:\", faiss.should_use_cuvs(p))  # True \u2192 bfKNN will route to cuVS\n\n# 2) For an index you plan to build:\ncfg = faiss.GpuIndexIVFConfig()\nprint(\"Index should_use_cuvs:\", faiss.should_use_cuvs(cfg))  # True \u2192 set cfg/co.use_cuvs = True\n</code></pre> <ul> <li>If you skip <code>should_use_cuvs</code>, simply set <code>use_cuvs=True</code> and catch a <code>RuntimeError</code> on construction; re\u2011try with <code>False</code>. This keeps your code robust across environments while preferring cuVS whenever available. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#pointers-unchanged","title":"Pointers (unchanged)","text":"<ul> <li>cuVS API &amp; usage (neighbors, distance, resources, device arrays) \u2014 your cuVS reference. </li> <li>cuVS loader &amp; environment (preload order, env vars, system vs wheel libs) \u2014 libcuvs loader reference. </li> <li>System architecture &amp; defaults (2560\u2011d, OPQ64/IVF8192/PQ64, persistence, observability) \u2014 high\u2011level architecture. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#7-bottom-line-what-to-change-in-your-code","title":"7) Bottom line (what to change in your code)","text":"<ol> <li>Call <code>load_library()</code> once at startup. </li> <li>When cloning CPU\u2192GPU, set <code>co.use_cuvs = True</code> and fall back on error. This now enables cuVS for Flat, IVF\u2011Flat, IVF\u2011PQ, CAGRA, and Binary\u2011CAGRA in this build.</li> <li>When constructing GPU indexes directly, set <code>cfg.use_cuvs = True</code> on the appropriate <code>GpuIndex*Config</code> (or use the cloner).</li> <li>For bfKNN, pass <code>use_cuvs=True</code> (or guard with <code>should_use_cuvs</code>). </li> <li>Keep the rest of the pipeline (factory string, normalization, memory planning, multi\u2011GPU strategy, persistence) unchanged. </li> </ol>"},{"location":"explanations/251025_FAISS_whl_overview/#addendum-base-content-on-faiss-gpu-library","title":"addendum, base content on faiss gpu library","text":""},{"location":"explanations/251025_FAISS_whl_overview/#0-what-changed-vs-the-prior-wheel-and-how-to-think-about-it","title":"0) What changed vs. the prior wheel (and how to think about it)","text":"<p>Hardware/ABI</p> <ul> <li>CUDA runtime linked: <code>libcudart.so.13</code> \u2192 the wheel targets CUDA\u202f13 (Blackwell\u2011ready).</li> <li>GPU arch: <code>sm_120</code> SASS + PTX (\u201c<code>sm_120\u2011virtual</code>\u201d) for forward compatibility\u2014native kernels when they match, PTX JIT otherwise.</li> </ul> <p>cuVS inside FAISS (this wheel)</p> <ul> <li>The GPU SWIG layer exports <code>use_cuvs</code> gates on index configs and cloner options and dynamically links against cuVS/RAPIDS libs. With cuVS libraries preloaded, FAISS GPU indexes can route to cuVS kernels for Flat, IVF\u2011Flat, IVF\u2011PQ, CAGRA, Binary\u2011CAGRA; otherwise FAISS falls back to its own kernels. </li> </ul> <p>Reminder: the kernels themselves live in the cuVS shared libraries; the FAISS wheel holds dispatch hooks and dynamic links. Ensure the cuVS loader is invoked at process start. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#1-cpu-vs-gpu-vs-cuvs-updated-capability-map-this-wheel","title":"1) CPU vs GPU vs cuVS \u2014 updated capability map (this wheel)","text":"Family / class CPU GPU (FAISS kernels) cuVS through FAISS (this wheel) cuVS direct (Python) Flat (exact) \u2705 <code>IndexFlat{L2,IP}</code> \u2705 <code>GpuIndexFlat{L2,IP}</code> \u2705 via index\u2011level <code>use_cuvs</code> on <code>GpuIndexFlat*</code> and via bfKNN (<code>knn_gpu/bfKnn</code> <code>use_cuvs</code>) \u2705 <code>cuvs.neighbors.brute_force</code> IVF\u2011Flat \u2705 \u2705 <code>GpuIndexIVFFlat</code> \u2705 via index\u2011level <code>use_cuvs</code> / GPU cloner \u2705 <code>cuvs.neighbors.ivf_flat</code> IVF\u2011PQ \u2705 \u2705 <code>GpuIndexIVFPQ</code> \u2705 via index\u2011level <code>use_cuvs</code> / GPU cloner \u2705 <code>cuvs.neighbors.ivf_pq</code> IVF\u2011SQ \u2705 \u2705 <code>GpuIndexIVFScalarQuantizer</code> \u2013 (no cuVS path) \u2013 Graph (HNSW) \u2705 \u2013 \u2013 \u2705 <code>cuvs.neighbors.hnsw</code> Graph (CAGRA) \u2705 <code>IndexHNSWCagra</code> \u2705 <code>GpuIndexCagra</code> \u2705 via index\u2011level <code>use_cuvs</code> \u2705 <code>cuvs.neighbors.cagra</code> (+ multi\u2011GPU) Binary \u2705 \u2705 <code>GpuIndexBinaryFlat</code>, <code>GpuIndexBinaryCagra</code> \u2705 Binary\u2011CAGRA via index\u2011level <code>use_cuvs</code> \u2013 Direct bfKNN \u2013 \u2705 <code>bfKnn(...)</code> / <code>knn_gpu(...)</code> \u2705 <code>use_cuvs=True</code> (guard with <code>should_use_cuvs</code>) \u2705 <code>cuvs.distance</code> / brute_force <p>Filters &amp; selectors: your build includes <code>CuvsFilterConvert.cu.o</code>; FAISS will convert FAISS selectors/bitsets into cuVS\u2019 filter format automatically when the index was created with <code>use_cuvs=True</code>. You keep using standard FAISS search params at the Python layer. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#2-blackwell-rtx-5090-support-sass-sm_120-ptx-sm_120virtual","title":"2) Blackwell (RTX\u202f5090) support: SASS <code>sm_120</code> + PTX <code>sm_120\u2011virtual</code>","text":"<ul> <li>Why it matters: perfect performance when SASS matches; PTX fallback when driver/toolkit moves ahead (no \u201cno kernel image\u201d errors).</li> <li>Ops tips: leave <code>CUDA_CACHE_MAXSIZE</code> roomy to avoid first\u2011run JIT thrash; never force JIT in production.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#3-cuvs-enablement-model-final-patterns","title":"3) cuVS enablement model (final patterns)","text":"<p>Always preload cuVS/RAPIDS libs before FAISS:</p> <pre><code>from libcuvs import load_library\nload_library()  # loads libcuvs, librmm, rapids_logger; idempotent. :contentReference[oaicite:12]{index=12}\n</code></pre> <p>A) Index\u2011level enablement (recommended) Set <code>use_cuvs=True</code> on the GPU cloner or GPU index config; catch &amp; fallback:</p> <pre><code>import faiss\n\n# Clone CPU \u2192 GPU (works uniformly for Flat / IVF-Flat / IVF-PQ / CAGRA / Binary-CAGRA)\nco = faiss.GpuClonerOptions(); co.use_cuvs = True\ntry:\n    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)\nexcept RuntimeError:\n    co.use_cuvs = False\n    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)\n</code></pre> <p>Or construct GPU indexes directly with a config inheriting <code>GpuIndexConfig.use_cuvs</code>:</p> <pre><code>flat_cfg = faiss.GpuIndexFlatConfig(); flat_cfg.use_cuvs = True\ngpu_flat = faiss.GpuIndexFlatIP(res, d, flat_cfg)\n\nivf_cfg = faiss.GpuIndexIVFConfig(); ivf_cfg.use_cuvs = True\n# pass ivf_cfg into the appropriate GpuIndexIVF* constructor\n\ncagra_cfg = faiss.GpuIndexCagraConfig(); cagra_cfg.use_cuvs = True\ngpu_cagra = faiss.GpuIndexCagra(res, d, cagra_cfg)\n</code></pre> <p>B) Brute\u2011force KNN path Guard <code>use_cuvs</code> with the central probe (falls back if not viable):</p> <pre><code>params = faiss.GpuDistanceParams(); params.dims=d; params.k=10; params.metric=faiss.METRIC_INNER_PRODUCT\nuse_cuvs = bool(faiss.should_use_cuvs(params))  # detects viability at runtime. :contentReference[oaicite:13]{index=13}\nD, I = faiss.knn_gpu(res, xq.astype('float32'), xb.astype('float32'), 10,\n                     metric=faiss.METRIC_INNER_PRODUCT, use_cuvs=use_cuvs)\n</code></pre> <p>C) Direct cuVS ANN (optional) Use cuVS Python APIs for IVF\u2011Flat/IVF\u2011PQ/CAGRA/HNSW (especially when you want multi\u2011GPU <code>mg.*</code> variants): <code>cuvs.neighbors.{ivf_flat, ivf_pq, cagra, hnsw}</code> with <code>Resources</code> handle and device arrays. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#4-bestinclass-faiss-build-on-rtx-5090-cosine-2560d-now-with-cuvs-toggles","title":"4) \u201cBest\u2011in\u2011class\u201d FAISS build on RTX\u202f5090 (cosine, 2560\u2011d) \u2014 now with cuVS toggles","text":"<p>Index choice: <code>OPQ64,IVF8192,PQ64</code> (cosine via L2 normalization + IP). Train on up to 10\u202fM samples (seed=42). Search with <code>nprobe=64</code>. </p> <p>GPU clone with cuVS (preferred path):</p> <pre><code>import faiss, numpy as np\n\nd = 2560\ncpu = faiss.index_factory(d, \"OPQ64,IVF8192,PQ64\", faiss.METRIC_INNER_PRODUCT)\ntrain = np.random.randn(10_000_000, d).astype('float32'); faiss.normalize_L2(train)\ncpu.train(train)\n\nres = faiss.StandardGpuResources()\nres.setTempMemory(1_200_000_000); res.setPinnedMemory(512_000_000)\n\nco = faiss.GpuClonerOptions(); co.use_cuvs = True\ntry:\n    gpu = faiss.index_cpu_to_gpu(res, 0, cpu, co)  # cuVS-backed IVFPQ in this build\nexcept RuntimeError:\n    co.use_cuvs = False\n    gpu = faiss.index_cpu_to_gpu(res, 0, cpu, co)\n\n# add / search as usual\n</code></pre> <p>Notes &amp; knobs</p> <ul> <li>Precompute lookup tables on GPU if your PQ config benefits (via cloner/options); keep working set \u2264 80% VRAM; reuse pinned host buffers for larger transfers. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#5-when-to-call-cuvs-directly-and-when-faisscuvs-is-enough","title":"5) When to call cuVS directly (and when FAISS+cuVS is enough)","text":"<ul> <li>Stay inside FAISS (with <code>use_cuvs=True</code>) for: Flat, IVF\u2011Flat, IVF\u2011PQ, CAGRA, Binary\u2011CAGRA; you get FAISS\u2019 ergonomics and cuVS performance.</li> <li>Call cuVS directly when you need: multi\u2011GPU <code>mg.*</code> pipelines, algorithm\u2011specific parameters not exposed by FAISS\u2019 wrappers, or you want to co\u2011locate with other RAFT pipelines with explicit stream choreography. </li> </ul> <p>Direct IVF\u2011PQ example (unchanged) \u2014 device arrays + <code>Resources</code>:</p> <pre><code>from cuvs.common import Resources\nfrom cuvs.neighbors import ivf_pq\nfrom pylibraft.common import device_ndarray\n# build/search as in your prior example \u2026 :contentReference[oaicite:18]{index=18}\n</code></pre>"},{"location":"explanations/251025_FAISS_whl_overview/#6-quick-api-map-what-youll-call-most-often","title":"6) Quick API map \u2014 what you\u2019ll call most often","text":"<p>CPU <code>IndexFlat{L2,IP}</code>, <code>IndexIVFFlat</code>, <code>IndexIVFPQ</code>, <code>IndexIVFScalarQuantizer</code>, <code>IndexHNSW*</code>, <code>IndexHNSWCagra</code>, <code>IndexBinary*</code>, <code>IndexPreTransform</code>, <code>OPQMatrix</code>, <code>PCAMatrix</code>, <code>IndexRefineFlat</code>, <code>IndexShards</code>, <code>IndexReplicas</code>.</p> <p>GPU (FAISS) <code>StandardGpuResources</code>, <code>GpuIndexFlat{L2,IP}</code>, <code>GpuIndexIVFFlat</code>, <code>GpuIndexIVFPQ</code>, <code>GpuIndexIVFScalarQuantizer</code>, <code>GpuIndexCagra</code>, <code>GpuIndexBinaryFlat</code>, <code>GpuIndexBinaryCagra</code>, <code>index_cpu_to_gpu[_multiple]</code>, <code>index_gpu_to_cpu</code>, <code>GpuClonerOptions</code>, <code>GpuParameterSpace</code>, <code>knn_gpu(...)</code>, <code>bfKnn(...)</code>.</p> <p>cuVS hooks inside FAISS <code>GpuClonerOptions.use_cuvs</code>, <code>GpuIndex*Config.use_cuvs</code>, <code>should_use_cuvs(...)</code> (for bfKNN probe), <code>knn_gpu(..., use_cuvs=...)</code>. </p> <p>cuVS (direct) <code>cuvs.neighbors.{ivf_flat, ivf_pq, cagra, hnsw, brute_force, mg.*}</code>, <code>cuvs.cluster.kmeans</code>, <code>cuvs.distance.pairwise_distance</code>, <code>cuvs.common.Resources</code>. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#7-validation-checklist-runtime","title":"7) Validation checklist (runtime)","text":"<ol> <li>Preload check (did we load the libraries?):</li> </ol> <pre><code>from libcuvs import load_library\nprint([h._name for h in load_library()])  # expect libcuvs*, librmm, rapids_logger. :contentReference[oaicite:21]{index=21}\n</code></pre> <ol> <li> <p>Index\u2011level cuVS (construction guard):</p> </li> <li> <p>Clone/construct with <code>use_cuvs=True</code>. If it raises, retry with <code>False</code>. Log the final <code>use_cuvs</code> state per index.</p> </li> <li> <p>bfKNN cuVS probe (fast sanity):</p> </li> </ol> <pre><code>from faiss import GpuDistanceParams, should_use_cuvs, METRIC_INNER_PRODUCT\np = GpuDistanceParams(); p.metric = METRIC_INNER_PRODUCT; p.dims = 2560; p.k = 10\nprint(\"bfKNN should_use_cuvs:\", should_use_cuvs(p))  # True =&gt; bfKNN will route to cuVS. :contentReference[oaicite:22]{index=22}\n</code></pre> <ol> <li>Direct cuVS smoke (IVF\u2011PQ or brute\u2011force) with device arrays + <code>Resources</code>. </li> </ol>"},{"location":"explanations/251025_FAISS_whl_overview/#8-operational-guidance-unchanged-principles-cuvsaware","title":"8) Operational guidance (unchanged principles, cuVS\u2011aware)","text":"<ul> <li>Loader order: preload RAPIDS/cuVS first; then import FAISS. </li> <li>Memory: size <code>setTempMemory</code>/<code>setPinnedMemory</code> once; keep VRAM headroom ~20%; batch adds to avoid OOM; precompute tables where helpful. </li> <li>Streams: cuVS direct calls accept a <code>Resources</code> handle (shared stream) and only sync when you call <code>handle.sync()</code>. FAISS tends to sync at call boundaries; overlap I/O with compute using pinned buffers. </li> <li>Indexes at rest: save CPU index (<code>.faiss</code>) + <code>.ids</code>; rebuild GPU clones at startup (set <code>use_cuvs</code> again when cloning). </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#9-readytopaste-helpers-with-cuvs-autoengage","title":"9) Ready\u2011to\u2011paste helpers (with cuVS auto\u2011engage)","text":"<p>A. GPU clone (cuVS\u2011first, safe fallback)</p> <pre><code>def to_gpu_cuvs_first(cpu_index, res, device=0):\n    co = faiss.GpuClonerOptions(); co.use_cuvs = True\n    try:\n        return faiss.index_cpu_to_gpu(res, device, cpu_index, co), True\n    except RuntimeError:\n        co.use_cuvs = False\n        return faiss.index_cpu_to_gpu(res, device, cpu_index, co), False\n</code></pre> <p>B. bfKNN with automatic cuVS</p> <pre><code>def knn_gpu_auto(res, xq_f32, xb_f32, k=10, metric=faiss.METRIC_INNER_PRODUCT):\n    params = faiss.GpuDistanceParams()\n    params.metric = metric; params.k = k; params.dims = xq_f32.shape[1]\n    use = faiss.should_use_cuvs(params)  # True when viable. :contentReference[oaicite:28]{index=28}\n    return faiss.knn_gpu(res, xq_f32.astype('float32', copy=False),\n                         xb_f32.astype('float32', copy=False),\n                         k, metric=metric, use_cuvs=use)\n</code></pre> <p>C. Direct cuVS IVF\u2011PQ (device arrays) \u2014 unchanged. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#10-bottom-line","title":"10) Bottom line","text":"<ul> <li>This wheel: Blackwell\u2011ready (<code>sm_120</code> + PTX), CUDA\u201113, and cuVS\u2011enabled inside FAISS for Flat, IVF\u2011Flat, IVF\u2011PQ, CAGRA, Binary\u2011CAGRA.</li> <li>Your code: preload cuVS; set <code>use_cuvs=True</code> on GPU clones/constructors; guard with try/except; use <code>should_use_cuvs(...)</code> for bfKNN.</li> <li>Architecture &amp; defaults (factory, training, persistence, multi\u2011GPU) remain per plan; you simply get cuVS speed paths where available without changing the higher\u2011level design. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#addendum-agentoriented-details-updated-to-match-the-new-cuvs-coverage","title":"Addendum \u2014 Agent\u2011oriented details (updated to match the new cuVS coverage)","text":"<p>These sections expand the \u201chow\u201d with concrete interfaces, schemas, and runbooks across the stack and already assume FAISS can route to cuVS for the families above.</p>"},{"location":"explanations/251025_FAISS_whl_overview/#a1-process-bootstrap-deterministic-cuvsready","title":"A1. Process bootstrap (deterministic &amp; cuVS\u2011ready)","text":"<pre><code>from libcuvs import load_library; load_library()   # ensures libcuvs/librmm/logging are loaded. :contentReference[oaicite:31]{index=31}\nimport faiss\nres = faiss.StandardGpuResources()\nres.setTempMemory(1_200_000_000); res.setPinnedMemory(512_000_000)\n</code></pre> <p>Keep a single <code>StandardGpuResources</code> per device for the process lifetime and configure it once. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#a2-zerocopy-interop-faiss-cuvs","title":"A2. Zero\u2011copy interop (FAISS \u2194 cuVS)","text":"<p>Use RAFT <code>Resources</code> and device arrays when calling cuVS directly; prefer pinned host arenas + contiguous <code>float32</code> for FAISS inputs to minimize hidden copies. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#a3-multigpu-replicas-and-shards-unchanged","title":"A3. Multi\u2011GPU: replicas and shards (unchanged)","text":"<p>Use <code>IndexReplicas</code> (QPS) or <code>IndexShards</code> (capacity). When cloning to multiple GPUs, set <code>use_cuvs=True</code> on the cloner options once; FAISS will apply it per device (fallback per GPU on error). </p>"},{"location":"explanations/251025_FAISS_whl_overview/#a4-memory-planning-batching","title":"A4. Memory planning &amp; batching","text":"<p>Budget scratch + PQ tables + codes + ids + batch \u2264 80% VRAM. Tune batch add/search sizes empirically; precompute lookup tables where it helps latency. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#a5-persistence-registry","title":"A5. Persistence &amp; registry","text":"<p>Persist CPU index (<code>.faiss</code>) + <code>.ids</code>; rebuild GPU clones at service start using <code>use_cuvs=True</code>. Record the final applied state (<code>cuvs=true/false</code>) in your registry\u2019s <code>faiss_indexes</code> rows for observability. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#a6-observability-probes","title":"A6. Observability &amp; probes","text":"<ul> <li>Log <code>{gpu_id, nlist, m, nprobe, k, batch, temp_mem, pinned_mem, use_cuvs}</code> for each search; expose p50/p95/p99 histograms. </li> <li>Quick \u201care the libs loaded?\u201d probe: <code>list(h._name for h in load_library())</code>. </li> <li>Quick \u201cwill bfKNN use cuVS?\u201d probe: <code>should_use_cuvs(GpuDistanceParams(...))</code>. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a7-failure-taxonomy-fallbacks-gpu-paths","title":"A7. Failure taxonomy &amp; fallbacks (GPU paths)","text":"<ul> <li>Missing libs \u2192 run <code>load_library()</code> first (or fix <code>LD_LIBRARY_PATH</code>); resume with FAISS kernels if cuVS fails to initialize. </li> <li>OOM \u2192 halve batch; lower <code>setTempMemory</code>; retry twice; then quarantine batch. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a8-direct-cuvs-catalog-unchanged-surfaces","title":"A8. Direct cuVS catalog (unchanged surfaces)","text":"<p><code>cuvs.neighbors.{ivf_flat, ivf_pq, cagra, hnsw, brute_force, mg.*}</code>, <code>cuvs.cluster.kmeans</code>, <code>cuvs.distance.pairwise_distance</code>, <code>cuvs.common.Resources</code> \u2014 consistent <code>IndexParams</code>/<code>SearchParams</code>/<code>build</code>/<code>extend</code>/<code>save</code>/<code>load</code> patterns. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#a9-architecture-defaults-for-the-broader-system","title":"A9. Architecture defaults (for the broader system)","text":"<p>Keep the 2560\u2011d embedder, chunking settings, Parquet schemas, DuckDB catalog/migrations, and hybrid retrieval pipeline exactly as specified in your architecture doc; nothing about the cuVS enablement changes those contracts. </p> <p>If you\u2019d like, I can also output a drop\u2011in <code>vectorstore_faiss</code> module (Python package) that wraps the train/add/search/persist flow with cuVS\u2011first cloning and automatic bfKNN fallback, plus a tiny validator script that prints library load locations, shows <code>should_use_cuvs(...)</code>, and runs a 10k\u2011vector bfKNN smoke test on the RTX\u202f5090.</p> <p>Below is a comprehensive, example\u2011driven reference that fills in the remaining FAISS and cuVS Python surfaces we hadn\u2019t previously shown with code. It\u2019s organized so an agent can copy/paste snippets and understand shapes, params, edge\u2011cases, and performance notes. Where relevant, I point to the cuVS and loader docs you provided.   </p>"},{"location":"explanations/251025_FAISS_whl_overview/#a-faiss-cpuside-classes-utilities-with-examples","title":"A) FAISS \u2014 CPU\u2011side classes &amp; utilities (with examples)","text":""},{"location":"explanations/251025_FAISS_whl_overview/#a1-indexidmap-indexidmap2-stable-ids-around-any-index","title":"A1) <code>IndexIDMap</code> / <code>IndexIDMap2</code> \u2014 stable IDs around any index","text":"<p>Why: FAISS stores integer IDs; these wrappers let you control them.</p> <pre><code>import faiss, numpy as np\n\n# Base ANN index (any type works)\nd = 2560\nbase = faiss.index_factory(d, \"IVF8192,PQ64\", faiss.METRIC_INNER_PRODUCT)\n\n# Wrap with an ID map so we add our own 64-bit IDs\nidmap = faiss.IndexIDMap2(base)\n\n# Train as usual (on base)\ntrain = np.random.randn(1_000_000, d).astype('float32'); faiss.normalize_L2(train)\nidmap.train(train)\n\n# Add vectors with application-specific IDs\nxb = np.random.randn(500_000, d).astype('float32'); faiss.normalize_L2(xb)\nids = np.arange(100_000, 100_000 + xb.shape[0], dtype='int64')\nidmap.add_with_ids(xb, ids)\n\n# Search returns your IDs\nxq = np.random.randn(1000, d).astype('float32'); faiss.normalize_L2(xq)\nD, I = idmap.search(xq, k=10)\n</code></pre> <p>Notes</p> <ul> <li>Use <code>IndexIDMap2</code> (64\u2011bit IDs) for large corpora.</li> <li>Persist CPU form (see A6) and keep a parallel <code>.ids</code> copy if you ever unwrap the map. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a2-indexrefineflat-refine-ann-hits-with-exact-scoring","title":"A2) <code>IndexRefineFlat</code> \u2014 refine ANN hits with exact scoring","text":"<p>Why: Get IVF/graph latency with exact cosine/IP re\u2011ranking.</p> <pre><code># Build a fast coarse index (e.g., IVF,PQ) then refine with Flat\ncoarse = faiss.index_factory(d, \"IVF8192,PQ64\", faiss.METRIC_INNER_PRODUCT)\ncoarse.train(train); coarse.add(xb)\n\nrefined = faiss.IndexRefineFlat(coarse)   # wraps the coarse index with an exact refiner\nrefined.kfactor = 2.0                     # search 2\u00d7k in coarse stage, re-rank down to k\n\nD, I = refined.search(xq, k=10)\n</code></pre> <p>Notes</p> <ul> <li>On GPU you\u2019ll typically run the coarse stage on device, then refine on device or CPU depending on memory; the wrapper manages the sequence.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a3-indexpretransform-transforms-opqmatrix-pcamatrix","title":"A3) <code>IndexPreTransform</code> + transforms (<code>OPQMatrix</code>, <code>PCAMatrix</code>)","text":"<p>Why: Pre\u2011rotate or reduce dimension inside the index for better PQ fit.</p> <pre><code># 1) Build transforms: PCA (optional) then OPQ\npca = faiss.PCAMatrix(d_in=2560, d_out=1024, eigen_power=-0.5)  # example PCA step\nopq = faiss.OPQMatrix(d_out=1024, m=64)  # OPQ with 64 subquantizers\n\n# 2) Chain transforms in a vector\nvt = faiss.VectorTransformChain()\nvt.append(pca)\nvt.append(opq)\n\n# 3) Base index AFTER transforms (note d=1024 now)\nbase = faiss.index_factory(1024, \"IVF8192,PQ64\", faiss.METRIC_INNER_PRODUCT)\n\n# 4) Wrap into IndexPreTransform\npre = faiss.IndexPreTransform(vt, base)\n\n# 5) Train on original-dim data; transform is applied internally\npre.train(train)       # 'train' runs PCA/OPQ fitting + IVF/PQ training\npre.add(xb)\nD, I = pre.search(xq, 10)\n</code></pre> <p>Notes</p> <ul> <li>For our default blueprint we already use OPQ64; <code>IndexPreTransform</code> is the explicit form of that pipeline. Keep cosine/IP consistency: normalize before train/add/query. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a4-clustering-kmeans-build-coarse-quantizers-explicitly","title":"A4) <code>Clustering</code> (K\u2011means) \u2014 build coarse quantizers explicitly","text":"<p>Why: Manual control when you don\u2019t use <code>index_factory()</code>.</p> <pre><code># Train IVF coarse quantizer centroids explicitly with K-means\nnlist = 8192\nkmeans = faiss.Clustering(d, nlist)\nkmeans.niter = 25; kmeans.verbose = True\n\nquantizer = faiss.IndexFlatIP(d)        # cosine via normalization + IP\nkmeans.train(train, quantizer)\n\n# Use trained quantizer to build IVF, then attach PQ\nivf = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\nivf.train(train)\nivf.add(xb)\n</code></pre> <p>Notes</p> <ul> <li><code>index_factory()</code> automates this, but direct <code>Clustering</code> is useful for specialized training schedules.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a5-parameterspace-gpuparameterspace-named-tuning-knobs","title":"A5) <code>ParameterSpace</code> / <code>GpuParameterSpace</code> \u2014 named tuning knobs","text":"<p>Why: Set parameters (e.g., <code>nprobe</code>) by name, works across families.</p> <pre><code># CPU\nps = faiss.ParameterSpace()\nps.set_index_parameter(base, \"nprobe\", 64)\n\n# GPU\ngps = faiss.GpuParameterSpace(); gps.initialize(gpu_index)\ngps.set_index_parameter(gpu_index, \"nprobe\", 64)   # same name on GPU\n</code></pre> <p>Notes</p> <ul> <li>Prefer these over ad\u2011hoc setters; helps keep code index\u2011agnostic.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a6-persistence-write_index-read_index","title":"A6) Persistence \u2014 <code>write_index</code> / <code>read_index</code>","text":"<p>Why: Save CPU index for fast reload; re\u2011clone to GPU at runtime.</p> <pre><code>faiss.write_index(idmap, \"/data/faiss/shard_000.idx\")          # save CPU index (any type)\nidmap2 = faiss.read_index(\"/data/faiss/shard_000.idx\")         # load CPU index\n</code></pre> <p>Notes</p> <ul> <li>Don\u2019t save GPU indexes; reconstruct by cloning the CPU index on startup. Track paths and config in DuckDB (<code>faiss_indexes</code>). </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a7-range-search-range_search-aka-radius-search","title":"A7) Range search \u2014 <code>range_search</code> (a.k.a. radius search)","text":"<p>Why: Get all neighbors within a distance threshold instead of top\u2011K.</p> <pre><code>radius = 0.1  # for cosine/IP, pick meaningful threshold on normalized vectors\nrs = faiss.RangeSearchResult(xq.shape[0])\nidmap.range_search(xq, radius, rs)\n\n# Extract result per query\nlims, D, I = rs.lims, rs.distances, rs.labels\nfor qi in range(xq.shape[0]):\n    begin, end = lims[qi], lims[qi+1]\n    neigh_ids = I[begin:end]\n    neigh_dist = D[begin:end]\n</code></pre> <p>Notes</p> <ul> <li>Range search is less common for large\u2011scale serving, but handy for thresholded neighbor graphs.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a8-filtered-search-idselector-searchparametersivf","title":"A8) Filtered search \u2014 <code>IDSelector*</code> + <code>SearchParametersIVF</code>","text":"<p>Why: Exclude/allow subsets of IDs at query time.</p> <pre><code># Build a selector (ids in [lo, hi))\nsel = faiss.IDSelectorRange(lo=100_000, hi=150_000)\n\n# Tie selector to IVF search params\nsp = faiss.SearchParametersIVF()\nsp.nprobe = 64\nsp.sel = sel\n\n# Call the 3-arg search to pass params\nD, I = idmap.search(xq, 10, params=sp)\n</code></pre> <p>Notes</p> <ul> <li>With cuVS\u2011backed GPU indexes enabled (<code>use_cuvs=True</code> when cloning/constructing), FAISS converts these selectors into cuVS filters under the hood (your build includes the converter). </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a9-deletions-remove_ids","title":"A9) Deletions \u2014 <code>remove_ids</code>","text":"<p>Why: Remove a set of IDs (e.g., retractions, updates).</p> <pre><code>to_remove = faiss.IDSelectorRange(200_000, 210_000)  # OR IDSelectorBatch([...])\nn_removed = idmap.remove_ids(to_remove)\n</code></pre> <p>Notes</p> <ul> <li>IVF/PQ structures become \u201choley\u201d over time; periodic rebuilds (or re\u2011add to a fresh shard) keep performance consistent.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#a10-reconstruction-reconstruct-reconstruct_n","title":"A10) Reconstruction \u2014 <code>reconstruct</code>, <code>reconstruct_n</code>","text":"<p>Why: Retrieve vector approximations (exact for Flat/HNSW; approximate for PQ).</p> <pre><code>vec = idmap.reconstruct(int(I[0,0]))\nmat = idmap.reconstruct_n(0, 100)   # first 100 vectors\n</code></pre>"},{"location":"explanations/251025_FAISS_whl_overview/#a11-cpu-parallelism-omp_set_num_threads","title":"A11) CPU parallelism \u2014 <code>omp_set_num_threads</code>","text":"<p>Why: Pin thread count for CPU add/search/training.</p> <pre><code>faiss.omp_set_num_threads(14)  # match your server threads\n</code></pre>"},{"location":"explanations/251025_FAISS_whl_overview/#b-faiss-gpu-multigpu-functions-deeper-with-cuvs-enablement","title":"B) FAISS \u2014 GPU &amp; multi\u2011GPU functions (deeper, with cuVS enablement)","text":"<p>Reminder: In your wheel, FAISS GPU indexes can dispatch to cuVS for Flat, IVF\u2011Flat, IVF\u2011PQ, CAGRA, Binary\u2011CAGRA when you set <code>use_cuvs=True</code> on the GPU cloner or the GPU index config, and the cuVS libraries are preloaded. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#b1-cpugpu-cloning-single-multigpu","title":"B1) CPU\u2192GPU cloning (single &amp; multi\u2011GPU)","text":"<pre><code>import faiss\n\n# 1) Single GPU (cuVS-first, safe fallback)\nco = faiss.GpuClonerOptions(); co.use_cuvs = True\ntry:\n    gpu_index = faiss.index_cpu_to_gpu(res, 0, idmap, co)\nexcept RuntimeError:\n    co.use_cuvs = False\n    gpu_index = faiss.index_cpu_to_gpu(res, 0, idmap, co)\n\n# 2) Replicated (QPS-scale)\nngpu = faiss.get_num_gpus()\nresources = [faiss.StandardGpuResources() for _ in range(ngpu)]\nreplicas = faiss.IndexReplicas()\nfor dev in range(ngpu):\n    replicas.addIndex(faiss.index_cpu_to_gpu(resources[dev], dev, idmap, co))\n\n# 3) Sharded (capacity-scale)\nshards = faiss.IndexShards(d, threaded=True, successive_ids=False)\n# Build per-shard CPU indexes first, then clone each with co.use_cuvs=True and add to 'shards'\n</code></pre> <p>Notes</p> <ul> <li>Replicas improve QPS; shards expand capacity; wrap both under one logical index in your registry and fan\u2011out search. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#b2-direct-gpu-constructors-with-gpuindexconfiguse_cuvs","title":"B2) Direct GPU constructors with <code>GpuIndex*Config.use_cuvs</code>","text":"<p>Flat</p> <pre><code>cfg = faiss.GpuIndexFlatConfig(); cfg.use_cuvs = True\ngpu_flat = faiss.GpuIndexFlatIP(res, d, cfg)\n</code></pre> <p>IVF\u2011Flat / IVF\u2011PQ</p> <pre><code>ivf_cfg = faiss.GpuIndexIVFConfig(); ivf_cfg.use_cuvs = True\n# Pass ivf_cfg to the appropriate GpuIndexIVF* constructor for your metric/quantizer\n</code></pre> <p>CAGRA (graph ANN)</p> <pre><code># Prefer CPU\u2192GPU cloning unless you need specific GPU-only build options.\n# If constructing directly, set the config that inherits GpuIndexConfig:\n# cagra_cfg = faiss.GpuIndexCagraConfig(); cagra_cfg.use_cuvs = True\n# gpu_cagra = faiss.GpuIndexCagra(res, d, cagra_cfg)\n</code></pre> <p>Binary\u2011CAGRA</p> <ul> <li>Use the cloner (<code>GpuClonerOptions.use_cuvs=True</code>) when moving from CPU binary graph indexes to GPU.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#b3-gpu-distance-primitives-bfknn-knn_gpubfknn","title":"B3) GPU distance primitives \u2014 bfKNN (<code>knn_gpu</code>/<code>bfKnn</code>)","text":"<p>Exact GPU kNN (cuVS\u2011aware):</p> <pre><code>params = faiss.GpuDistanceParams(); params.metric = faiss.METRIC_INNER_PRODUCT\nparams.k = 10; params.dims = d\nuse_cuvs = bool(faiss.should_use_cuvs(params))     # central probe, then:\n\nD, I = faiss.knn_gpu(res, xq.astype('float32'), xb.astype('float32'),\n                     k=10, metric=faiss.METRIC_INNER_PRODUCT, use_cuvs=use_cuvs)\n</code></pre> <p>Notes</p> <ul> <li>Your build routes bfKNN to cuVS when possible; otherwise FAISS kernels are used. Preload cuVS so the probe can return True when viable. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#b4-gpu-tuning-knobs-gpuparameterspace-configs","title":"B4) GPU tuning knobs \u2014 <code>GpuParameterSpace</code> &amp; configs","text":"<ul> <li><code>GpuParameterSpace.set_index_parameter(gpu_index, \"nprobe\", 64)</code> \u2014 uniform way to set IVF probes.</li> <li> <p>IVFPQ config highlights:</p> </li> <li> <p><code>usePrecomputedTables</code> \u2014 faster scans at higher memory.</p> </li> <li><code>useFloat16LookupTables</code> \u2014 lower memory, slight precision trade\u2011off; useful for large <code>m</code> or tight VRAM budgets.</li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#c-faiss-contrib-helpers-youll-likely-use","title":"C) FAISS contrib helpers you\u2019ll likely use","text":"<p>(These ship in the python package and are handy for pipelines.)</p> <ul> <li><code>contrib.factory_tools</code> \u2014 convenience builders &amp; validators for factory strings.</li> <li><code>contrib.ivf_tools</code> \u2014 IVF diagnostics, centroid utilities.</li> <li><code>contrib.big_batch_search</code> \u2014 large\u2011batch search helpers to pipeline host\u2194device copies.</li> <li><code>contrib.ondisk</code> \u2014 on\u2011disk inverted lists for huge CPU\u2011side corpora.</li> </ul> <p>Use them to simplify scripts; your production path should still keep CPU index persisted + GPU clone at runtime as the source of truth. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#d-cuvs-functions-modules-we-hadnt-exemplified-now-with-code","title":"D) cuVS \u2014 functions &amp; modules we hadn\u2019t exemplified (now with code)","text":"<p>cuVS runs entirely on GPU with a consistent <code>build / extend / search / save / load</code> shape per algorithm. Provide a <code>Resources</code> handle to share a stream and avoid implicit syncs; pass device arrays for best performance. </p>"},{"location":"explanations/251025_FAISS_whl_overview/#d1-neighborsbrute_force-exact-knn-baseline-qa","title":"D1) <code>neighbors.brute_force</code> \u2014 exact kNN (baseline &amp; QA)","text":"<pre><code>import numpy as np\nfrom cuvs.common import Resources\nfrom cuvs.neighbors import brute_force\nfrom pylibraft.common import device_ndarray\n\nh = Resources()\nxb = device_ndarray(np.random.rand(1_000_000, 2560).astype(np.float32))\nxq = device_ndarray(np.random.rand(10_000,    2560).astype(np.float32))\n\nindex = brute_force.build(brute_force.IndexParams(metric=\"sqeuclidean\"), xb, resources=h)\n\nD = device_ndarray(np.empty((xq.shape[0], 10), np.float32))\nI = device_ndarray(np.empty((xq.shape[0], 10), np.int64))\nbrute_force.search(brute_force.SearchParams(metric=\"sqeuclidean\"),\n                   index, xq, k=10, distances=D, neighbors=I, resources=h)\n\n# Incremental ingest\nxb2 = device_ndarray(np.random.rand(200_000, 2560).astype(np.float32))\nbrute_force.extend(index, xb2, resources=h)\n\n# Persistence\nbrute_force.save(index, \"/tmp/cuvs_bruteforce.idx\")\nindex = brute_force.load(\"/tmp/cuvs_bruteforce.idx\", resources=h)\nh.sync()\n</code></pre> <p>Notes</p> <ul> <li>Good for ground truth during tuning and for small datasets. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#d2-neighborsivf_flat-faisslike-ivf-on-gpu","title":"D2) <code>neighbors.ivf_flat</code> \u2014 FAISS\u2011like IVF on GPU","text":"<pre><code>from cuvs.neighbors import ivf_flat\n\nh = Resources()\nxb = device_ndarray(np.random.rand(10_000, 2560).astype(np.float32))\nxq = device_ndarray(np.random.rand(1_000,  2560).astype(np.float32))\n\nidx = ivf_flat.build(ivf_flat.IndexParams(n_lists=8192, metric=\"sqeuclidean\"), xb, resources=h)\n\nD = device_ndarray(np.empty((xq.shape[0], 10), np.float32))\nI = device_ndarray(np.empty((xq.shape[0], 10), np.int64))\nivf_flat.search(ivf_flat.SearchParams(n_probes=64), idx, xq, k=10,\n                distances=D, neighbors=I, resources=h)\n\n# Add more\nivf_flat.extend(idx, device_ndarray(np.random.rand(5_000, 2560).astype(np.float32)), resources=h)\n\n# Save / load\nivf_flat.save(idx, \"/tmp/cuvs_ivfflat.idx\")\nidx2 = ivf_flat.load(\"/tmp/cuvs_ivfflat.idx\", resources=h)\nh.sync()\n</code></pre> <p>Notes</p> <ul> <li>Parameter names mirror FAISS concepts: <code>n_lists</code>, <code>n_probes</code>, metric. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#d3-neighborsivf_pq-ivf-with-product-quantization","title":"D3) <code>neighbors.ivf_pq</code> \u2014 IVF with Product Quantization","text":"<p>(We\u2019d shown build/search before; here\u2019s <code>extend/save/load</code> and host output conversion.) </p> <pre><code>from cuvs.neighbors import ivf_pq\n\nh = Resources()\nidx = ivf_pq.build(ivf_pq.IndexParams(n_lists=8192, metric=\"sqeuclidean\", pq_bits=8),\n                   xb, resources=h)\n\n# Extend incrementally\nivf_pq.extend(idx, device_ndarray(np.random.rand(20_000, 2560).astype(np.float32)), resources=h)\n\n# Search to host arrays (optional)\nD_dev = device_ndarray(np.empty((xq.shape[0], 10), np.float32))\nI_dev = device_ndarray(np.empty((xq.shape[0], 10), np.int64))\nivf_pq.search(ivf_pq.SearchParams(n_probes=64), idx, xq, k=10, distances=D_dev, neighbors=I_dev, resources=h)\nD_host, I_host = D_dev.copy_to_host(), I_dev.copy_to_host()\n\n# Save / load\nivf_pq.save(idx, \"/tmp/cuvs_ivfpq.idx\")\nidx = ivf_pq.load(\"/tmp/cuvs_ivfpq.idx\", resources=h)\nh.sync()\n</code></pre>"},{"location":"explanations/251025_FAISS_whl_overview/#d4-neighborscagra-highrecall-graph-ann","title":"D4) <code>neighbors.cagra</code> \u2014 high\u2011recall graph ANN","text":"<pre><code>from cuvs.neighbors import cagra\n\nh = Resources()\nxb = device_ndarray(np.random.rand(2_000_000, 2560).astype(np.float32))\nxq = device_ndarray(np.random.rand(20_000,    2560).astype(np.float32))\n\nidx = cagra.build(cagra.IndexParams(metric=\"sqeuclidean\"), xb, resources=h)\n\nD = device_ndarray(np.empty((xq.shape[0], 10), np.float32))\nI = device_ndarray(np.empty((xq.shape[0], 10), np.int64))\ncagra.search(cagra.SearchParams(), idx, xq, k=10, distances=D, neighbors=I, resources=h)\n\n# Extend / persist\ncagra.extend(idx, device_ndarray(np.random.rand(100_000, 2560).astype(np.float32)), resources=h)\ncagra.save(idx, \"/tmp/cuvs_cagra.idx\")\nidx = cagra.load(\"/tmp/cuvs_cagra.idx\", resources=h)\nh.sync()\n</code></pre> <p>Notes</p> <ul> <li>CAGRA is excellent for tight latency at high recall; memory\u2011heavier than PQ. Use it for premium tiers. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#d5-neighborshnsw","title":"D5) <code>neighbors.hnsw</code>","text":"<pre><code>from cuvs.neighbors import hnsw\n\nh = Resources()\nidx = hnsw.build(hnsw.IndexParams(metric=\"sqeuclidean\"), xb, resources=h)\nhnsw.search(hnsw.SearchParams(), idx, xq, k=10,\n            distances=D, neighbors=I, resources=h)\nhnsw.save(idx, \"/tmp/cuvs_hnsw.idx\")\n</code></pre> <p>Notes</p> <ul> <li>A familiar graph ANN baseline on GPU. Pick either CAGRA (higher perf) or HNSW depending on constraints. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#d6-filters-refine-tiered-indexes-highlevel-patterns","title":"D6) Filters, refine, tiered indexes (high\u2011level patterns)","text":"<ul> <li>Filters \u2014 apply post\u2011search restrictions or pre\u2011filter candidates with <code>neighbors.filters</code> utilities (e.g., by ID set). Use them for policy constraints or tenancy. </li> <li>Refine \u2014 re\u2011score ANN candidates with an exact step (<code>neighbors.refine</code>) for accuracy boosts. Good when you need Flat re\u2011ranking on a subset. </li> <li>Tiered index \u2014 orchestrate multi\u2011stage pipelines (e.g., IVF\u2011PQ \u2192 refine) using <code>neighbors.tiered_index</code> helpers. </li> </ul> <p>(Function names follow the <code>build/search/extend/save/load</code> theme; wire them like the examples above.)</p>"},{"location":"explanations/251025_FAISS_whl_overview/#d7-multigpu-neighborsmg-distributed-ivfpq-ivfflat-cagra","title":"D7) Multi\u2011GPU (<code>neighbors.mg</code>) \u2014 distributed IVF\u2011PQ / IVF\u2011Flat / CAGRA","text":"<p>Conceptual usage (keep the same <code>Resources</code> concept per device and use NCCL):</p> <pre><code>from cuvs.neighbors.mg import ivf_pq as mg_ivfpq\n# Build: mg_ivfpq.build(mg_ivfpq.IndexParams(...), dataset_per_gpu, resources=handles_per_gpu)\n# Search: mg_ivfpq.search(..., k=..., resources=handles_per_gpu)\n</code></pre> <p>Notes</p> <ul> <li>Requires NCCL. Use this path when one GPU\u2019s memory is insufficient or you need cluster\u2011like throughput while staying on a single multi\u2011GPU box. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#d8-distancepairwise_distance-gpu-distance-matrix","title":"D8) <code>distance.pairwise_distance</code> \u2014 GPU distance matrix","text":"<pre><code>from cuvs import distance\nDx = distance.pairwise_distance(xq, xb, metric=\"sqeuclidean\", resources=h)  # Dx on device by default\n</code></pre> <p>Notes</p> <ul> <li>Handy for diagnostics, unit tests, or custom scoring layers. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#d9-clusterkmeans-gpu-kmeans-centers-predict-cost","title":"D9) <code>cluster.kmeans</code> \u2014 GPU KMeans (centers, predict, cost)","text":"<pre><code>from cuvs.cluster import kmeans\n\ncenters, inertia, n_iters = kmeans.fit(kmeans.KMeansParams(n_clusters=8192), xb, resources=h)\nlabels = kmeans.predict(xb, centers, resources=h)\ncost = kmeans.cluster_cost(xb, centers, resources=h)\n</code></pre> <p>Notes</p> <ul> <li>Use for custom IVF training or other clustering tasks; interoperates with device arrays. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#e-libcuvs-loader-functions-env-youll-use-in-code","title":"E) libcuvs loader \u2014 functions &amp; env you\u2019ll use in code","text":"<p>Preload cuVS and RAPIDS libs once per process before any FAISS/cuVS calls to guarantee symbol resolution and allocator setup. </p> <pre><code>from libcuvs import load_library\nhandles = load_library()\nprint(\"Loaded:\", [h._name for h in handles])  # libcuvs.so, libcuvs_c.so, librmm.so, rapids_logger.so\n</code></pre> <p>Environment knobs (optional):</p> <ul> <li><code>RAPIDS_LIBCUVS_PREFER_SYSTEM_LIBRARY=true</code> \u2192 prefer system libs over wheel\u2011bundled copies.</li> <li>HybridSearch\u2019s <code>_ensure_cuvs_loader_path()</code> adds all <code>lib64/</code> dirs to <code>LD_LIBRARY_PATH</code> for child processes (inheritance is useful for workers). </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#f-endtoend-patterns-that-combine-the-above","title":"F) End\u2011to\u2011end patterns that combine the above","text":""},{"location":"explanations/251025_FAISS_whl_overview/#f1-filtered-refined-ivfpq-on-gpu-with-cuvs","title":"F1) Filtered, refined IVF\u2011PQ on GPU with cuVS","text":"<pre><code># 0) Preload cuVS (once)\nfrom libcuvs import load_library; load_library()                               # loader. :contentReference[oaicite:25]{index=25}\nimport faiss, numpy as np\n\nd=2560\ncpu = faiss.index_factory(d, \"OPQ64,IVF8192,PQ64\", faiss.METRIC_INNER_PRODUCT)\ntrain = np.random.randn(10_000_000, d).astype('float32'); faiss.normalize_L2(train)\ncpu.train(train)\n\n# ID map to carry your IDs\nidmap = faiss.IndexIDMap2(cpu)\nxb  = np.random.randn(20_000_000, d).astype('float32'); faiss.normalize_L2(xb)\nids = np.arange(1, 1+xb.shape[0], dtype='int64')\nidmap.add_with_ids(xb, ids)\n\n# Clone to GPU with cuVS enabled (fallback safe)\nres = faiss.StandardGpuResources(); res.setTempMemory(1_200_000_000); res.setPinnedMemory(512_000_000)\nco  = faiss.GpuClonerOptions(); co.use_cuvs=True\ntry: gpu = faiss.index_cpu_to_gpu(res, 0, idmap, co)\nexcept RuntimeError:\n    co.use_cuvs=False; gpu = faiss.index_cpu_to_gpu(res, 0, idmap, co)\n\n# Filter: only IDs in a range, then search &amp; refine\nsp = faiss.SearchParametersIVF(); sp.nprobe = 64; sp.sel = faiss.IDSelectorRange(100_000, 500_000)\nrefiner = faiss.IndexRefineFlat(gpu); refiner.kfactor = 2.0\n\nxq = np.random.randn(1000, d).astype('float32'); faiss.normalize_L2(xq)\nD, I = refiner.search(xq, 10, params=sp)\n</code></pre> <p>Why this works well for you</p> <ul> <li>Cosine via normalization + IP (2560\u2011d per architecture). </li> <li>cuVS path engaged for IVF\u2011PQ when available; otherwise FAISS kernels. </li> <li>Filter converted to cuVS filter internally in your build. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#f2-direct-cuvs-multigpu-ivfpq-outline","title":"F2) Direct cuVS multi\u2011GPU IVF\u2011PQ (outline)","text":"<pre><code># Pseudocode outline (patterns match single-GPU examples; use mg.ivf_pq)\nfrom cuvs.neighbors.mg import ivf_pq as mg_ivfpq\n# handles = [Resources() per GPU]; datasets split per GPU as device arrays\n# idx = mg_ivfpq.build(mg_ivfpq.IndexParams(n_lists=8192, metric=\"sqeuclidean\", pq_bits=8),\n#                      datasets, resources=handles)\n# mg_ivfpq.search(mg_ivfpq.SearchParams(n_probes=64), idx, queries, k=10,\n#                 distances=..., neighbors=..., resources=handles)\n</code></pre> <p>Notes</p> <ul> <li>Reach for multi\u2011GPU cuVS when a single GPU\u2019s VRAM is the bottleneck. </li> </ul>"},{"location":"explanations/251025_FAISS_whl_overview/#g-quick-checklist-for-agents-so-you-dont-miss-a-function-again","title":"G) Quick checklist for agents (so you don\u2019t miss a function again)","text":"<ul> <li>Always normalize embeddings for cosine/IP.</li> <li>Use <code>IndexIDMap2</code> to own IDs; <code>IndexRefineFlat</code> when you need exact re\u2011ranking.</li> <li>Set knobs with <code>ParameterSpace</code> / <code>GpuParameterSpace</code>.</li> <li>For filters, use <code>IDSelector*</code> + <code>SearchParametersIVF</code>; your build converts to cuVS automatically when cuVS is enabled. </li> <li>Persist CPU index with <code>write_index</code>; re\u2011clone to GPU on startup.</li> <li>Preload cuVS (<code>libcuvs.load_library()</code>), then set <code>use_cuvs=True</code> on cloner/config for Flat, IVF\u2011Flat, IVF\u2011PQ, CAGRA, Binary\u2011CAGRA; guard with try/except or <code>should_use_cuvs</code> for bfKNN. </li> <li>For direct cuVS: use <code>build/extend/search/save/load</code> in <code>neighbors.{ivf_flat, ivf_pq, cagra, hnsw, brute_force}</code> with device arrays and a shared <code>Resources</code> handle. </li> <li>Keep defaults from the architecture doc (2560\u2011d, <code>OPQ64,IVF8192,PQ64</code>, <code>nprobe=64</code>, shards \u226410\u202fM vectors). </li> </ul> <p>If you want, I can also package these into a reference Jupyter notebook that runs each snippet end\u2011to\u2011end (with small synthetic data) and prints sanity checks (dimensions, top\u2011K shapes, cuVS enabled flags).</p>"},{"location":"explanations/251025_HighLevelArchitecture/","title":"Implementation-Grade Architecture Overview","text":"<p>Below is the fully elaborated, implementation\u2011grade architecture for your end\u2011to\u2011end, single\u2011machine system. It updates every topic from the original plan and adds all new constraints you set:</p> <ul> <li>OS/Host: Ubuntu 24.04 (single box, no external servers).</li> <li>GPU: NVIDIA RTX 5090 (CUDA 13.0 toolchain).</li> <li>CPU/RAM: AMD 9950X (16 cores), 192\u202fGB RAM.</li> <li>Python: 3.13.</li> <li>PyTorch: 2.9 (CUDA 13 build).</li> <li>vLLM: latest pre\u2011release that supports CUDA 13.</li> <li>DuckDB: \u2265\u202f1.4.1 (interpreting your \u201c1.41\u201d as 1.4.1).</li> <li>Dense embeddings: Qwen3\u2011Embedding\u20114B at 2560\u2011dimensional output.</li> <li>Sparse embeddings: BM25 + SPLADE\u2011v3 (GPU).</li> <li>PDF\u2192DocTags &amp; chunking: Docling VLM (Granite\u2011Docling) + Docling HybridChunker.</li> <li>All embeddings in Parquet (no JSONL at rest).</li> <li>Vector indexing &amp; ops: FAISS GPU with cuVS enabled.</li> <li>Model serving: a single logical endpoint fronting two local vLLM processes (Granite\u2011Docling VLM + Qwen3\u2011Embedding\u20114B), routed by Nginx (necessary because one vLLM process \u2259 one base model).</li> <li>Registry: all artifacts (PDFs, DocTags, chunks, embeddings, indices, ontologies, links) registered in local DuckDB.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#0-big-picture-endtoend-singlebox","title":"0) Big picture (end\u2011to\u2011end, single\u2011box)","text":"<pre><code>Topic \u2192 (PyAlex) Harvest &amp; OA PDF Download (with fallbacks)\n     \u2192 Docling VLM (Granite\u2011Docling) \u2192 DocTags\n     \u2192 Docling HybridChunker \u2192 Chunk Parquet\n     \u2192 Dense (Qwen3\u2011Embedding\u20114B, 2560\u2011d via vLLM) \u2192 Parquet \u2192 FAISS (GPU, cuVS)\n     \u2192 Sparse (SPLADE\u2011v3, GPU \u2192 Parquet \u2192 Lucene impact) + BM25 (Lucene)\n     \u2192 Ontology ingest \u2192 Concept catalog + embeddings (dense 2560\u2011d + SPLADE)\n     \u2192 Chunk\u2013Concept linker \u2192 Assertions (Parquet) \u2192 KG (Neo4j local)\n     \u2192 Hybrid search API (FAISS + BM25 + SPLADE + KG\u2011aware rerank)\n     \u2192 Everything registered in DuckDB (\u22651.4.1)\n</code></pre> <p>Architectural style: Ports &amp; Adapters (Hexagonal) + Domain contracts (Pydantic v2) + Plugin registry (entry points) + Immutable, content\u2011addressed artifacts. All heavy compute runs locally (no remote compute/storage).</p>"},{"location":"explanations/251025_HighLevelArchitecture/#1-design-principles-concretized","title":"1) Design principles (concretized)","text":"<ul> <li>Interface\u2011first: every subsystem exposes an ABC (Abstract Base Class). Impl classes are pluggable via entry\u2011points.</li> <li>Encapsulation &amp; cohesion: one public fa\u00e7ade per package; internals hidden. Each module owns a single concern.</li> <li>Idempotency: outputs keyed by content hashes; re\u2011runs never duplicate artifacts.</li> <li>Determinism: global <code>seed=42</code>, fixed training samples for FAISS, fixed chunking parameters.</li> <li>All embeddings in Parquet: columnar, compressed with ZSTD=6, row_group=4096; no JSONL persisted.</li> <li>Observability from day one: OpenTelemetry traces; structured logs with artifact IDs; Prometheus counters/histograms.</li> <li>Local\u2011only: no remote indices or vector DBs; FAISS, Pyserini, Neo4j, DuckDB all on the box.</li> <li>Security (local): vLLM bound to localhost; Nginx fronts a single endpoint; API\u2011key auth for the search API.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#2-repository-layout-monorepo-independently-ownable-packages","title":"2) Repository layout (monorepo; independently ownable packages)","text":"<pre><code>/src\n  /kgfoundry_common            # contracts, IDs, hashing, config, utils\n  /download                  # PyAlex harvester + OA PDF downloader + fallbacks\n  /docling                   # VLM convert-to-DocTags + HybridChunker wrappers\n  /embeddings_dense          # vLLM client, Qwen3-Embedding-4B (2560-d)\n  /embeddings_sparse         # SPLADE-v3 GPU encoder (Parquet) + BM25/SPLADE indices (Pyserini)\n  /vectorstore_faiss         # FAISS GPU/cuVS index build/search adapters\n  /ontology                  # OWL/OBO/SKOS loaders, normalization, concept embeddings\n  /linking                   # candidate gen, scoring, calibration, assertions\n  /kg_builder                # Neo4j adapter; nodes/edges upsert\n  /search_api                # FastAPI; hybrid retrieval + KG-aware rerank; OpenAPI\n  /orchestration             # Prefect flows &amp; CLI commands (local)\n  /registry                  # DuckDB schema, migrations, dataset registration\n  /observability             # OTEL + Prometheus exporters\n/tests\n/config                      # YAML config(s), ngnix.conf, systemd units\n/scripts                     # bootstrap &amp; build scripts (CUDA 13, FAISS+cuVS, vLLM)\n</code></pre> <p>Packaging: <code>pyproject.toml</code> (Poetry or uv). Entry\u2011points register providers:</p> <pre><code>[project.entry-points.kgfoundry.plugins]\ndense.qwen3 = \"embeddings_dense.qwen3:Qwen3Embedder\"\nsparse.splade_v3 = \"embeddings_sparse.splade:SPLADEv3Encoder\"\nsparse.bm25 = \"embeddings_sparse.bm25:BM25Index\"\ndocling.vlm = \"docling.vlm:GraniteDoclingVLM\"\nchunker.docling_hybrid = \"docling.hybrid:HybridChunker\"\nvector.faiss_gpu = \"vectorstore_faiss.gpu:FaissGpuIndex\"\ngraph.neo4j = \"kg_builder.neo4j:Neo4jStore\"\nontology.loader = \"ontology.loader:OntologyLoader\"\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#3-core-data-contracts-pydantic-v2-contentaddressed-ids","title":"3) Core data contracts (Pydantic v2; content\u2011addressed IDs)","text":"<p>Keep models lean. Payloads (large text, vectors) live in Parquet files; rows refer to them via IDs.</p> <pre><code># kgfoundry_common/models.py (Python 3.13, Pydantic v2)\nfrom pydantic import BaseModel, Field, AwareDatetime\nfrom typing import Optional, List, Dict, Literal\nfrom dataclasses import dataclass\n\nId = str  # URN-like opaque IDs, stable and content-addressed\n\nclass Doc(BaseModel):\n    id: Id                           # urn:doc:sha256:&lt;16b&gt;\n    openalex_id: Optional[str] = None\n    doi: Optional[str] = None\n    arxiv_id: Optional[str] = None\n    pmcid: Optional[str] = None\n    title: str\n    authors: List[str] = []\n    pub_date: Optional[str] = None   # ISO8601\n    license: Optional[str] = None\n    language: Optional[str] = \"en\"\n    pdf_uri: str                     # local path (e.g., /data/pdfs/&lt;id&gt;.pdf)\n    source: str                      # 'openalex', 'arxiv', 'pmc', ...\n    content_hash: str                # sha256 of canonical text (after DocTags-&gt;text)\n    created_at: AwareDatetime\n\nclass DoctagsAsset(BaseModel):\n    doc_id: Id\n    doctags_uri: str                 # /data/doctags/&lt;doc_id&gt;.dt.json.zst\n    pages: int\n    vlm_model: str                   # granite-docling-258M\n    vlm_revision: str                # 'untied' if used\n    avg_logprob: Optional[float] = None\n    created_at: AwareDatetime\n\nclass Chunk(BaseModel):\n    id: Id                           # urn:chunk:&lt;doc_hash&gt;:&lt;start&gt;-&lt;end&gt;\n    doc_id: Id\n    section: Optional[str]\n    start_char: int\n    end_char: int\n    tokens: int\n    doctags_span: Dict[str, int]     # {node_id, start, end}\n    created_at: AwareDatetime\n    dataset_id: str                  # backpointer to Parquet dataset\n\nclass DenseVectorMeta(BaseModel):\n    chunk_id: Id\n    model: str                       # 'Qwen3-Embedding-4B'\n    run_id: str\n    dim: int                         # 2560\n    l2_norm: float\n    created_at: AwareDatetime\n\nclass SparseVectorMeta(BaseModel):\n    chunk_id: Id\n    model: str                       # 'SPLADE-v3-distilbert'\n    run_id: str\n    nnz: int\n    created_at: AwareDatetime\n\nclass Concept(BaseModel):\n    id: Id                           # urn:concept:&lt;ontology&gt;:&lt;curie&gt;\n    ontology: str\n    pref_label: str\n    alt_labels: List[str] = []\n    definition: Optional[str] = None\n    parents: List[Id] = []\n    meta: Dict[str, str] = {}\n\nclass LinkAssertion(BaseModel):\n    id: Id                           # urn:assert:&lt;chunk_id&gt;:&lt;concept_id&gt;@&lt;run_id&gt;\n    chunk_id: Id\n    concept_id: Id\n    score: float\n    decision: Literal['link','reject','uncertain']\n    evidence_span: Optional[str] = None\n    features: Dict[str, float] = {}  # dense_sim, sparse_sim, lexical_overlap, depth_bonus\n    run_id: str\n    created_at: AwareDatetime\n</code></pre> <p>ID scheme (deterministic)</p> <ul> <li><code>doc_id = urn:doc:sha256:&lt;first16 bytes of sha256 canonical_text, base32&gt;</code></li> <li><code>chunk_id = urn:chunk:&lt;doc_hash&gt;:&lt;start&gt;-&lt;end&gt;</code></li> <li><code>dense vec id = urn:vec:&lt;chunk_id&gt;:qwen3@&lt;run_id&gt;</code></li> <li><code>sparse id = urn:sparse:&lt;chunk_id&gt;:splade_v3@&lt;run_id&gt;</code></li> <li><code>concept id = urn:concept:&lt;ontology&gt;:&lt;curie&gt;</code></li> <li><code>assertion id = urn:assert:&lt;chunk_id&gt;:&lt;concept_id&gt;@&lt;run_id&gt;</code></li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#4-storage-layers-adapters-ports","title":"4) Storage layers &amp; adapters (ports)","text":"<p>VectorStore (FAISS GPU + cuVS)</p> <pre><code>class VectorStore(Protocol):\n    def train(self, train_vectors: \"np.ndarray\", **params) -&gt; None: ...\n    def add(self, keys: list[str], vectors: \"np.ndarray\") -&gt; None: ...\n    def search(self, query: \"np.ndarray\", k: int) -&gt; list[tuple[str, float]]: ...\n    def save(self, index_uri: str, idmap_uri: str) -&gt; None: ...\n    def load(self, index_uri: str, idmap_uri: str) -&gt; None: ...\n</code></pre> <p>SparseIndex</p> <pre><code>class SparseIndex(Protocol):\n    def build(self, docs_iterable: \"Iterable[SparseDoc]\") -&gt; None: ...\n    def search(self, query: str, k: int, fields: dict|None=None) -&gt; list[tuple[str, float]]: ...\n    def stats(self) -&gt; dict: ...\n</code></pre> <p>Implementations: BM25Index (Pyserini) and SpladeImpactIndex (Pyserini).</p> <p>GraphStore</p> <pre><code>class GraphStore(Protocol):\n    def upsert_nodes(self, docs: list[Doc], concepts: list[Concept], chunks: list[Chunk]) -&gt; None: ...\n    def upsert_mentions(self, assertions: list[LinkAssertion]) -&gt; None: ...\n    def neighbors(self, concept_id: str, depth:int=1) -&gt; list[str]: ...\n    def linked_concepts(self, chunk_id: str) -&gt; list[str]: ...\n</code></pre> <p>Registry (DuckDB \u22651.4.1)</p> <ul> <li>Provides DDL/migrations, safe registration (two\u2011phase commit: write Parquet \u2192 atomically register).</li> <li>Exposes views across Parquet datasets (union_by_name).</li> <li>Threading: default PRAGMA threads=14.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#5-orchestration-prefect-2x-all-local","title":"5) Orchestration (Prefect 2.x; all local)","text":"<p>Flows (idempotent, content\u2011hash keyed):</p> <ol> <li><code>harvest_and_download(topic, years, max_works)</code></li> <li><code>convert_to_doctags(doc_ids[])</code></li> <li><code>chunk_with_docling(doc_ids[])</code></li> <li><code>embed_dense_qwen3(chunk_dataset_id)</code></li> <li><code>encode_splade_v3(chunk_dataset_id)</code></li> <li><code>build_bm25_index(chunk_dataset_id)</code></li> <li><code>build_faiss_index(dense_run_id)</code></li> <li><code>ingest_ontologies(ontology_specs[])</code></li> <li><code>embed_concepts(ontology_id)</code></li> <li><code>link_chunks_to_concepts(chunk_dataset_id, ontology_id)</code></li> <li><code>upsert_kg(link_run_id)</code></li> <li><code>serve_search_api()</code> (starts uvicorn if not already)</li> </ol> <p>Events recorded in <code>registry.pipeline_events</code>: <code>DocumentIngested</code>, <code>DoctagsReady</code>, <code>ChunksCreated</code>, <code>DenseEmbedded</code>, <code>SpladeEncoded</code>, <code>BM25Built</code>, <code>FAISSBuilt</code>, <code>OntologyLoaded</code>, <code>ConceptEmbeddingsReady</code>, <code>LinkerRun</code>, <code>KGUpdated</code>.</p> <p>Concurrency defaults:</p> <ul> <li>Downloader 8 parallel fetches;</li> <li>VLM 2 docs in flight;</li> <li>Dense/SPLADE batchers tuned to ~80% VRAM;</li> <li>FAISS build: 2 shards concurrently;</li> <li>All others CPU\u2011bound threads = 14.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#6-harvesters-pdf-download-pyalex-first-resilient-fallbacks","title":"6) Harvesters &amp; PDF download (PyAlex first; resilient fallbacks)","text":"<p>Workflow</p> <ul> <li>Search: PyAlex (OpenAlex) by <code>topic</code> across title/abstract/fulltext; filter OA flags; optionally by year range.</li> <li>Extract candidate OA locations: <code>best_oa_location</code>, <code>primary_location</code>, <code>locations[]</code>, DOI, arXiv id, pmcid.</li> <li> <p>Download resolution (in order):</p> </li> <li> <p>OpenAlex <code>best_oa_location.pdf_url</code>.</p> </li> <li>Any <code>locations[].pdf_url</code> with <code>is_oa=true</code> favoring <code>publishedVersion</code> or <code>acceptedVersion</code>.</li> <li>Unpaywall by DOI (<code>url_for_pdf</code>) \u2014 requires configured contact email and rate limits.</li> <li> <p>Source\u2011specific:</p> <ul> <li>arXiv \u2192 <code>https://arxiv.org/pdf/&lt;id&gt;.pdf</code></li> <li>PubMed Central \u2192 <code>https://www.ncbi.nlm.nih.gov/pmc/articles/&lt;pmcid&gt;/pdf</code></li> <li>License guard: accept only OA\u2011compatible licenses; persist license string on <code>Doc</code>.</li> <li>Download: 8 concurrency; 60s timeout; 3 retries w/ exp backoff; <code>User-Agent</code> includes contact/email.</li> <li>Storage: <code>/data/pdfs/&lt;doc_id&gt;.pdf</code> where <code>doc_id</code> derived later from canonical text (post\u2011DocTags). Temporarily name by OpenAlex ID then rename after canonicalization.</li> <li>Registration: each successful PDF produces a <code>documents</code> row (openalex id, doi, license, pdf_uri, source).</li> </ul> </li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#7-pdf-doctags-docling-vlm-granitedocling","title":"7) PDF \u2192 DocTags (Docling VLM Granite\u2011Docling)","text":"<ul> <li>Serving: vLLM pre\u2011release (CUDA 13) process #1 on localhost:8001.</li> <li>Router: Nginx exposes <code>/vlm/* \u2192 8001</code>.</li> <li>Defaults: DPI=220, page_batch=8, bf16 if supported, fallback fp16.</li> <li>Quality gate: if avg token logprob &lt; 0.5 on a page \u2192 fallback OCR for that page; provenance notes retained.</li> <li>Timeouts: 120s per 100 pages; max 2000 pages.</li> <li>Outputs: <code>/data/doctags/&lt;doc_id&gt;.dt.json.zst</code>.</li> <li>Registration: DuckDB <code>doctags</code> with pages, model, revision (<code>untied</code> if applicable), avg_logprob.</li> </ul> <p>We intentionally present one logical model endpoint via Nginx, but use two local vLLM processes (one model each) due to vLLM\u2019s one\u2011model\u2011per\u2011process design.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#8-chunking-docling-hybridchunker","title":"8) Chunking (Docling HybridChunker)","text":"<ul> <li>Parameters: <code>target_tokens=400</code>, <code>overlap=80</code>, <code>min_tokens=120</code>, <code>max_tokens=480</code>.</li> <li>Structure\u2011aware segmentation first; spillover via sliding windows.</li> <li>Captures <code>doctags_span</code> and <code>start_char/end_char</code> for explainable highlights.</li> <li>Parquet dataset: <code>parquet/chunks/model=docling_hybrid/run=&lt;run_id&gt;/part-*.parquet</code> Schema:</li> </ul> <p><code>chunk_id: string   doc_id: string   section: string   start_char: int32   end_char: int32   doctags_span: struct&lt;node_id:string,start:int32,end:int32&gt;   text: string   tokens: int32   created_at: timestamp</code> * Register resulting dataset in <code>registry.datasets</code> (kind='chunks') and each row in <code>chunks</code>.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#9-dense-embeddings-qwen3embedding4b-2560dim-vllm","title":"9) Dense embeddings (Qwen3\u2011Embedding\u20114B @ 2560\u2011dim, vLLM)","text":"<ul> <li>Serving: vLLM pre\u2011release process #2 on localhost:8002; router maps <code>/v1/embeddings \u2192 8002</code>.</li> <li>Embedding length: 2560 (model supports 32\u20132560; set explicitly).</li> <li>Batching: automatic; target \u2264 80% of available VRAM.</li> <li>Normalization: L2; store norm (float32).</li> <li>Parquet dataset: <code>parquet/dense/model=Qwen3-Embedding-4B/run=&lt;run_id&gt;/part-*.parquet</code> Schema:</li> </ul> <p><code>chunk_id: string   model: string              -- 'Qwen3-Embedding-4B'   run_id: string   dim: int16                 -- 2560   vector: list&lt;float&gt;        -- length==2560 (float32 on disk)   l2_norm: float   created_at: timestamp</code> * Compression: ZSTD=6; row_group=4096; dictionary encoding on categorical cols.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#10-sparse-embeddings-indices-spladev3-gpu-bm25","title":"10) Sparse embeddings &amp; indices (SPLADE\u2011v3 GPU + BM25)","text":"<ul> <li> <p>SPLADE\u2011v3 encoder: <code>naver/splade-v3-distilbert</code></p> </li> <li> <p>CUDA (Torch 2.9, cu130), AMP fp16, <code>max_seq_len=512</code>, <code>topK=256</code> nnz per chunk.</p> </li> <li>Tokenizer: distilbert WordPiece, <code>vocab_size=30522</code>.</li> <li> <p>Parquet dataset: <code>parquet/sparse/model=SPLADE-v3-distilbert/run=&lt;run_id&gt;/part-*.parquet</code> Schema:</p> <p><code>chunk_id: string model: string run_id: string vocab_ids: list&lt;int32&gt;    -- sorted, unique weights:   list&lt;float&gt;    -- same length as vocab_ids nnz: int16 created_at: timestamp</code> * Indexing: Lucene impact index (Pyserini).</p> </li> <li> <p>No JSONL at rest: a streaming adapter reads Parquet rows and writes directly to Lucene via Pyserini builders (any temporary files are ephemeral and excluded from registry).</p> </li> <li>BM25: Pyserini (Lucene). Params: k1=0.9, b=0.4; field boosts <code>title^2.0, section^1.2, body^1.0</code>.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#11-faiss-gpu-with-cuvs-cuda-13","title":"11) FAISS (GPU with cuVS), CUDA 13","text":"<ul> <li>Install: Prefer GPU binary (wheel) built for CUDA 13 with cuVS enabled. If unavailable, build from source with <code>-DFAISS_ENABLE_GPU=ON -DFAISS_ENABLE_CUVS=ON</code>.</li> <li>Index factory (default): <code>OPQ64,IVF8192,PQ64</code> (good for d=2560; OPQ pre\u2011rotation at 64; PQ m=64).</li> <li>Training: sample 10M vectors or all (if fewer), seed=42.</li> <li>Search: <code>nprobe=64</code>. Codes 8\u2011bit per subvector.</li> <li>Memory: PQ codes ~64\u202fB/vector (+ID/overhead \u21d2 ~80\u2013100\u202fB/vector typical).</li> <li>Persistence: <code>.faiss</code> index + <code>.ids</code> idmap; shards \u226410M vectors each.</li> <li>Registration: rows in DuckDB <code>faiss_indexes</code> (logical index id groups shards).</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#12-ontology-ingestion-concept-encoders","title":"12) Ontology ingestion &amp; concept encoders","text":"<ul> <li>Formats: OWL/RDF (TTL/RDF\u2011XML), OBO, SKOS.</li> <li>Normalization: lowercase, NFC, strip most punctuation, lemmatize EN, stopword\u2011smart; build surface forms from <code>pref_label</code>, <code>alt_labels</code>, <code>synonyms</code>, and <code>definition</code>.</li> <li>Dense concept embeddings: Qwen3\u2011Embedding\u20114B with dim=2560 (concat text: <code>pref_label | definition | 5 best synonyms</code>).</li> <li>Sparse concept embeddings: SPLADE\u2011v3 with topK=128.</li> <li>Parquet datasets mirroring chunk schemas; registered in <code>concept_embeddings</code>.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#13-linker-chunkconcept-calibrated","title":"13) Linker (chunk\u2192concept), calibrated","text":"<p>Candidate generation</p> <ul> <li>SPLADE concept index @100 using chunk text;</li> <li>Lexical dictionary match @50 (max phrase len=6 tokens, case\u2011folded).</li> </ul> <p>Feature scoring</p> <ul> <li><code>dense_sim</code> = cosine(qwen_chunk, qwen_concept)</li> <li><code>sparse_sim</code> = normalized SPLADE (min\u2013max per query)</li> <li><code>lexical_overlap</code> = matched_chars / chunk_chars (clamp [0, 0.2])</li> <li><code>depth_bonus</code> = +0.02 \u00d7 levels from root (cap +0.10)</li> </ul> <p>Fusion &amp; decision</p> <ul> <li><code>score = 0.55*dense + 0.35*sparse + 0.10*lexical + depth_bonus</code></li> <li>Thresholds: link \u2265 0.62, reject \u2264 0.35, else uncertain.</li> <li>Calibration: isotonic regression stored per linker run; output <code>LinkAssertion</code> Parquet.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#14-knowledge-graph-neo4j-local","title":"14) Knowledge Graph (Neo4j, local)","text":"<p>Nodes</p> <ul> <li><code>(:Doc {doc_id, title, year, license, source, source_id})</code></li> <li><code>(:Chunk {chunk_id, section, start_char, end_char})</code></li> <li><code>(:Concept {concept_id, ontology, pref_label})</code></li> </ul> <p>Relationships</p> <ul> <li><code>(:Doc)-[:HAS_CHUNK]-&gt;(:Chunk)</code></li> <li><code>(:Chunk)-[:MENTIONS {score, run_id, evidence_span, created_at}]-&gt;(:Concept)</code></li> <li><code>(:Concept)-[:IS_A]-&gt;(:Concept)</code> and <code>[:RELATED_TO]</code> (from ontology).</li> </ul> <p>Constraints &amp; indexes</p> <ul> <li>Uniqueness on <code>doc_id</code>, <code>chunk_id</code>, <code>concept_id</code>; B\u2011tree index on <code>MENTIONS.score</code> and <code>MENTIONS.run_id</code>.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#15-hybrid-search-api-fastapi-local","title":"15) Hybrid search API (FastAPI, local)","text":"<p>Endpoints</p> <ul> <li><code>POST /search</code>   Body: <code>{query: str, k?: int=10, filters?: {...}, explain?: bool=false}</code>   Returns top chunks &amp; doc rollups with dense/sparse scores, KG boosts, spans, linked concepts.</li> <li><code>POST /graph/concepts</code> (browse ontology)</li> <li><code>GET /healthz</code></li> </ul> <p>Algorithm (fixed)</p> <ol> <li>Query embedding via Qwen3 (2560\u2011d) + parse lexical concept mentions.</li> <li>Retrieve dense@200 (FAISS) + sparse@200 (BM25 and SPLADE).</li> <li>Fuse via RRF(k=60).</li> <li>KG boosts: +0.08 for direct concept match; +0.04 for one\u2011hop neighbor.</li> <li>MMR (\u03bb=0.7) at doc level.</li> <li>Return top\u2011k.</li> </ol>"},{"location":"explanations/251025_HighLevelArchitecture/#16-configuration-yaml-singlebox-defaults","title":"16) Configuration (YAML; single\u2011box defaults)","text":"<pre><code>system:\n  os: ubuntu-24.04\n  threads: 14\n  seed: 42\n  parquet_root: /data/parquet\n  artifacts_root: /data/artifacts\n  duckdb_path: /data/catalog/catalog.duckdb\n\nruntime:\n  python: \"3.13\"\n  cuda: \"13.0\"\n  torch: \"2.9\"\n  duckdb_min: \"1.4.1\"\n  vllm_channel: \"pre-release-cuda13\"\n\nnetwork:\n  nginx_port: 80\n  vlm_port: 8001\n  emb_port: 8002\n  api_port: 8080\n\nharvest:\n  provider: pyalex\n  per_page: 200\n  max_works: 20000\n  years: \"&gt;=2018\"\n  filters:\n    is_oa: true\n    has_oa_published_version: true\n  fallbacks:\n    unpaywall: true\n    arxiv: true\n    pmc: true\n  concurrency: 8\n  timeout_sec: 60\n  retries: 3\n\ndoc_conversion:\n  vlm_model: ibm-granite/granite-docling-258M\n  vlm_revision: untied\n  endpoint: http://localhost/vlm/\n  dpi: 220\n  page_batch: 8\n  ocr_fallback: true\n  max_pages: 2000\n  timeout_sec: 120\n\nchunking:\n  engine: docling_hybrid\n  target_tokens: 400\n  overlap_tokens: 80\n  min_tokens: 120\n  max_tokens: 480\n\ndense_embedding:\n  model: Qwen/Qwen3-Embedding-4B\n  endpoint: http://localhost/v1/embeddings\n  output_dim: 2560\n  parquet_out: ${system.parquet_root}/dense/model=Qwen3-Embedding-4B/run=${run_id}\n\nsparse_embedding:\n  splade:\n    model: naver/splade-v3-distilbert\n    device: cuda\n    amp: fp16\n    max_seq_len: 512\n    topk: 256\n    parquet_out: ${system.parquet_root}/sparse/model=SPLADE-v3-distilbert/run=${run_id}\n  bm25:\n    k1: 0.9\n    b: 0.4\n    field_boosts: { title: 2.0, section: 1.2, body: 1.0 }\n    index_dir: /data/lucene/bm25\n\nfaiss:\n  index_factory: OPQ64,IVF8192,PQ64\n  nprobe: 64\n  train_samples: 10000000\n  shards:\n    max_vectors_per_shard: 10000000\n  gpu: true\n  cuvs: true\n  output_dir: /data/faiss/qwen3_ivfpq\n\nontology:\n  inputs:\n    - { ontology_id: mesh, format: obo, uri: /data/ontologies/mesh.obo }\n    - { ontology_id: go,   format: obo, uri: /data/ontologies/go.obo }\n  concept_embed:\n    dense_model: Qwen3-Embedding-4B\n    dense_dim: 2560\n    splade_model: SPLADE-v3-distilbert\n    splade_topk: 128\n\nlinker:\n  candidates: { splade_topk: 100, lexicon_topk: 50 }\n  fusion_weights: { dense: 0.55, sparse: 0.35, lexical: 0.10, depth_bonus_per_level: 0.02, depth_cap: 0.10 }\n  thresholds: { high: 0.62, low: 0.35 }\n  calibration: isotonic\n\ngraph:\n  backend: neo4j\n  uri: bolt://localhost:7687\n  user: neo4j\n  password_env: NEO4J_PASSWORD\n\nsearch:\n  k: 10\n  dense_candidates: 200\n  sparse_candidates: 200\n  rrf_k: 60\n  mmr_lambda: 0.7\n  kg_boosts: { direct: 0.08, one_hop: 0.04 }\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#17-duckdb-141-schema-views-full-fidelity","title":"17) DuckDB (\u2265\u202f1.4.1) schema &amp; views (full fidelity)","text":"<p>Tables (key ones)</p> <pre><code>CREATE TABLE model_registry (\n  model_id TEXT,\n  repo TEXT,\n  revision TEXT,\n  tokenizer TEXT,\n  embedding_dim INT,      -- 2560 for Qwen3 embeddings\n  vocab_size INT,         -- 30522 for SPLADE v3 tokenizer\n  framework TEXT,         -- 'vllm'|'hf'\n  framework_version TEXT,\n  build_info JSON,        -- FAISS flags, CUDA, cuVS info\n  PRIMARY KEY (model_id, revision)\n);\n\nCREATE TABLE runs (\n  run_id TEXT PRIMARY KEY,\n  purpose TEXT,           -- 'dense_embed'|'splade_encode'|'bm25_build'|'faiss_build'|...\n  model_id TEXT,\n  revision TEXT,\n  started_at TIMESTAMP,\n  finished_at TIMESTAMP,\n  config JSON\n);\n\nCREATE TABLE documents (\n  doc_id TEXT PRIMARY KEY,\n  openalex_id TEXT, doi TEXT, arxiv_id TEXT, pmcid TEXT,\n  title TEXT, authors JSON, pub_date TIMESTAMP,\n  license TEXT, language TEXT,\n  pdf_uri TEXT, source TEXT,\n  content_hash TEXT,         -- canonical text hash (after DocTags\u2192text)\n  created_at TIMESTAMP\n);\n\nCREATE TABLE doctags (\n  doc_id TEXT PRIMARY KEY REFERENCES documents(doc_id),\n  doctags_uri TEXT, pages INT,\n  vlm_model TEXT, vlm_revision TEXT,\n  avg_logprob DOUBLE,\n  created_at TIMESTAMP\n);\n\nCREATE TABLE datasets (\n  dataset_id TEXT PRIMARY KEY,\n  kind TEXT,                 -- 'chunks'|'dense'|'sparse'|'concepts'\n  parquet_root TEXT,\n  run_id TEXT REFERENCES runs(run_id),\n  created_at TIMESTAMP\n);\n\nCREATE TABLE chunks (\n  chunk_id TEXT PRIMARY KEY,\n  doc_id TEXT REFERENCES documents(doc_id),\n  section TEXT, start_char INT, end_char INT,\n  doctags_span JSON, tokens INT,\n  dataset_id TEXT REFERENCES datasets(dataset_id),\n  created_at TIMESTAMP\n);\n\nCREATE TABLE dense_runs (\n  run_id TEXT PRIMARY KEY REFERENCES runs(run_id),\n  model TEXT, dim INT,       -- 2560\n  parquet_root TEXT,\n  created_at TIMESTAMP\n);\n\nCREATE TABLE sparse_runs (\n  run_id TEXT PRIMARY KEY REFERENCES runs(run_id),\n  model TEXT, vocab_size INT,\n  parquet_root TEXT,\n  created_at TIMESTAMP,\n  backend TEXT               -- 'lucene-impact'|'lucene-bm25'\n);\n\nCREATE TABLE faiss_indexes (\n  logical_index_id TEXT,     -- groups shards into a single logical index\n  run_id TEXT REFERENCES dense_runs(run_id),\n  shard_id INT,\n  index_type TEXT, nlist INT, m INT, opq INT, nprobe INT,\n  gpu BOOLEAN, cuvs BOOLEAN,\n  index_uri TEXT, idmap_uri TEXT,\n  created_at TIMESTAMP,\n  PRIMARY KEY (logical_index_id, shard_id)\n);\n\nCREATE TABLE ontologies (\n  ontology_id TEXT PRIMARY KEY,\n  format TEXT, src_uri TEXT,\n  loaded_at TIMESTAMP, concept_count INT\n);\n\nCREATE TABLE concept_embeddings (\n  ontology_id TEXT REFERENCES ontologies(ontology_id),\n  model TEXT, dim INT,\n  parquet_root TEXT,\n  created_at TIMESTAMP,\n  PRIMARY KEY (ontology_id, model)\n);\n\nCREATE TABLE link_assertions (\n  id TEXT PRIMARY KEY,\n  chunk_id TEXT REFERENCES chunks(chunk_id),\n  concept_id TEXT,\n  score DOUBLE, decision TEXT,\n  evidence_span TEXT,\n  features JSON,\n  run_id TEXT REFERENCES runs(run_id),\n  created_at TIMESTAMP\n);\n\nCREATE TABLE pipeline_events (\n  event_id TEXT PRIMARY KEY,\n  event_name TEXT, subject_id TEXT, payload JSON,\n  created_at TIMESTAMP\n);\n\n-- Helpful indexes\nCREATE INDEX idx_chunks_doc ON chunks(doc_id);\nCREATE INDEX idx_link_chunk ON link_assertions(chunk_id);\nCREATE INDEX idx_link_concept ON link_assertions(concept_id);\n</code></pre> <p>Views (Parquet unification)</p> <pre><code>PRAGMA threads=14;\n\nCREATE OR REPLACE VIEW dense_vectors_view AS\nSELECT * FROM read_parquet(\n  (SELECT parquet_root FROM dense_runs), union_by_name=true);\n\nCREATE OR REPLACE VIEW splade_vectors_view AS\nSELECT * FROM read_parquet(\n  (SELECT parquet_root FROM sparse_runs WHERE backend='lucene-impact'), union_by_name=true);\n\nCREATE OR REPLACE VIEW chunk_texts AS\nSELECT * FROM read_parquet(\n  (SELECT parquet_root FROM datasets WHERE kind='chunks'), union_by_name=true);\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#18-vllm-nginx-single-logical-endpoint","title":"18) vLLM &amp; Nginx (single logical endpoint)","text":"<p>Nginx (local):</p> <pre><code>server {\n  listen 80;\n  server_name localhost;\n\n  location /vlm/ {              # Docling VLM\n    proxy_pass http://127.0.0.1:8001/;\n  }\n  location /v1/embeddings {     # Qwen3 embeddings (OpenAI-compatible)\n    proxy_pass http://127.0.0.1:8002/v1/embeddings;\n  }\n  location /healthz {\n    return 200 'ok\\n';\n  }\n}\n</code></pre> <p>vLLM processes (systemd units suggested)</p> <ul> <li>Granite\u2011Docling: <code>vllm serve ibm-granite/granite-docling-258M --revision untied --host 127.0.0.1 --port 8001 --dtype bfloat16</code></li> <li>Qwen3\u2011Embedding: <code>vllm serve Qwen/Qwen3-Embedding-4B --host 127.0.0.1 --port 8002 --dtype float16 --max-num-seqs 1024 --trust-remote-code</code></li> </ul> <p>(One model per vLLM process; router gives you a single host/endpoint.)</p>"},{"location":"explanations/251025_HighLevelArchitecture/#19-implementation-skeletons-selected","title":"19) Implementation skeletons (selected)","text":""},{"location":"explanations/251025_HighLevelArchitecture/#191-downloader-pyalex-fallbacks","title":"19.1 Downloader (PyAlex + fallbacks)","text":"<pre><code># download/harvester.py\nclass OpenAccessHarvester:\n    def __init__(self, cfg, http, unpaywall_client):\n        ...\n\n    def search_openalex(self, topic: str, years: str, max_works: int) -&gt; list[dict]:\n        # paginate PyAlex queries; collect candidate OA PDF URLs + metadata\n        ...\n\n    def resolve_pdf(self, work: dict) -&gt; str | None:\n        # try best_oa_location.pdf_url \u2192 locations[].pdf_url \u2192 unpaywall \u2192 arxiv/pmc\n        ...\n\n    def download_pdf(self, url: str, dest_path: str) -&gt; bool:\n        # 60s timeout, 3 retries, 8 concurrent workers\n        ...\n\n    def run(self, topic: str, years: str, max_works: int) -&gt; list[Doc]:\n        # returns registered documents (DuckDB insert)\n        ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#192-doctags-conversion","title":"19.2 DocTags conversion","text":"<pre><code># docling/vlm.py\nclass GraniteDoclingVLM:\n    def __init__(self, endpoint: str, dpi: int, page_batch: int, timeout: int, ocr_fallback: bool):\n        ...\n\n    def to_doctags(self, pdf_path: str) -&gt; dict:\n        # call vLLM endpoint; assemble DocTags JSON; OCR fallback per page if needed\n        ...\n\n    def persist(self, doc_id: str, doctags: dict) -&gt; str:\n        # write to /data/doctags/&lt;doc_id&gt;.dt.json.zst, return URI\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#193-chunking","title":"19.3 Chunking","text":"<pre><code># docling/hybrid.py\nclass HybridChunker:\n    def __init__(self, target: int=400, overlap: int=80, min_tokens: int=120, max_tokens: int=480):\n        ...\n    def chunk(self, doctags_uri: str) -&gt; list[Chunk]:\n        # parse, section-aware splitting, sliding windows, record spans/offsets\n        ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#194-dense-embeddings-qwen3-2560d-via-vllm","title":"19.4 Dense embeddings (Qwen3 2560\u2011d via vLLM)","text":"<pre><code># embeddings_dense/qwen3.py\nclass Qwen3Embedder:\n    name = \"Qwen3-Embedding-4B\"\n    dim = 2560\n    def __init__(self, endpoint: str):\n        ...\n    def embed_texts(self, texts: list[str]) -&gt; \"np.ndarray\":\n        # call /v1/embeddings; enforce dim=2560; L2-normalize\n        ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#195-spladev3-encoder-gpu","title":"19.5 SPLADE\u2011v3 encoder (GPU)","text":"<pre><code># embeddings_sparse/splade.py\nclass SPLADEv3Encoder:\n    def __init__(self, model_id: str, device=\"cuda\", topk=256, max_seq_len=512):\n        ...\n    def encode(self, texts: list[str]) -&gt; list[tuple[list[int], list[float]]]:\n        # torch.no_grad + autocast; prune to topK\n        ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#196-faiss-index-gpu-cuvs","title":"19.6 FAISS index (GPU + cuVS)","text":"<pre><code># vectorstore_faiss/gpu.py\nclass FaissGpuIndex:\n    def __init__(self, factory: str, nprobe: int, gpu: bool=True, cuvs: bool=True):\n        ...\n    def train(self, train_vectors: \"np.ndarray\") -&gt; None:\n        ...\n    def add(self, keys: list[str], vectors: \"np.ndarray\") -&gt; None:\n        ...\n    def search(self, query: \"np.ndarray\", k: int) -&gt; list[tuple[str, float]]:\n        ...\n    def save(self, index_uri: str, idmap_uri: str) -&gt; None:\n        ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#197-hybrid-retrieval-rerank-search_api","title":"19.7 Hybrid retrieval &amp; rerank (search_api)","text":"<pre><code># search_api/service.py\ndef hybrid_search(query: str, k: int) -&gt; list[dict]:\n    q_vec = dense_embedder.embed_texts([query])[0]\n    dense_hits = faiss.search(q_vec, k=200)\n    sparse_hits = bm25.search(query, k=200) + splade.search(query, k=200)\n    fused = rrf_fuse(dense_hits, sparse_hits, k=60)\n    boosted = apply_kg_boosts(fused, query)\n    results = mmr_deduplicate(boosted, lambda_=0.7)\n    return explain(results, k)\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#20-quality-gates-evaluation-ci","title":"20) Quality gates, evaluation &amp; CI","text":"<ul> <li>Retrieval: nDCG@10 \u2265 0.50 (baseline), Recall@1K \u2265 0.85.</li> <li>Linker: F1 \u2265 0.70 on a labeled set; ECE \u2264 0.08 after isotonic calibration.</li> <li>Latency: p95 search &lt; 300\u202fms (top\u201110); p50 chunk\u2192embed throughput tracked.</li> <li>CI: Unit tests (mypy, ruff), golden tests, nightly 1k\u2011doc mini\u2011pipeline; fail on regressions.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#21-security-licensing-governance-local","title":"21) Security, licensing, governance (local)","text":"<ul> <li>License guard at harvest; store license string in <code>documents.license</code>.</li> <li>vLLM binds to localhost only; Nginx is the only front door; API\u2011key auth on search API.</li> <li>Secrets (if any) from environment; never persisted.</li> <li>Provenance: every artifact row includes <code>{run_id, model_id, revision, created_at}</code>.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#22-capacity-sizing-with-2560dim","title":"22) Capacity &amp; sizing with 2560\u2011dim","text":"<ul> <li>Dense Parquet: \u2248 10\u202fKB/chunk pre\u2011compression (2560 \u00d7 4\u202fB).</li> <li>IVF\u2011PQ (PQ64) codes: \u2248 64\u202fB/vector (+ID/overhead ~80\u2013100\u202fB).</li> <li>SPLADE postings: ~2\u202fKB/chunk before Lucene compression (256 nnz).</li> <li>Scale by chunk count (e.g., 5M chunks \u2192 ~50\u202fGB dense Parquet uncompressed; index is small enough for GPU).</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#23-operational-runbooks","title":"23) Operational runbooks","text":"<ul> <li>CUDA 13 + Torch 2.9: install CUDA 13, confirm <code>nvcc --version</code>, install cu130 wheels.</li> <li>vLLM pre\u2011release: install channel with CUDA\u201113 support; start two processes; verify health on <code>:8001</code>, <code>:8002</code>; Nginx routes.</li> <li>FAISS GPU + cuVS: install GPU binary (preferred); if needed, build from source with CMake flags enabling cuVS.</li> <li> <p>Directories:</p> </li> <li> <p>PDFs: <code>/data/pdfs</code></p> </li> <li>DocTags: <code>/data/doctags</code></li> <li>Parquet: <code>/data/parquet</code></li> <li>Lucene: <code>/data/lucene/{bm25,splade}</code></li> <li>FAISS: <code>/data/faiss</code></li> <li>DuckDB: <code>/data/catalog/catalog.duckdb</code></li> <li>Resource limits: <code>ulimit -n 65535</code>; I/O scheduler <code>mq-deadline</code>; set <code>PRAGMA threads=14</code> in DuckDB.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#24-workstreams-implementation-plans-deliverables","title":"24) Workstreams (implementation plans &amp; deliverables)","text":"<ol> <li> <p>Downloader &amp; Harvester (PyAlex + fallbacks)</p> </li> <li> <p>PyAlex client; OA filters; DOI/ID extract; Unpaywall integration; robust downloader; license guard; DuckDB registration.</p> </li> <li> <p>Deliverables: module, config, tests (mock HTTP), metrics (<code>pdf_download_success_total</code>, <code>oa_resolution_latency</code>).</p> </li> <li> <p>Docling VLM &amp; DocTags</p> </li> <li> <p>vLLM integration; page batching; OCR fallback; DocTags writer; quality metrics (avg logprob/page).</p> </li> <li> <p>Deliverables: module, Nginx+vLLM configs, DDL updates, golden DocTags samples.</p> </li> <li> <p>Chunking (Docling Hybrid)</p> </li> <li> <p>Hybrid configuration; token accounting; offsets/spans; Parquet writer; golden chunk fixtures.</p> </li> <li> <p>Dense embeddings (Qwen3 2560\u2011d)</p> </li> <li> <p>vLLM client; batching &amp; normalization; Parquet writer; memory instrumentation; retries.</p> </li> <li> <p>SPLADE\u2011v3 GPU encoder + Pyserini indices</p> </li> <li> <p>Torch 2.9 encoder; Parquet encoder; streaming into Lucene (impact); BM25 build; query APIs.</p> </li> <li> <p>FAISS GPU/cuVS</p> </li> <li> <p>Binary installation or source build; index builder (OPQ64,IVF8192,PQ64); training sampler; shard manager; search adapter.</p> </li> <li> <p>Ontology &amp; concept embeddings</p> </li> <li> <p>Loaders (OWL/OBO/SKOS); normalization; concept Parquet; Qwen3(2560) + SPLADE encoders.</p> </li> <li> <p>Linker</p> </li> <li> <p>Candidate gen; fusion; calibration (isotonic); assertions Parquet; explainability; ablations.</p> </li> <li> <p>KG Builder (Neo4j)</p> </li> <li> <p>Node/edge upserts; constraints; indexes; maintenance; graph query helpers.</p> </li> <li> <p>Search API</p> <ul> <li>FastAPI; hybrid retrieval; RRF; KG boosts; MMR; explanations; OpenAPI schema; auth.</li> </ul> </li> <li> <p>Registry (DuckDB)</p> <ul> <li>Migrations; registration helpers; views across Parquet; integrity checks; provenance guards.</li> </ul> </li> <li> <p>Orchestration &amp; Observability</p> <ul> <li>Prefect flows; retries; dashboards; tracing; alerting; local startup scripts.</li> </ul> </li> </ol>"},{"location":"explanations/251025_HighLevelArchitecture/#25-coding-standards-bestinclass-python-practices","title":"25) Coding standards &amp; best\u2011in\u2011class Python practices","text":"<ul> <li>Static typing everywhere (<code>mypy --strict</code>).</li> <li>Pydantic v2 for all externalized contracts; validate at boundaries only.</li> <li>Dataclasses/NamedTuples for internal\u2011only small records on hot paths.</li> <li>Pure functions for transformations; side effects behind adapters.</li> <li>Dependency inversion: constructors accept interfaces, not concretes.</li> <li>Factories resolve plugins from entry\u2011points; no <code>if/elif</code> on provider names in business logic.</li> <li>Error handling: wrap external calls in <code>Result[T, E]</code>\u2011like helpers (or <code>try/except</code> mapping to domain errors).</li> <li>Logging: structured (<code>json</code>); include <code>run_id</code>, <code>doc_id</code>, <code>chunk_id</code>.</li> <li>Testing: unit + contract tests per port; \u201cfake\u201d in\u2011memory providers; golden tests for DocTags/chunks; end\u2011to\u2011end smoke.</li> <li>Performance: vector ops on GPU; streaming I/O; Parquet row groups sized for cache locality; avoid pandas on hot paths.</li> <li>Docs: every public class/method has docstrings; README per package; ADRs for significant choices.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#addendum-additional-architecture-information","title":"Addendum - additional architecture information","text":""},{"location":"explanations/251025_HighLevelArchitecture/#part-a-gap-analysis-whats-unspecified-resolutions-in-addendum","title":"PART A \u2014 GAP ANALYSIS (WHAT\u2019S UNSPECIFIED) \u2192 RESOLUTIONS IN ADDENDUM","text":"Area Gap / ambiguity Why it matters Final specification (summary) Canonical text &amp; ID hashing Exact canonicalization pipeline from DocTags to text (line breaks, whitespace, Unicode norms) not fixed; ID hashing step unclear. Reproducibility; stable <code>doc_id</code>, <code>chunk_id</code>. Define canonicalizer (NFC \u2192 collapse whitespace \u2192 normalize bullets/ligatures \u2192 Unix line endings). Hash canonical text SHA\u2011256, use first 16 bytes (base32) for URNs. (\u00a7B1) Tokenizer for chunk sizes Which tokenizer governs <code>target_tokens</code> not pinned. Chunk consistency, embedding costs. Use Qwen3\u2011Embedding tokenizer via HF <code>AutoTokenizer</code> for token counts. (\u00a7B2) DocTags fidelity Which DocTags nodes to include/exclude (headers/footers/refs), and OCR fallback merge rules unclear. Content quality; linkability back to pages. Include body, headings, tables, captions, figure text; exclude references by default. OCR per\u2011page fallback with provenance flag; page order preserved. (\u00a7B3) Downloader resolution order &amp; de\u2011dupe Ties between multiple OA locations; DOI/arXiv/PMCID overlaps; file dedup not specified. Avoid duplicate processing and wasted space. Stable priority list (OpenAlex best_oa \u2192 other locations \u2192 Unpaywall \u2192 source\u2011specific), MIME check; compute <code>pdf_sha256</code>; deduplicate by hash; symlink duplicate DoIs. (\u00a7B4) PyAlex topic query spec Which OpenAlex fields are searched; year filters and pagination; retries/timeouts not fixed. Recall and reliability. Search <code>title, abstract_inverted_index, fulltext</code> with AND semantics; filter by <code>from_year..to_year</code>; <code>per_page=200</code>; backoff retries 3\u00d7; http timeout 30\u202fs; polite <code>User-Agent</code> and mailto. (\u00a7B4) Qwen3 embeddings 2560\u2011d Request parameter to enforce 2560 output; failure behavior if unsupported. Index shape; FAISS train. Request <code>dimension=2560</code>; assert response dim; if unsupported (model mismatch), fail fast and record incident. (\u00a7B5) SPLADE\u2011v3 GPU encoder Pre\u2011processing (lowercasing, punctuation), truncation, batching, AMP, and top\u2011K pruning details not pinned. Index size and quality; repeatability. Lowercase; strip control chars; keep punctuation; truncation at 512 WP tokens; AMP fp16; batch size auto\u2011tuned; top\u2011K=256 with stable tiebreaker (token id). (\u00a7B6) BM25 analyzer Analyzer pipeline (tokenizer, stopwords, stemming, casefold) not fixed. Matching behavior and score comparability. Lucene StandardTokenizer; English minimal stemming; lowercase; English stoplist; synonyms off by default (can be added per domain). (\u00a7B6) Parquet schemas &amp; partitioning Columns, dtypes, compression, row group size, directory partitioning not fully fixed. IO performance; schema drift. Fixed schemas listed; ZSTD=6, <code>row_group=4096</code>; partition by <code>model</code>, <code>run_id</code>, <code>shard</code>; filename <code>part-&lt;nnnnn&gt;.parquet</code>. (\u00a7B7) DuckDB catalog Migrations, integrity checks, FK behaviors, thread settings unspecified. Registry reliability. Versioned migrations (<code>registry/migrations/*.sql</code>), FK constraints, indexes, <code>PRAGMA threads=14</code>, two\u2011phase registration. (\u00a7B8) FAISS details (GPU+cuVS) Factory choice confirmed, but training sample selection, OPQ params, memory budgeting, shard policy not fully pinned. Performance &amp; determinism. <code>OPQ64,IVF8192,PQ64</code>, train on 10M random (seed 42); <code>nprobe=64</code>; \u226410M vectors/shard; GPU add/search; save <code>.faiss</code> + <code>.ids</code>. (\u00a7B9) vLLM topology Routing is known, but request/response schemas, timeouts, retries, health checks not fully specified. Client stability. OpenAI\u2011compatible <code>/v1/embeddings</code>; per\u2011request timeout 30\u202fs; 3\u00d7 retries; health endpoints <code>/v1/models</code> &amp; <code>/health</code>; router (Nginx) config pinned; backpressure policy documented. (\u00a7B10) Ontology ingestion Supported predicates for labels/synonyms/defs, CURIE mapping, deprecations, and merges not fully specified. Correct concept catalog and IDs. Accept SKOS (<code>prefLabel</code>,<code>altLabel</code>), OBO (<code>hasExactSynonym</code>,<code>def</code>), RDFS labels; support <code>deprecated</code> and <code>replaced_by</code>; CURIE mapping table; normalize text; unique <code>urn:concept:&lt;ont&gt;:&lt;curie&gt;</code>. (\u00a7B11) Linker calibration Dev set size, fold policy, calibration storage, and runtime application not explicit. Score interpretability. Dev set 2,000 chunk\u2011concept pairs labeled; 5\u2011fold isotonic regression; parameters stored per <code>linker_run_id</code>; applied at runtime; ECE reported. (\u00a7B12) Hybrid fusion math RRF <code>k</code>, candidate pool sizes, KG boost exact math not completely fixed. Deterministic results. Dense@200 + Sparse@200; RRF(k=60); KG boosts: direct +0.08, 1\u2011hop +0.04; MMR \u03bb=0.7 at doc level. (\u00a7B13) API design Only endpoint sketch; errors, pagination, filters, sorting, rate limits, auth, and OpenAPI not fully specified. Independent backend/frontend work. Full OpenAPI spec, error schema, pagination, filters (year, source, license), API\u2011key auth, per\u2011key rate limits (local token bucket), JSON response formats. (\u00a7B14) End\u2011to\u2011end testing What E2E scenarios and fixtures to use; success criteria per stage unclear. Integration confidence. Provide seed corpora (10 docs), synthetic ontologies, golden outputs; E2E pipeline test that asserts all DDLs, artifact counts, index sizes, retrieval metrics over thresholds. (\u00a7B15) Observability Log schema, metric names, labels, exemplar traces, dashboards unspecified. Ops readiness. Define metrics (names, types), logs (JSON schema), traces (span names &amp; attributes); provide Grafana panels JSON. (\u00a7B16) Failure handling Error taxonomy, retry matrices, poison\u2011pill protocol, quarantine dirs not fixed. Robustness under real corpora. Standard error classes; retry/backoff tables; quarantine locations; incident log table and CLI. (\u00a7B17) Security &amp; licensing Key storage, license enforcement points, audit fields not formalized. Compliance and reproducibility. <code>.env</code> for secrets; license filters in downloader; persist license string and OA source; audit table records provenance; API keys in env; localhost binding. (\u00a7B18) Project structure &amp; code quality Pre\u2011commit, linters, typing level, docstrings, ADRs, contribution workflow not locked. Team velocity &amp; consistency. Pre\u2011commit (ruff, black, mypy\u2011strict), conventional commits, ADR template, mkdocs site, contribution guide, codeowners. (\u00a7B19) Local dev &amp; bootstrap No exact bootstrap steps and Make targets. Fast onboarding. Provide <code>scripts/bootstrap.sh</code>, <code>Makefile</code> targets, systemd units for vLLM + Nginx, folder layout creation. (\u00a7B20) Performance SLAs Hard SLAs and perf targets partially stated. Planning &amp; regression gates. SLAs/SLOs formalized per stage; CI gates with thresholds; perf budgets by stage. (\u00a7B21)"},{"location":"explanations/251025_HighLevelArchitecture/#part-b-exhaustive-specifications-resolutions","title":"PART B \u2014 EXHAUSTIVE SPECIFICATIONS (RESOLUTIONS)","text":""},{"location":"explanations/251025_HighLevelArchitecture/#b1-canonical-text-hashing-ids","title":"B1. Canonical text, hashing &amp; IDs","text":"<ul> <li> <p>Canonicalizer (DocTags \u2192 text):</p> </li> <li> <p>Decode UTF\u20118; apply Unicode NFC.</p> </li> <li>Replace Windows/old Mac line breaks with <code>\\n</code>.</li> <li>Collapse runs of whitespace to a single space except keep single <code>\\n</code> line breaks between block nodes (paragraph/heading/list/table caption).</li> <li>Normalize bullets (<code>\u2022</code>, <code>\u25e6</code>, <code>\u2013</code>) to <code>-</code>.</li> <li>Strip page headers/footers using DocTags region types; exclude References section by default (configurable).</li> <li>Retain figure/table captions, equations (inline LaTeX left as\u2011is), and footnotes (inlined at end of page).</li> <li>doc_id: <code>urn:doc:sha256:&lt;first16B base32&gt;</code> over canonical text (not PDF bytes).</li> <li>chunk_id: <code>urn:chunk:&lt;doc_hash&gt;:&lt;start&gt;-&lt;end&gt;</code> using character offsets in canonical text (inclusive start, exclusive end).</li> <li>Hashes computed with SHA\u2011256; extraction reproducible.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b2-tokenizer-chunking-quantization","title":"B2. Tokenizer &amp; chunking quantization","text":"<ul> <li>Tokenizer of record: HuggingFace tokenizer for Qwen/Qwen3\u2011Embedding\u20114B.</li> <li>Chunker parameters (fixed defaults): <code>target=400</code>, <code>overlap=80</code>, <code>min=120</code>, <code>max=480</code> Qwen tokens; stride=<code>target-overlap=320</code>.</li> <li>If a section is shorter than 120 tokens and can be merged with its successor without exceeding 480, merge; otherwise leave standalone.</li> <li>Sliding spill\u2011windows never cross major section boundaries (title/abstract/intro/methods/results/discussion).</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b3-doctags-selection-ocr-fallback","title":"B3. DocTags selection &amp; OCR fallback","text":"<ul> <li>Include: title, authors, abstract, section headings, paragraphs, lists, tables, figures\u2019 captions, equations.</li> <li>Exclude by default: acknowledgements, references; can enable via <code>config.doc_conversion.include_references=true</code>.</li> <li>OCR: page\u2011level fallback using Tesseract when Docling VLM avg logprob &lt; 0.5; OCR text anchored into DocTags page node with <code>provenance=\"ocr\"</code>.</li> <li>Provenance fields in DoctagsAsset: <code>{avg_logprob, ocr_pages:[int]}</code>.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b4-harvester-downloader-pyalex-first-with-fallbacks","title":"B4. Harvester &amp; downloader (PyAlex first, with fallbacks)","text":"<ul> <li> <p>PyAlex search:</p> </li> <li> <p>Query compiled as AND of: <code>topic</code> (applied to <code>title</code>, <code>abstract_inverted_index</code>, <code>fulltext</code>), optional <code>year &gt;= from_year</code>, <code>is_oa=true OR has_oa_published_version=true OR has_oa_accepted_or_published_version=true</code>.</p> </li> <li>Pagination: <code>per_page=200</code>, resume with <code>cursor</code>, cap at <code>max_works</code>.</li> <li>HTTP timeout=30\u202fs, retries=3 (exponential 1.0/2.0/4.0\u202fs), <code>User-Agent: kgfoundry/1.0 (+contact@example.com)</code>.</li> <li> <p>Resolution priority (first success wins):</p> </li> <li> <p><code>best_oa_location.pdf_url</code> (OpenAlex).</p> </li> <li>Any <code>locations[].pdf_url</code> with <code>is_oa=true</code> (prefer <code>publishedVersion</code> &gt; <code>acceptedVersion</code>).</li> <li>Unpaywall by DOI \u2192 <code>url_for_pdf</code>.</li> <li>Source\u2011specific fallback: arXiv (<code>/pdf/&lt;id&gt;.pdf</code>), PMC article PDF.</li> <li>MIME + size checks: Require <code>application/pdf</code>; max size 100\u202fMB (configurable).</li> <li>De\u2011dup: compute <code>pdf_sha256</code> of bytes; if duplicate of an existing doc, link new metadata to existing file and record alias (<code>openalex_id</code>, <code>doi</code>, etc.).</li> <li>Registration: Insert into <code>documents</code> with all source IDs, <code>pdf_uri</code>, license, OA source, and timestamp.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b5-dense-embeddings-qwen3-2560d","title":"B5. Dense embeddings (Qwen3 2560\u2011d)","text":"<ul> <li>API: OpenAI\u2011compatible <code>/v1/embeddings</code> with body:</li> </ul> <p><code>json   {\"model\":\"Qwen/Qwen3-Embedding-4B\",\"input\":[ \"...chunk text...\" ],\"dimensions\":2560}</code> * Response validation: assert vector length 2560; otherwise fail run with incident record. * Batching: dynamic batch size uses VRAM probe; keep \u226480% VRAM. * Normalization: client L2\u2011normalizes vector (float32) and stores original L2. * Errors: HTTP 429/5xx \u2192 retries (3\u00d7); timeouts \u2192 retries; after 3 failures, quarantine batch.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#b6-sparse-spladev3-gpu-bm25-details","title":"B6. Sparse (SPLADE\u2011v3 GPU) &amp; BM25 details","text":"<ul> <li> <p>SPLADE\u2011v3 encoder:</p> </li> <li> <p>Pre\u2011proc: lowercase; strip control chars (<code>\\x00</code>..<code>\\x1f</code>), keep punctuation; whitespace collapse single space; no stopword removal.</p> </li> <li>Tokenization: DistilBERT WordPiece; <code>max_seq_len=512</code> (truncate tail).</li> <li>AMP: <code>torch.autocast(device_type=\"cuda\", dtype=torch.float16)</code>.</li> <li>Batch size: auto\u2011tuned per GPU memory; cap at 2048 tokens/batch.</li> <li>Top\u2011K pruning: select top 256 weights by value; tie\u2011break by token id asc; sort ascending token ids for indexer.</li> <li> <p>BM25 (Pyserini/Lucene):</p> </li> <li> <p>Analyzer: StandardTokenizer \u2192 Lowercase \u2192 EnglishMinimalStemFilter \u2192 EnglishStopFilter.</p> </li> <li>Params: <code>k1=0.9</code>, <code>b=0.4</code>.</li> <li>Fields: <code>title</code> (boost 2.0), <code>section</code> (1.2), <code>body</code> (1.0). Title from DocTags title node; section from chunk.section; body from chunk text.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b7-parquet-datasets-all-embeddings-and-chunks","title":"B7. Parquet datasets (ALL embeddings and chunks)","text":"<ul> <li>Common options: <code>compression=\"ZSTD\"</code>, level=6; <code>row_group_size=4096</code>; <code>use_dictionary=True</code> for categorical columns (<code>model</code>, <code>run_id</code>, <code>doc_id</code>, <code>section</code>).</li> <li>Dense:</li> </ul> <p><code>chunk_id: string, model: string, run_id: string, dim: int16 (=2560),   vector: list&lt;float&gt;, l2_norm: float, created_at: timestamp</code></p> <p>Partitions: <code>model=Qwen3-Embedding-4B/run_id=&lt;run&gt;/shard=&lt;nnnn&gt;</code>. * SPLADE:</p> <p><code>chunk_id: string, model: string, run_id: string,   vocab_ids: list&lt;int32&gt;, weights: list&lt;float&gt;, nnz: int16, created_at: timestamp</code></p> <p>Partitions: <code>model=SPLADE-v3-distilbert/run_id=&lt;run&gt;/shard=&lt;nnnn&gt;</code>. * Chunks:</p> <p><code>chunk_id, doc_id, section, start_char:int32, end_char:int32,   doctags_span: struct&lt;node_id:string,start:int32,end:int32&gt;,   text:string, tokens:int32, created_at:timestamp</code></p> <p>Partition: <code>model=docling_hybrid/run_id=&lt;run&gt;/shard=&lt;nnnn&gt;</code>.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#b8-duckdb-registry-141-migrations-guards","title":"B8. DuckDB registry (\u2265\u202f1.4.1): migrations &amp; guards","text":"<ul> <li>Migrations: SQL files in <code>/registry/migrations</code>, applied by <code>registry apply-migrations</code>. Each migration increments <code>schema_version</code> table.</li> <li>Two\u2011phase registration: write Parquet to temp dir \u2192 validate row counts &amp; schema \u2192 <code>BEGIN</code> \u2192 insert into <code>datasets</code>/<code>runs</code> \u2192 <code>COMMIT</code> \u2192 rename dir into place; on failure <code>ROLLBACK</code> and delete temp.</li> <li>Threading: <code>PRAGMA threads=14</code>; all read_parquet with <code>union_by_name=true</code>.</li> <li>Integrity: FKs and unique constraints as defined.</li> <li>Views: <code>dense_vectors_view</code>, <code>splade_vectors_view</code>, <code>chunk_texts</code> pre\u2011created.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b9-faiss-gpu-cuvs-cuda-13","title":"B9. FAISS GPU + cuVS (CUDA 13)","text":"<ul> <li>Factory: <code>OPQ64,IVF8192,PQ64</code> (for d=2560).</li> <li>Train: Random sample of min(10M, 100% of corpus) dense vectors; seed=42; normalize before train.</li> <li>Add: On GPU; batch of N where memory \u2264 80% VRAM; record adds per shard in DuckDB.</li> <li>Search: <code>nprobe=64</code>; return distances as IP or L2 consistent with training (we use IP on normalized vectors \u2192 cosine).</li> <li>Persist: <code>.faiss</code> index + <code>.ids</code> idmap per shard; <code>faiss_indexes</code> table records <code>logical_index_id</code>, <code>shard_id</code>, file paths, config.</li> <li>Merge: logical index = set of shards; search fans out to shards, merges top\u2011K.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b10-vllm-servers-router","title":"B10. vLLM servers &amp; router","text":"<ul> <li>Granite\u2011Docling (VLM): vLLM process #1 on <code>127.0.0.1:8001</code>.</li> <li>Qwen3 embeddings: vLLM process #2 on <code>127.0.0.1:8002</code>.</li> <li>Router: Nginx on <code>:80</code> maps <code>/vlm/*</code> \u2192 8001 and <code>/v1/embeddings</code> \u2192 8002.</li> <li>Health checks: <code>/healthz</code> on router; <code>/v1/models</code> on each vLLM.</li> <li>Client policy: per\u2011request timeout 30\u202fs, retries \u00d73 (429/5xx), exponential backoff 1/2/4\u202fs; circuit\u2011breaker opens after 10 consecutive failures (per\u2011service) for 30\u202fs.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b11-ontology-ingestion-catalog","title":"B11. Ontology ingestion &amp; catalog","text":"<ul> <li>Formats: OBO, OWL/RDF (TTL, RDF/XML), SKOS (RDF).</li> <li> <p>Predicates:</p> </li> <li> <p>Labels: <code>rdfs:label</code>, <code>skos:prefLabel</code>.</p> </li> <li>Synonyms: <code>oboInOwl:hasExactSynonym</code>, <code>skos:altLabel</code>.</li> <li>Definitions: <code>IAO:0000115</code>, <code>skos:definition</code>.</li> <li>Hierarchy: <code>rdfs:subClassOf</code>, <code>skos:broader</code>, <code>BFO:part_of</code> (normalized to <code>IS_A</code> or <code>PART_OF</code>).</li> <li>Deprecation: <code>owl:deprecated</code> true; replacements: <code>iao:0100001</code> or custom <code>replaced_by</code>.</li> <li>CURIEs: Provide <code>prefix \u2192 base IRI</code> map per ontology; compute <code>urn:concept:&lt;ont&gt;:&lt;CURIE&gt;</code>.</li> <li>Normalization: lowercased, NFC, punctuation trimmed (except hyphen and slash), English lemmatization.</li> <li> <p>Concept embeddings:</p> </li> <li> <p>Dense (Qwen3 2560\u2011d): text = <code>pref_label | definition | top5_synonyms</code>; same Parquet schema as chunks (swap <code>chunk_id \u2192 concept_id</code>).</p> </li> <li>Sparse (SPLADE): text same as above; topK=128.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b12-linker-calibration-decision-policy","title":"B12. Linker calibration &amp; decision policy","text":"<ul> <li>Labeling: 2,000 chunk\u2011concept candidate pairs labeled by domain curators.</li> <li>Splits: 5\u2011fold CV; train isotonic regression to map fused score \u2192 calibrated [0,1].</li> <li>Thresholds: <code>\u03c4_high=0.62</code>, <code>\u03c4_low=0.35</code> applied after calibration.</li> <li>Outputs: save calibration params in DuckDB as JSON attached to <code>run_id</code> (linker run).</li> <li>Metrics: precision/recall/F1; Expected Calibration Error (ECE).</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b13-hybrid-retrieval-fusion-fixed-math","title":"B13. Hybrid retrieval fusion (fixed math)","text":"<ul> <li>Retrieval: <code>dense@200</code> (FAISS), <code>sparse@200</code> (BM25 and SPLADE each produce lists; we interleave by highest score then take top 200 combined sparse).</li> <li>RRF: <code>RRF_k = 60</code>. Score = <code>\u03a3 1/(k + rank_i)</code> over participating rankers (dense, bm25, splade).</li> <li>KG boost: +0.08 if any linked concept of chunk \u2208 query concepts; +0.04 if within one ontology hop; max boost +0.12.</li> <li>MMR: doc\u2011level, <code>\u03bb=0.7</code>; similarity measured by cosine of mean chunk vectors per doc.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b14-search-api-openapi-31-final","title":"B14. Search API \u2014 OpenAPI 3.1 (final)","text":"<ul> <li>Auth: Bearer API key header <code>Authorization: Bearer &lt;token&gt;</code> (tokens read from env <code>SEARCH_API_KEYS</code>, comma\u2011separated).</li> <li>Rate limits: token\u2011bucket: 120 req/min per key; 1,000 req/day per key (in\u2011memory counters, process\u2011local).</li> <li> <p>Endpoints:</p> </li> <li> <p><code>POST /search</code></p> <ul> <li>Body:</li> </ul> <p><code>json   {     \"query\": \"string\",     \"k\": 10,     \"filters\": {\"year_from\": 2018, \"year_to\": 2025, \"source\": [\"openalex\",\"arxiv\"], \"license\":[\"CC-BY\",\"CC0\"]},     \"explain\": false   }</code> * 200 Response (per result):</p> <p><code>json   {     \"doc_id\": \"urn:doc:...\",     \"chunk_id\": \"urn:chunk:...\",     \"title\": \"string\",     \"section\": \"Methods\",     \"score\": 2.381,     \"signals\": {\"dense\": 0.73, \"sparse\": 0.61, \"rrf\": 0.025, \"kg_boost\": 0.08},     \"spans\": {\"start_char\": 120, \"end_char\": 420},     \"concepts\": [{\"concept_id\":\"urn:concept:mesh:D012345\",\"label\":\"...\",\"match\":\"direct\"}]   }</code> * Errors: <code>400</code> (bad input), <code>401</code> (auth), <code>429</code> (rate limit), <code>500</code> (server).   * <code>POST /graph/concepts</code></p> <ul> <li>Body: <code>{\"q\":\"string\",\"limit\":50}</code> \u2192 returns matching concept IDs + labels + paths to root.</li> <li><code>GET /healthz</code> \u2192 <code>{status:\"ok\", components:{faiss:\"ok\", bm25:\"ok\", vllm_embeddings:\"ok\", neo4j:\"ok\"}}</code>.</li> </ul> </li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b15-endtoend-stage-testing","title":"B15. End\u2011to\u2011end &amp; stage testing","text":"<ul> <li> <p>Seed fixtures:</p> </li> <li> <p>10 OA PDFs (mix arXiv/PMC/journal) on disk for offline tests.</p> </li> <li>Tiny ontology (50 concepts, 2 levels) + MeSH subset (for integration).</li> <li>Golden outputs: a) DocTags JSON; b) chunk Parquet; c) sample dense/sparse Parquets; d) FAISS index shards; e) SPLADE/BM25 Lucene dirs.</li> <li> <p>E2E test (<code>pytest -m e2e</code>):</p> </li> <li> <p>Harvest &amp; download (mock pyalex/unpaywall): expect N PDFs.</p> </li> <li>DocTags conversion: expect pages&gt;0, DocTags file exists, avg_logprob recorded.</li> <li>Chunking: expect chunks&gt;0; token counts within bounds; offsets monotonic.</li> <li>Dense embed: expect vectors dim=2560; Parquet rows==chunks.</li> <li>SPLADE encode &amp; BM25 index: Lucene dirs exist; postings &gt;0.</li> <li>FAISS build: index files exist; search for \u201ctopic\u201d returns &gt;0 results.</li> <li>Ontology ingest + concept embeddings: counts &gt;0; CURIEs valid.</li> <li>Linker: assertions produced; at least X% linked.</li> <li>KG upsert: node/edge counts as expected.</li> <li>Search API: <code>/search</code> returns 200 and k results; latency p95 &lt; 300\u202fms on fixtures.</li> <li>Stage unit tests: contract tests per ABC; property\u2011based tests (idempotency; stable IDs); golden tests for DocTags/chunks; benchmarking tests for encoder throughput.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b16-observability-diagnostics","title":"B16. Observability &amp; diagnostics","text":"<ul> <li>Logs: JSON lines with fields: <code>ts, level, module, event, run_id, doc_id, chunk_id, duration_ms, err_code, message</code>.</li> <li> <p>Metrics (Prometheus):</p> </li> <li> <p>Counters: <code>pdf_download_success_total</code>, <code>pdf_download_failure_total{reason}</code>, <code>chunks_total</code>, <code>dense_vectors_total</code>, <code>splade_vectors_total</code>, <code>faiss_queries_total</code>, <code>bm25_queries_total</code>, <code>splade_queries_total</code>.</p> </li> <li>Histograms: <code>download_latency_ms</code>, <code>doctags_latency_ms</code>, <code>chunking_latency_ms</code>, <code>embed_dense_latency_ms</code>, <code>splade_encode_latency_ms</code>, <code>faiss_search_latency_ms</code>, <code>search_total_latency_ms</code>.</li> <li>Gauges: <code>faiss_index_size_vectors</code>, <code>lucene_docs_count</code>, <code>duckdb_open_files</code>.</li> <li> <p>Tracing (OpenTelemetry):</p> </li> <li> <p>Spans: <code>harvest.search</code>, <code>download.pdf</code>, <code>docling.to_doctags</code>, <code>chunking.hybrid</code>, <code>embed.qwen3</code>, <code>encode.splade</code>, <code>index.faiss.add</code>, <code>index.lucene.build</code>, <code>linker.run</code>, <code>kg.upsert</code>, <code>api.search</code>.</p> </li> <li>Attributes: <code>{run_id, doc_id, shard_id, model, dim, nprobe, nnz}</code>.</li> <li> <p>Dashboards: provide JSON exports for:</p> </li> <li> <p>Ingest Health (download rates/errors).</p> </li> <li>Pipeline Throughput (chunks/min, embeddings/min).</li> <li>Search SLOs (p50/p95/p99 latency, error rate).</li> <li>Index Footprint (FAISS vectors, Lucene docs).</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b17-failure-handling-resilience","title":"B17. Failure handling &amp; resilience","text":"<ul> <li> <p>Error taxonomy:</p> </li> <li> <p><code>DownloadError</code>, <code>UnsupportedMIMEError</code>, <code>DoclingError</code>, <code>OCRTimeout</code>, <code>ChunkingError</code>, <code>EmbeddingError</code>, <code>SpladeOOM</code>, <code>FaissOOM</code>, <code>IndexBuildError</code>, <code>OntologyParseError</code>, <code>LinkerCalibrationError</code>, <code>Neo4jError</code>.</p> </li> <li> <p>Retry/backoff table:</p> </li> <li> <p>External HTTP (download, vLLM): retries=3, backoff 1/2/4\u202fs.</p> </li> <li>GPU OOM: reduce batch by 50%, retry up to 2 times; if still fails \u2192 quarantine.</li> <li>FAISS add error: split shard; retry once per split.</li> <li> <p>Quarantine:</p> </li> <li> <p>PDFs: <code>/data/quarantine/pdfs/\u2026</code></p> </li> <li>Parquet batches: <code>/data/quarantine/parquet/\u2026</code></li> <li>Incidents table: <code>registry.incidents(event, subject_id, error_class, message, created_at)</code>.</li> <li>Poison pill: if a <code>doc_id</code> fails at stage X twice, mark <code>documents.status='poison'</code> and exclude from subsequent runs until manual override.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b18-security-licensing-config-secrets","title":"B18. Security, licensing, config secrets","text":"<ul> <li>Local\u2011only network binding for vLLM and Neo4j.</li> <li>API keys for Search API read from env <code>SEARCH_API_KEYS</code> (comma\u2011separated).</li> <li>Licenses stored verbatim from source; <code>documents.license</code> required for downstream processing; refuse embedding/indexing if missing or not OA.</li> <li>.env template for secrets and contact email; never commit real secrets.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b19-code-quality-governance","title":"B19. Code quality &amp; governance","text":"<ul> <li>Pre\u2011commit: <code>ruff</code>, <code>black</code>, <code>mypy --strict</code>, <code>pyupgrade</code>, <code>interrogate</code> (docstring coverage).</li> <li>Type discipline: <code>from __future__ import annotations</code>, Protocol/ABC for interfaces, <code>Final</code> constants.</li> <li>Docs: <code>mkdocs</code> site with API docs (pdoc or mkdocstrings), how\u2011to guides, and ADRs (<code>/docs/adr/0001-...md</code>).</li> <li>Conventional commits and semantic versioning per package.</li> <li>Codeowners by directory.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#b20-bootstrap-devex","title":"B20. Bootstrap &amp; DevEx","text":"<ul> <li> <p>Makefile targets:</p> </li> <li> <p><code>make bootstrap</code> \u2192 create venv, install deps, install CUDA\u201113 Torch wheels, build/install FAISS GPU/cuVS (or fetch wheel), install vLLM prerelease, create dirs under <code>/data/*</code>, apply migrations, install systemd units for vLLM and Nginx.</p> </li> <li><code>make run</code> \u2192 start router, start API, ensure vLLM services up.</li> <li><code>make e2e</code> \u2192 run the full fixture pipeline &amp; API checks.</li> <li><code>make clean</code> \u2192 remove indexes and temp outputs.</li> <li>Systemd units: <code>vllm-vlm.service</code>, <code>vllm-embed.service</code>, <code>nginx.service</code>, <code>search-api.service</code>.</li> <li>Directory layout:</li> </ul> <p><code>/data/pdfs   /data/doctags   /data/parquet/{chunks,dense,sparse,concepts}   /data/lucene/{bm25,splade}   /data/faiss   /data/catalog/catalog.duckdb   /data/quarantine/{pdfs,parquet}</code></p>"},{"location":"explanations/251025_HighLevelArchitecture/#b21-slasslos-performance-budgets","title":"B21. SLAs/SLOs &amp; performance budgets","text":"<ul> <li>Downloader: \u2265 95% of accessible OA PDFs succeed within 60\u202fs; retries included.</li> <li>DocTags: p95 \u2264 6\u202fs/10 pages (Granite\u2011Docling, RTX\u202f5090).</li> <li>Chunking: \u2265 5,000 chunks/min CPU.</li> <li>Dense embedding: \u2265 8,000 chunks/min (2560\u2011d) on RTX\u202f5090 under 80% VRAM.</li> <li>SPLADE encoding: \u2265 5,000 chunks/min (AMP).</li> <li>FAISS search: p95 \u2264 40\u202fms for 200\u2011NN; build time \u2264 2\u202fh per 10M vectors shard.</li> <li>Search API: p95 end\u2011to\u2011end \u2264 300\u202fms for <code>k=10</code>, concurrency 64.</li> <li>Linker run: \u2265 1M chunk\u2011concept candidate scorings/hour; ECE \u2264 0.08; F1 \u2265 0.70 on dev set.</li> <li>CI gates: fail if nDCG@10 drops &gt; 0.02 from last green or p95 latency \u2191 &gt; 20%.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#part-c-interfaces-module-contracts-ready-for-coding","title":"PART C \u2014 INTERFACES &amp; MODULE CONTRACTS (READY FOR CODING)","text":"<p>Below are Python interface signatures (type\u2011checked; exceptions documented). All implementations must adhere to these contracts.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#c1-registry-duckdb-registryapipy","title":"C1. Registry (DuckDB) \u2014 <code>registry/api.py</code>","text":"<pre><code>class Registry(Protocol):\n    def begin_dataset(self, kind: str, run_id: str) -&gt; str: ...\n    def commit_dataset(self, dataset_id: str, parquet_root: str, rows: int) -&gt; None: ...\n    def rollback_dataset(self, dataset_id: str) -&gt; None: ...\n\n    def insert_run(self, purpose: str, model_id: str | None, revision: str | None, config: dict) -&gt; str: ...\n    def close_run(self, run_id: str, success: bool, notes: str | None = None) -&gt; None: ...\n\n    def register_documents(self, docs: list[Doc]) -&gt; None: ...\n    def register_doctags(self, assets: list[DoctagsAsset]) -&gt; None: ...\n    def register_chunks(self, dataset_id: str, chunk_rows: int) -&gt; None: ...\n    def register_dense_run(self, run_id: str, model: str, dim: int, parquet_root: str) -&gt; None: ...\n    def register_sparse_run(self, run_id: str, model: str, vocab_size: int, parquet_root: str, backend: str) -&gt; None: ...\n    def register_faiss_shard(self, logical_index_id: str, run_id: str, shard_id: int, cfg: dict, index_uri: str, idmap_uri: str) -&gt; None: ...\n\n    def emit_event(self, event_name: str, subject_id: str, payload: dict) -&gt; None: ...\n    def incident(self, event: str, subject_id: str, error_class: str, message: str) -&gt; None: ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#c2-downloader-downloadharvesterpy","title":"C2. Downloader \u2014 <code>download/harvester.py</code>","text":"<pre><code>class Harvester(Protocol):\n    def search(self, topic: str, years: str, max_works: int) -&gt; list[dict]: ...\n    def resolve_pdf(self, work: dict) -&gt; str | None: ...\n    def download_pdf(self, url: str, target_path: str) -&gt; str: ...  # returns final path\n    def run(self, topic: str, years: str, max_works: int) -&gt; list[Doc]: ...\n</code></pre> <p>Exceptions: <code>DownloadError</code>, <code>UnsupportedMIMEError</code>.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#c3-doctags-chunking-doclingvlmpy-doclinghybridpy","title":"C3. DocTags &amp; chunking \u2014 <code>docling/vlm.py</code>, <code>docling/hybrid.py</code>","text":"<pre><code>class DocTagsConverter(Protocol):\n    def to_doctags(self, pdf_path: str) -&gt; dict: ...\n    def persist(self, doc_id: str, doctags: dict) -&gt; str: ...\n\nclass Chunker(Protocol):\n    def chunk(self, doctags_uri: str) -&gt; list[Chunk]: ...\n</code></pre> <p>Exceptions: <code>DoclingError</code>, <code>OCRTimeout</code>, <code>ChunkingError</code>.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#c4-embeddings-embeddings_denseqwen3py-embeddings_sparsespladepy-embeddings_sparsebm25py","title":"C4. Embeddings \u2014 <code>embeddings_dense/qwen3.py</code>, <code>embeddings_sparse/splade.py</code>, <code>embeddings_sparse/bm25.py</code>","text":"<pre><code>class DenseEmbedder(Protocol):\n    name: str\n    dim: int\n    def embed_texts(self, texts: list[str]) -&gt; \"np.ndarray\": ...\n\nclass SparseEncoder(Protocol):\n    name: str\n    def encode(self, texts: list[str]) -&gt; list[tuple[list[int], list[float]]]: ...\n\nclass SparseIndex(Protocol):\n    def build(self, docs_iterable: \"Iterable[tuple[str, dict]]\") -&gt; None: ...\n    def search(self, query: str, k: int, fields: dict | None = None) -&gt; list[tuple[str, float]]: ...\n</code></pre> <p>Exceptions: <code>EmbeddingError</code>, <code>SpladeOOM</code>, <code>IndexBuildError</code>.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#c5-vector-store-vectorstore_faissgpupy","title":"C5. Vector store \u2014 <code>vectorstore_faiss/gpu.py</code>","text":"<pre><code>class VectorStore(Protocol):\n    def train(self, train_vectors: \"np.ndarray\", *, factory: str, seed: int = 42) -&gt; None: ...\n    def add(self, keys: list[str], vectors: \"np.ndarray\") -&gt; None: ...\n    def search(self, query: \"np.ndarray\", k: int) -&gt; list[tuple[str, float]]: ...\n    def save(self, index_uri: str, idmap_uri: str) -&gt; None: ...\n    def load(self, index_uri: str, idmap_uri: str) -&gt; None: ...\n</code></pre> <p>Exceptions: <code>FaissOOM</code>, <code>IndexBuildError</code>.</p>"},{"location":"explanations/251025_HighLevelArchitecture/#c6-ontology-catalog-ontologyloaderpy","title":"C6. Ontology &amp; catalog \u2014 <code>ontology/loader.py</code>","text":"<pre><code>class OntologyCatalog(Protocol):\n    def load(self, inputs: list[dict]) -&gt; list[Concept]: ...\n    def neighbors(self, concept_id: str, depth: int = 1) -&gt; list[str]: ...\n    def get(self, concept_id: str) -&gt; Concept | None: ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#c7-linker-linkinglinkerpy","title":"C7. Linker \u2014 <code>linking/linker.py</code>","text":"<pre><code>class Linker(Protocol):\n    def candidate_concepts(self, chunk_text: str, k_sparse: int, k_lex: int) -&gt; list[str]: ...\n    def score(self, chunk_vec: \"np.ndarray\", concept_vecs: dict[str, \"np.ndarray\"], features: dict) -&gt; float: ...\n    def calibrate(self, devset: list[tuple[float, int]]) -&gt; dict: ...\n    def link_chunk(self, chunk: Chunk) -&gt; list[LinkAssertion]: ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#c8-kg-builder-kg_builderneo4jpy","title":"C8. KG builder \u2014 <code>kg_builder/neo4j.py</code>","text":"<pre><code>class GraphStore(Protocol):\n    def upsert_docs(self, docs: list[Doc]) -&gt; None: ...\n    def upsert_chunks(self, chunks: list[Chunk]) -&gt; None: ...\n    def upsert_concepts(self, concepts: list[Concept]) -&gt; None: ...\n    def upsert_mentions(self, assertions: list[LinkAssertion]) -&gt; None: ...\n    def linked_concepts(self, chunk_id: str) -&gt; list[str]: ...\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#c9-search-api-search_apiapppy","title":"C9. Search API \u2014 <code>search_api/app.py</code>","text":"<ul> <li> <p>Bootstraps FAISS handle(s), BM25, SPLADE, OntologyCatalog, GraphStore, DenseEmbedder; exposes FastAPI app with routers:</p> </li> <li> <p><code>/search</code>, <code>/graph/concepts</code>, <code>/healthz</code>.</p> </li> <li>All responses validated with <code>pydantic</code> models.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#part-d-orchestration-prefect-2x","title":"PART D \u2014 ORCHESTRATION (PREFECT 2.x)","text":""},{"location":"explanations/251025_HighLevelArchitecture/#d1-flow-graph-idempotency-keys","title":"D1. Flow graph &amp; idempotency keys","text":"<pre><code>harvest_and_download \u2192 convert_to_doctags \u2192 chunk_with_docling \u2192 \\\nembed_dense_qwen3 \u2510                                        \\\nencode_splade_v3  \u251c\u2500\u2500 build_bm25_index \u2510                    \\\n                   \u2514\u2500\u2500 build_faiss_index  \u2192 ingest_ontologies \u2192 embed_concepts \\\n                                                   \u2192 link_chunks_to_concepts \u2192 kg_upsert\n</code></pre> <ul> <li> <p>Idempotency keys:</p> </li> <li> <p>Harvest: <code>(topic, years, work_id)</code></p> </li> <li>DocTags: <code>pdf_sha256</code></li> <li>Chunking: <code>(doc_id, chunker_version)</code></li> <li>Dense: <code>(chunk_dataset_id, model=Qwen3, dim=2560)</code></li> <li>SPLADE: <code>(chunk_dataset_id, model=SPLADE-v3, topk=256)</code></li> <li>FAISS: <code>(dense_run_id, factory, nlist, m, nprobe)</code></li> <li>Ontology: <code>(ontology_id, src_uri, loader_version)</code></li> <li>Linker: <code>(chunk_dataset_id, ontology_id, linker_version)</code></li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#d2-concurrency-retries","title":"D2. Concurrency &amp; retries","text":"<ul> <li>Download: 8 workers, retry matrix as in \u00a7B17.</li> <li>VLM/embeddings/SPLADE: GPU queue depth=2; batch auto\u2011tune; 3 retries on transient errors.</li> <li>Index builds: FAISS shards in parallel up to 2; Lucene index single\u2011writer with merges at end.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#d3-cli-commands-invoke-via-typer","title":"D3. CLI commands (invoke via Typer)","text":"<ul> <li><code>kgf harvest --topic \"LLM alignment\" --years 2018..2025 --max 20000</code></li> <li><code>kgf doctags --input /data/pdfs --out /data/doctags</code></li> <li><code>kgf chunk --doctags /data/doctags --out /data/parquet/chunks</code></li> <li><code>kgf embed-dense --chunks &lt;dataset_id&gt;</code></li> <li><code>kgf encode-splade --chunks &lt;dataset_id&gt;</code></li> <li><code>kgf index-bm25 --chunks &lt;dataset_id&gt;</code></li> <li><code>kgf index-faiss --dense-run &lt;run_id&gt;</code></li> <li><code>kgf ingest-ontology --id mesh --uri /data/ontologies/mesh.obo</code></li> <li><code>kgf embed-concepts --ontology mesh</code></li> <li><code>kgf link --chunks &lt;dataset_id&gt; --ontology mesh</code></li> <li><code>kgf kg-upsert --link-run &lt;run_id&gt;</code></li> <li><code>kgf api --port 8080</code></li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#part-e-endtoend-acceptance-criteria-per-workstream","title":"PART E \u2014 END\u2011TO\u2011END ACCEPTANCE CRITERIA (PER WORKSTREAM)","text":"<p>Downloader</p> <ul> <li>Given topic and year range, at least N documents discovered; \u226595% OA downloads succeed; duplicates deduped; license persisted.</li> </ul> <p>DocTags</p> <ul> <li>For each PDF, DocTags exists; pages count &gt; 0; avg_logprob recorded; OCR fallback pages tracked.</li> </ul> <p>Chunking</p> <ul> <li>For each <code>doc_id</code>, sum of chunk spans covers \u226590% of canonical text; token counts in [min,max]; offsets monotonic.</li> </ul> <p>Dense embeddings</p> <ul> <li>Row count == chunk rows; all vectors dim=2560; Parquet schema validated; L2 norms avg in [0.95,1.05] post\u2011norm.</li> </ul> <p>SPLADE/BM25</p> <ul> <li>SPLADE nnz distribution mean \u2248 220\u2013260; Lucene impact index has postings; BM25 index doc count == chunk count.</li> </ul> <p>FAISS</p> <ul> <li>Trained index exists; top\u2011K search returns results; p95 latency \u2264 40\u202fms for K=200 on fixture set.</li> </ul> <p>Ontology &amp; concepts</p> <ul> <li>Loaded concepts &gt; 0; no duplicate CURIEs; concept embeddings Parquet rows == concepts; ontology edges inserted.</li> </ul> <p>Linker</p> <ul> <li>Produces assertions; on fixtures F1 \u2265 0.70; ECE \u2264 0.08; calibration saved and applied.</li> </ul> <p>KG</p> <ul> <li>Node uniqueness constraints enforced; edge counts match assertions; query <code>linked_concepts(chunk)</code> returns expected IDs.</li> </ul> <p>Search API</p> <ul> <li><code>/healthz</code> OK; <code>/search</code> returns k results with explanations; p95 \u2264 300\u202fms on fixtures.</li> </ul>"},{"location":"explanations/251025_HighLevelArchitecture/#part-f-sample-openapi-excerpt","title":"PART F \u2014 SAMPLE OPENAPI (EXCERPT)","text":"<pre><code>openapi: 3.1.0\ninfo: {title: kgfoundry Search API, version: 1.0.0}\npaths:\n  /search:\n    post:\n      security: [{ApiKeyAuth: []}]\n      requestBody:\n        content:\n          application/json:\n            schema:\n              type: object\n              required: [query]\n              properties:\n                query: {type: string, minLength: 1}\n                k: {type: integer, minimum: 1, maximum: 100, default: 10}\n                filters:\n                  type: object\n                  properties:\n                    year_from: {type: integer}\n                    year_to: {type: integer}\n                    source: {type: array, items: {type: string}}\n                    license: {type: array, items: {type: string}}\n                explain: {type: boolean, default: false}\n      responses:\n        '200':\n          description: OK\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  results:\n                    type: array\n                    items:\n                      $ref: '#/components/schemas/Result'\n        '400': {$ref: '#/components/responses/BadRequest'}\n        '401': {$ref: '#/components/responses/Unauthorized'}\n        '429': {$ref: '#/components/responses/TooManyRequests'}\n        '500': {$ref: '#/components/responses/ServerError'}\ncomponents:\n  securitySchemes:\n    ApiKeyAuth:\n      type: http\n      scheme: bearer\n  schemas:\n    Result:\n      type: object\n      required: [doc_id, chunk_id, title, section, score]\n      properties:\n        doc_id: {type: string}\n        chunk_id: {type: string}\n        title: {type: string}\n        section: {type: string}\n        score: {type: number}\n        signals:\n          type: object\n          properties:\n            dense: {type: number}\n            sparse: {type: number}\n            rrf: {type: number}\n            kg_boost: {type: number}\n        spans:\n          type: object\n          properties:\n            start_char: {type: integer}\n            end_char: {type: integer}\n        concepts:\n          type: array\n          items:\n            type: object\n            properties:\n              concept_id: {type: string}\n              label: {type: string}\n              match: {type: string, enum: [direct,nearby]}\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#part-g-implementation-checklists-by-workstream","title":"PART G \u2014 IMPLEMENTATION CHECKLISTS (BY WORKSTREAM)","text":"<ol> <li> <p>Download &amp; Harvest</p> </li> <li> <p>[ ] PyAlex client; config; retries; polite UA.</p> </li> <li>[ ] Fallback Unpaywall client; DOI resolver.</li> <li>[ ] PDF validator; MIME/size checks; hash; dedup; storage path; registry insert.</li> <li> <p>[ ] Metrics/logs/tests; CLI command; E2E hooks.</p> </li> <li> <p>DocTags Conversion</p> </li> <li> <p>[ ] vLLM client; page batches; OCR fallback; canonicalizer; provenance.</p> </li> <li> <p>[ ] Persist <code>.dt.json.zst</code>; register DoctagsAsset; golden tests.</p> </li> <li> <p>Hybrid Chunker</p> </li> <li> <p>[ ] Qwen tokenizer; section splitting; sliding windows; spans &amp; offsets.</p> </li> <li> <p>[ ] Parquet writer; ID scheme; tests for bounds &amp; coverage.</p> </li> <li> <p>Dense Embedder (Qwen3 2560)</p> </li> <li> <p>[ ] OpenAI API client; dimension param; batching; normalization; retries.</p> </li> <li> <p>[ ] Parquet writer; registry run; throughput metric; tests.</p> </li> <li> <p>SPLADE\u2011v3 &amp; BM25</p> </li> <li> <p>[ ] Torch encoder; AMP; top\u2011K; Parquet writer; impact index streaming.</p> </li> <li> <p>[ ] BM25 analyzer config; indexer; search adapter; tests.</p> </li> <li> <p>FAISS GPU/cuVS</p> </li> <li> <p>[ ] Loader; training sampler; add &amp; search; shard manager; save/load.</p> </li> <li> <p>[ ] Query adapter (top\u2011K); memory gauges; tests.</p> </li> <li> <p>Ontology &amp; Concepts</p> </li> <li> <p>[ ] Parsers (OBO/OWL/SKOS); normalization; CURIEs; edges.</p> </li> <li> <p>[ ] Dense &amp; SPLADE embeddings for concepts; Parquet &amp; registry.</p> </li> <li> <p>Linker</p> </li> <li> <p>[ ] Candidate gen (SPLADE+lexicon); feature extractor; fusion; calibration; thresholds.</p> </li> <li> <p>[ ] Assertions Parquet; metrics; tests.</p> </li> <li> <p>KG Builder (Neo4j)</p> </li> <li> <p>[ ] Node/edge upserts; uniqueness constraints; indexes.</p> </li> <li> <p>[ ] Linked concept lookups; tests.</p> </li> <li> <p>Search API</p> <ul> <li>[ ] Startup wiring; RRF; KG boosts; MMR; filters; auth; rate limits.</li> <li>[ ] OpenAPI; integration tests; latency regression tests.</li> </ul> </li> <li> <p>Registry &amp; Orchestration</p> <ul> <li>[ ] Migrations; two\u2011phase dataset registration; events &amp; incidents.</li> <li>[ ] Prefect flows; CLI; idempotency keys; dashboards.</li> </ul> </li> <li> <p>Ops</p> <ul> <li>[ ] systemd units; Nginx config; health checks; log rotation; backup scripts (Parquet+DuckDB).</li> </ul> </li> </ol>"},{"location":"explanations/251025_HighLevelArchitecture/#part-h-example-config-final-ready-to-commit","title":"PART H \u2014 EXAMPLE CONFIG (FINAL; READY TO COMMIT)","text":"<pre><code>system:\n  os: ubuntu-24.04\n  threads: 14\n  seed: 42\n  parquet_root: /data/parquet\n  artifacts_root: /data/artifacts\n  duckdb_path: /data/catalog/catalog.duckdb\n\nruntime:\n  python: \"3.13\"\n  cuda: \"13.0\"\n  torch: \"2.9\"\n  duckdb_min: \"1.4.1\"\n  vllm_channel: \"pre-release-cuda13\"\n\nnetwork:\n  nginx_port: 80\n  vlm_port: 8001\n  emb_port: 8002\n  api_port: 8080\n\nharvest:\n  provider: pyalex\n  per_page: 200\n  max_works: 20000\n  years: \"&gt;=2018\"\n  filters:\n    is_oa: true\n    has_oa_published_version: true\n  fallbacks:\n    unpaywall: true\n    arxiv: true\n    pmc: true\n  concurrency: 8\n  timeout_sec: 60\n  retries: 3\n\ndoc_conversion:\n  vlm_model: ibm-granite/granite-docling-258M\n  vlm_revision: untied\n  endpoint: http://localhost/vlm/\n  dpi: 220\n  page_batch: 8\n  ocr_fallback: true\n  max_pages: 2000\n  timeout_sec: 120\n\nchunking:\n  engine: docling_hybrid\n  target_tokens: 400\n  overlap_tokens: 80\n  min_tokens: 120\n  max_tokens: 480\n\ndense_embedding:\n  model: Qwen/Qwen3-Embedding-4B\n  endpoint: http://localhost/v1/embeddings\n  output_dim: 2560\n  parquet_out: ${system.parquet_root}/dense/model=Qwen3-Embedding-4B/run=${run_id}\n\nsparse_embedding:\n  splade:\n    model: naver/splade-v3-distilbert\n    device: cuda\n    amp: fp16\n    max_seq_len: 512\n    topk: 256\n    parquet_out: ${system.parquet_root}/sparse/model=SPLADE-v3-distilbert/run=${run_id}\n  bm25:\n    k1: 0.9\n    b: 0.4\n    field_boosts: { title: 2.0, section: 1.2, body: 1.0 }\n    index_dir: /data/lucene/bm25\n\nfaiss:\n  index_factory: OPQ64,IVF8192,PQ64\n  nprobe: 64\n  train_samples: 10000000\n  shards:\n    max_vectors_per_shard: 10000000\n  gpu: true\n  cuvs: true\n  output_dir: /data/faiss/qwen3_ivfpq\n\nontology:\n  inputs:\n    - { ontology_id: mesh, format: obo, uri: /data/ontologies/mesh.obo }\n    - { ontology_id: go,   format: obo, uri: /data/ontologies/go.obo }\n  concept_embed:\n    dense_model: Qwen3-Embedding-4B\n    dense_dim: 2560\n    splade_model: SPLADE-v3-distilbert\n    splade_topk: 128\n\nlinker:\n  candidates: { splade_topk: 100, lexicon_topk: 50 }\n  fusion_weights: { dense: 0.55, sparse: 0.35, lexical: 0.10, depth_bonus_per_level: 0.02, depth_cap: 0.10 }\n  thresholds: { high: 0.62, low: 0.35 }\n  calibration: isotonic\n\ngraph:\n  backend: neo4j\n  uri: bolt://localhost:7687\n  user: neo4j\n  password_env: NEO4J_PASSWORD\n\nsearch:\n  k: 10\n  dense_candidates: 200\n  sparse_candidates: 200\n  rrf_k: 60\n  mmr_lambda: 0.7\n  kg_boosts: { direct: 0.08, one_hop: 0.04 }\n</code></pre>"},{"location":"explanations/251025_HighLevelArchitecture/#part-i-what-happens-next","title":"PART I \u2014 WHAT HAPPENS NEXT","text":"<p>With these resolved specifications:</p> <ul> <li>Each workstream has clear interfaces, error types, Parquet schemas, DuckDB DDL, router/API contracts, orchestration flows, observability, SLAs, and acceptance tests.</li> <li>Teams can now draft detailed implementation plans and begin coding, confident that components will integrate cleanly on the single\u2011box target environment.</li> </ul> <p>If you want, I can generate skeleton packages (directories, ABCs, Pydantic models, Typer CLI, Prefect flows, Makefile, pre\u2011commit, and sample tests) to serve as the starting point for implementation.</p>"},{"location":"explanations/numpy-docstring-migration/","title":"Migration: Strict NumPy Docstring Compliance","text":"<p>This change removes legacy <code>NavMap:</code> blocks from module docstrings and enables full NumPy-style validation across the documentation pipeline. Use this guide to understand the impact and to verify your environment.</p>"},{"location":"explanations/numpy-docstring-migration/#summary-of-changes","title":"Summary of Changes","text":"<ul> <li>Sphinx now loads the <code>numpydoc</code> and <code>numpydoc_validation</code> extensions and treats   <code>GL01</code>, <code>SS01</code>, <code>ES01</code>, <code>RT01</code>, and <code>PR01</code> violations as build failures (<code>nitpicky = True</code>).</li> <li>Custom doq templates generate NumPy-compliant docstrings (including <code>Raises</code> and   <code>Examples</code> sections) for functions, classes, and modules.</li> <li><code>tools/auto_docstrings.py</code> rewrites missing or placeholder docstrings to include   <code>Parameters</code>, <code>Returns</code>, <code>Raises</code>, <code>Examples</code>, <code>See Also</code>, and <code>Notes</code> sections.</li> <li><code>tools/update_navmaps.py</code> is now a validator that aborts if any docstring still   contains a <code>NavMap:</code> section.</li> <li>Pre-commit gains a <code>pydoclint --style numpy src</code> step that enforces parameter/return parity.</li> <li><code>.numpydoc</code> configures repository-wide validation checks during Sphinx builds.</li> </ul>"},{"location":"explanations/numpy-docstring-migration/#performance","title":"Performance","text":"<p>Docstring regeneration now triggers additional validation (numpydoc + pydoclint). On a clean checkout this adds roughly a few seconds to <code>make docstrings</code>; monitor <code>tools/update_docs.sh</code> runtimes after large refactors and adjust tooling caches if needed.</p>"},{"location":"explanations/numpy-docstring-migration/#what-contributors-need-to-do","title":"What Contributors Need to Do","text":"<ol> <li>Regenerate docstrings after modifying public APIs:    <code>bash    make docstrings</code></li> <li>Run the documentation pipeline to confirm there are no warnings:    <code>bash    tools/update_docs.sh</code></li> <li>Execute the doctest suite to ensure <code>Examples</code> sections remain runnable:    <code>bash    pytest --doctest-modules src</code></li> <li>If pre-commit is not installed, install it and run all hooks:    <code>bash    pre-commit install    pre-commit run --all-files</code></li> </ol>"},{"location":"explanations/numpy-docstring-migration/#rollback-strategy","title":"Rollback Strategy","text":"<p>To revert to the previous behaviour:</p> <ol> <li>Remove the <code>numpydoc</code> extensions from <code>docs/conf.py</code> and restore the Napoleon defaults.</li> <li>Delete <code>.numpydoc</code> and revert the docstring templates in <code>tools/doq_templates/numpy/</code>.</li> <li>Restore <code>tools/update_navmaps.py</code> to inject <code>NavMap:</code> sections (and rerun the script).</li> <li>Drop the <code>pydoclint</code> dependency and hook from <code>pyproject.toml</code> and <code>.pre-commit-config.yaml</code>.</li> </ol> <p>Rolling back should be accompanied by regenerating docstrings (<code>make docstrings</code>) so that older Google-style docstrings can be reintroduced if necessary.</p>"},{"location":"how-to/","title":"How-to Guides","text":"<p>Practical recipes for common workflows will live here. Add a page per topic and extend the toctree below.</p> <pre><code>:maxdepth: 1\n:hidden:\n\ncontributing\ncontributing-gallery-examples\n</code></pre>"},{"location":"how-to/contributing-gallery-examples/","title":"Contributing Sphinx-Gallery Examples","text":"<p>Sphinx-Gallery builds our example gallery directly from the Python modules in <code>examples/</code>. Every file must follow a predictable docstring structure so the generated pages have stable titles and cross-reference anchors. This guide walks through the required format, explains the validation tooling, and provides troubleshooting tips for common issues.</p>"},{"location":"how-to/contributing-gallery-examples/#quick-start-checklist","title":"Quick start checklist","text":"<ol> <li>Copy the docstring template below.</li> <li>Update the title and description to match your example.</li> <li>Tag the example with relevant keywords using the <code>.. tags::</code> directive.</li> <li>Describe resource expectations inside the <code>Constraints</code> section.</li> <li>Run <code>python tools/validate_gallery.py --verbose</code> before committing.</li> <li>Execute <code>pytest --doctest-modules examples/&lt;your-file&gt;.py</code> to confirm code    snippets still run.</li> </ol>"},{"location":"how-to/contributing-gallery-examples/#docstring-template-docstring-template","title":"Docstring template {#docstring-template}","text":"<p>Each example begins with a module-level docstring that Sphinx-Gallery parses to create the page heading. The first non-empty line becomes the HTML title and is also used to derive the <code>sphx_glr_</code> reference label.</p> <pre><code>\"\"\"My concise title (\u226479 characters)\n====================================\n\nOne or two sentences describing what the example demonstrates.\n\n.. tags:: topic-one, topic-two\n\nConstraints\n-----------\n\n- Time: &lt;2s\n- GPU: no\n- Network: no\n\n&gt;&gt;&gt; # Optional doctest snippets\n&gt;&gt;&gt; answer = 40 + 2\n&gt;&gt;&gt; answer\n42\n\"\"\"\n</code></pre>"},{"location":"how-to/contributing-gallery-examples/#title-and-underline","title":"Title and underline","text":"<ul> <li>The first line must be plain text with no trailing period.</li> <li>The underline must consist only of <code>=</code> characters and match the title length   within \u00b11 character.</li> <li>Add a blank line between the underline and the body text.</li> </ul>"},{"location":"how-to/contributing-gallery-examples/#tags-directive","title":"Tags directive","text":"<p>Use <code>.. tags::</code> to help readers (and search) understand the categories an example belongs to. Separate multiple tags with commas. The validator treats the directive as mandatory.</p>"},{"location":"how-to/contributing-gallery-examples/#constraints-section","title":"Constraints section","text":"<p>Summarise runtime expectations in a dedicated <code>Constraints</code> section using a dashed underline. List each constraint as a bullet item such as execution time, GPU usage, or network access.</p>"},{"location":"how-to/contributing-gallery-examples/#doctests","title":"Doctests","text":"<p>Include <code>&gt;&gt;&gt;</code> prompts when demonstrating Python usage. All examples are doctested via <code>pytest --doctest-modules examples/</code>, so keep snippets small and deterministic.</p>"},{"location":"how-to/contributing-gallery-examples/#validation-tooling","title":"Validation tooling","text":"<p>The <code>tools/validate_gallery.py</code> script enforces the required structure. It is installed as a pre-commit hook and runs automatically whenever gallery files are staged, but you can also invoke it manually:</p> <pre><code>python tools/validate_gallery.py --verbose\n</code></pre>"},{"location":"how-to/contributing-gallery-examples/#cli-options","title":"CLI options","text":"<ul> <li><code>--examples-dir PATH</code> \u2013 Validate a different directory (default: <code>examples/</code>).</li> <li><code>--strict</code> \u2013 Enable additional checks, such as rejecting titles that end with   a period.</li> <li><code>--verbose</code> \u2013 Print confirmation for passing files.</li> <li><code>--fix</code> \u2013 Reserved for future automatic formatting helpers.</li> </ul> <p>The script exits with code <code>0</code> when all examples pass or <code>1</code> when it reports any violations. Missing directories or unsupported flags cause exit code <code>2</code>.</p>"},{"location":"how-to/contributing-gallery-examples/#common-validation-errors","title":"Common validation errors","text":"Error message How to fix <code>docstring is empty</code> Ensure the file starts with a module docstring. <code>title exceeds 79 characters</code> Shorten the first line of the docstring. <code>title underline must be composed of '=' characters</code> Replace decorative underlines with <code>=</code> characters. <code>remove ':orphan:' directive</code> Delete legacy <code>:orphan:</code> directives\u2014Sphinx-Gallery adds anchors automatically. <code>add a '.. tags::' directive</code> Insert a tags directive underneath the description. <code>add a 'Constraints' section with a dashed underline</code> Document runtime requirements under a <code>Constraints</code> heading."},{"location":"how-to/contributing-gallery-examples/#local-testing-workflow","title":"Local testing workflow","text":"<ol> <li>Run <code>python tools/validate_gallery.py --verbose</code> after editing examples.</li> <li>Execute <code>pytest --doctest-modules examples/&lt;file&gt;.py</code> to run doctests for a    single example.</li> <li>Execute <code>pytest --doctest-modules examples/</code> to run the entire suite.</li> <li>Build the docs with <code>tools/update_docs.sh</code> to confirm Sphinx renders pages    without warnings.</li> </ol>"},{"location":"how-to/contributing-gallery-examples/#debugging-sphinx-gallery-issues","title":"Debugging Sphinx-Gallery issues","text":"<ul> <li>Use <code>sphinx-build -b html docs docs/_build/html -n</code> to surface unresolved   references.</li> <li>Inspect the generated reStructuredText in <code>docs/gallery/</code> when titles or   anchors look incorrect.</li> <li>Ensure every gallery file imports quickly and avoids heavyweight dependencies.</li> <li>Keep example output deterministic so doctest snapshots remain stable.</li> </ul> <p>For a full overview of the automated documentation pipeline, see the repository's <code>README-AUTOMATED-DOCUMENTATION.md</code>.</p>"},{"location":"how-to/contributing/","title":"Contributing Docstrings and Documentation","text":"<p>The documentation pipeline now enforces strict NumPy-style docstrings across the entire project. Follow the guidelines below whenever you add or update code so that pre-commit hooks and the automated documentation build succeed.</p>"},{"location":"how-to/contributing/#required-sections","title":"Required Sections","text":"<p>Every public function must include the following sections (omit only when empty):</p> <ul> <li><code>Parameters</code> with entries of the form <code>name : type</code> and <code>, optional</code> when defaults exist</li> <li><code>Returns</code> describing the return type/value (skip if the function returns <code>None</code>)</li> <li><code>Raises</code> listing each exception that can be raised and the condition</li> <li><code>Examples</code> containing at least one doctestable block using <code>&gt;&gt;&gt;</code></li> <li><code>See Also</code> referencing related functions, classes, or modules</li> <li><code>Notes</code> summarising constraints, complexity, or behavioural caveats</li> </ul> <p>Public classes must also document an <code>Attributes</code> section and list each public method under <code>Methods</code>. When no attributes exist, include <code>None</code> with a short explanation.</p> <p>Module docstrings should start with an imperative summary and may include <code>Notes</code>, <code>See Also</code>, or <code>References</code>. Custom sections such as <code>NavMap:</code> are no longer allowed.</p>"},{"location":"how-to/contributing/#templates","title":"Templates","text":"<p>Docstring generation uses templates in <code>tools/doq_templates/numpy/</code>. When writing manual docstrings, mirror the structure below:</p> <pre><code>\"\"\"Summarise the function in the imperative mood.\n\nParameters\n----------\narg : type\n    Concise description of the argument.\n\nReturns\n-------\nreturn_type\n    Explain what is returned.\n\nRaises\n------\nExceptionType\n    State the condition that triggers the exception.\n\nExamples\n--------\n&gt;&gt;&gt; from package.module import function\n&gt;&gt;&gt; result = function(...)\n&gt;&gt;&gt; result  # doctest: +ELLIPSIS\n...\n\nSee Also\n--------\npackage.module.other_function\n\nNotes\n-----\nMention constraints, complexity, or other important behaviour.\n\"\"\"\n</code></pre>"},{"location":"how-to/contributing/#validation-pipeline","title":"Validation Pipeline","text":"<p>The following tools run automatically in CI and via <code>pre-commit</code>:</p> <ul> <li><code>pydoclint --style numpy src</code> enforces parameter/return parity</li> <li><code>pydocstyle</code> checks NumPy docstring formatting</li> <li><code>interrogate -i src --fail-under 90</code> maintains 90% coverage</li> <li><code>python tools/update_navmaps.py</code> aborts if any docstring still contains <code>NavMap:</code></li> </ul> <p>Run these locally before pushing:</p> <pre><code>make docstrings            # Regenerate docstrings using the latest templates\npre-commit run --all-files # Run the full docstring validation suite\npytest --doctest-modules src\n</code></pre> <p>Refer to the NumPy docstring style guide for additional examples and best practices.</p>"},{"location":"how-to/read-package-readmes/","title":"How to Read Package READMEs","text":"<p>The autogenerated package READMEs provide a quick, structured view of every public symbol in a package. This guide explains how to read them, how badges and links are constructed, and which commands keep the documents fresh.</p>"},{"location":"how-to/read-package-readmes/#readme-structure-at-a-glance","title":"README Structure at a Glance","text":"<p>Every generated README follows the same layout:</p> <ol> <li>H1 Title \u2013 <code># `&lt;package&gt;`</code> where <code>&lt;package&gt;</code> is the fully qualified    package name.</li> <li>Synopsis \u2013 The first sentence of the package <code>__init__</code> docstring. If    no docstring exists you will see the fallback sentence    \u201cPackage synopsis not yet documented.\u201d</li> <li>DocToc Markers \u2013 Two HTML comments that mark where DocToc inserts the    table of contents when you run it locally or via <code>--run-doctoc</code>.</li> <li>Sections \u2013 Symbols are grouped deterministically into    <code>Modules</code>, <code>Classes</code>, <code>Functions</code>, and <code>Exceptions</code>. Empty sections are    omitted.</li> <li>Entries \u2013 Each symbol renders as:</li> </ol> <p><code>markdown    - **`package.symbol`** \u2014 First sentence of the symbol docstring      `stability:beta` `owner:@docs` `section:getting-started`      `since:0.2.0` `deprecated:0.5.0` `tested-by: tests/unit/test_symbol.py:42`      \u2192 [open](vscode://file/ABS:LINE:1) | [view](https://github.com/org/repo/blob/SHA/path#Lstart-Lend)</code></p>"},{"location":"how-to/read-package-readmes/#badge-reference","title":"Badge Reference","text":"<p>Badges expose metadata captured in the NavMap (<code>site/_build/navmap/navmap.json</code>) and TestMap (<code>docs/_build/test_map.json</code>) artifacts. They always appear in the following order:</p> Badge Meaning Source <code>stability:&lt;state&gt;</code> Lifecycle state (<code>stable</code>, <code>beta</code>, <code>experimental</code>, <code>deprecated</code>) NavMap <code>meta</code> or <code>module_meta</code> <code>owner:&lt;@team&gt;</code> Responsible team or individual NavMap <code>section:&lt;id&gt;</code> Documentation section identifier NavMap <code>sections[*].id</code> <code>since:&lt;version&gt;</code> Version when the symbol was introduced NavMap <code>deprecated:&lt;version&gt;</code> Version when the symbol became deprecated NavMap <code>tested-by:file[:line]</code> Up to three test files that cover the symbol TestMap <p>Badges wrap onto a new line once the combined summary + badge text exceeds 80 characters; continuation lines are indented with four spaces so wrapped badges align visually.</p>"},{"location":"how-to/read-package-readmes/#link-types","title":"Link Types","text":"<p>Two link columns appear to the right of each entry depending on the selected link mode:</p> <ul> <li>[open] \u2013 Deep-link into your editor.</li> <li>VSCode: <code>vscode://file/&lt;abs_path&gt;:&lt;line&gt;:1</code></li> <li>Relative: <code>./&lt;repo-relative-path&gt;:&lt;line&gt;:1</code></li> <li>[view] \u2013 GitHub permalink to the exact line range in the current commit:   <code>https://github.com/&lt;org&gt;/&lt;repo&gt;/blob/&lt;sha&gt;/&lt;path&gt;#L&lt;start&gt;-L&lt;end&gt;</code></li> </ul> <p>Use <code>--link-mode editor</code>, <code>--link-mode github</code>, or <code>--link-mode both</code> to control which column appears. Pair <code>--editor relative</code> with <code>--link-mode editor</code> when you share READMEs outside of VSCode.</p>"},{"location":"how-to/read-package-readmes/#when-and-how-to-regenerate-readmes","title":"When and How to Regenerate READMEs","text":"<p>Regenerate READMEs whenever you:</p> <ul> <li>modify docstrings or public APIs in a package</li> <li>update NavMap/TestMap metadata (owners, stability, tests)</li> <li>add or remove Python modules, classes, functions, or exceptions</li> </ul> <p>To regenerate all READMEs:</p> <pre><code>python tools/gen_readmes.py --link-mode both --editor vscode\n</code></pre> <p>To regenerate a subset, set <code>DOCS_PKG</code> (comma-separated list) or pass <code>--packages</code> explicitly.</p>"},{"location":"how-to/read-package-readmes/#doctoc-integration","title":"DocToc Integration","text":"<p>Pass <code>--run-doctoc</code> to invoke DocToc automatically after each README write. The command checks for the <code>doctoc</code> binary (npm) and skips gracefully if it is not installed.</p>"},{"location":"how-to/read-package-readmes/#cli-flags-and-environment-variables","title":"CLI Flags and Environment Variables","text":"Flag Environment Variable Description <code>--packages</code> <code>DOCS_PKG</code> Comma-separated list of packages to document. Auto-detected when omitted. <code>--link-mode</code> <code>DOCS_LINK_MODE</code> <code>github</code>, <code>editor</code>, or <code>both</code>. Defaults to <code>both</code>. <code>--editor</code> <code>DOCS_EDITOR</code> <code>vscode</code> or <code>relative</code>. Controls editor link format. <code>--fail-on-metadata-miss</code> \u2013 Exit with code 2 when <code>owner</code> or <code>stability</code> metadata is missing. <code>--dry-run</code> \u2013 Print planned writes without touching the filesystem. <code>--verbose</code> \u2013 Emit timing information and DocToc output. <code>--run-doctoc</code> \u2013 Run DocToc after writing each README (requires the <code>doctoc</code> CLI). <p>Additional environment overrides: <code>DOCS_GITHUB_ORG</code>, <code>DOCS_GITHUB_REPO</code>, and <code>DOCS_GITHUB_SHA</code> customise the permalink target when working outside of a git repository.</p>"},{"location":"how-to/read-package-readmes/#examples","title":"Examples","text":"<p>Generate READMEs for two packages and open links in VSCode</p> <pre><code>DOCS_PKG=kgfoundry_common,download \\\npython tools/gen_readmes.py --link-mode both --editor vscode\n</code></pre> <p>Render GitHub-only links for CI validation</p> <pre><code>python tools/gen_readmes.py --link-mode github --editor relative\n</code></pre> <p>Preview without writing to disk</p> <pre><code>python tools/gen_readmes.py --dry-run --packages kgfoundry_common\n</code></pre>"},{"location":"how-to/read-package-readmes/#troubleshooting","title":"Troubleshooting","text":"Symptom Likely Cause Fix Missing badges NavMap/TestMap not built Run <code>python tools/navmap/build_navmap.py</code> and <code>python tools/docs/build_test_map.py</code> <code>tested-by</code> empty No tests recorded for the symbol Add coverage to <code>docs/_build/test_map.json</code> via <code>build_test_map.py</code> No README written Package lacks public symbols Add public modules/classes/functions or run with <code>--verbose</code> to inspect filtering Absolute VSCode paths Using default <code>vscode</code> editor mode outside repo root Switch to <code>--editor relative</code> DocToc skipped CLI not installed <code>npm install -g doctoc</code>"},{"location":"how-to/read-package-readmes/#quick-commands","title":"Quick Commands","text":"<ul> <li>Regenerate everything: <code>python tools/gen_readmes.py</code></li> <li>One package only: <code>DOCS_PKG=kgfoundry_common python tools/gen_readmes.py</code></li> <li>Run DocToc automatically: <code>python tools/gen_readmes.py --run-doctoc</code></li> <li>Enforce metadata completeness: <code>python tools/gen_readmes.py --fail-on-metadata-miss</code></li> <li>Validate README freshness (CI-equivalent):</li> </ul> <p><code>bash   python tools/gen_readmes.py --link-mode github --editor relative   git diff -- src/**/README.md</code></p>"},{"location":"how-to/read-package-readmes/#relationship-to-navmap-and-testmap","title":"Relationship to NavMap and TestMap","text":"<ul> <li>The NavMap drives ownership, stability, section, and version badges. If   a symbol lacks metadata the generator falls back to module defaults.</li> <li>The TestMap contributes up to three \u201ctested-by\u201d references per symbol,   prioritising unit tests first. Missing TestMap entries simply omit the badge.</li> </ul> <p>Keep both artifacts current to produce meaningful READMEs.</p>"},{"location":"policies/visibility-policy/","title":"NavMap Visibility Policy","text":""},{"location":"policies/visibility-policy/#visibility-policy-navmap","title":"Visibility Policy (NavMap)","text":"<p>This repository treats public API as the union of: 1) <code>__all__</code> in each module, and 2) <code>__navmap__[\"exports\"]</code> (may reference <code>__all__</code> but must match set-wise).</p> <p>Required per exported symbol: - <code>owner</code>: team handle (e.g., <code>@core-search</code>) - <code>stability</code>: <code>stable</code> | <code>beta</code> | <code>experimental</code> | <code>deprecated</code> - <code>since</code>: PEP 440 version string (e.g., <code>0.8.0</code>) - <code>deprecated_in</code> (optional): PEP 440; if present, must be &gt;= <code>since</code>.</p> <p>Sections - First section must be <code>public-api</code>. - Section IDs must be kebab-case (e.g., <code>domain-models</code>). - Each section symbol must have a nearby inline anchor: <code># [nav:anchor Name]</code>.</p> <p>Links - Editor: <code>vscode://file/&lt;path&gt;:&lt;line&gt;:&lt;col&gt;</code> (official scheme). - GitHub: <code>https://github.com/&lt;org&gt;/&lt;repo&gt;/blob/&lt;sha&gt;/&lt;path&gt;#Lstart-Lend</code> (permalink with <code>#L</code> anchors).</p> <p>Violations are reported by <code>tools/navmap/check_navmap.py</code>.</p>"},{"location":"reference/test-matrix/","title":"Test Matrix (symbol \u2192 tests)","text":"<p><code>{include} ../_build/test_map.json :literal:</code></p>"},{"location":"reference/graphs/","title":"Package Graphs","text":"<p>Below are generated import and UML diagrams for the project packages.</p> <p>```{image} ../../_build/graphs/kgfoundry-imports.svg :alt: kgfoundry import graph :width: 100%</p> <pre><code>\n```{image} ../../_build/graphs/kgfoundry-uml.svg\n:alt: kgfoundry UML diagram\n:width: 100%\n</code></pre>"},{"location":"reference/graphs/#subsystems-policy-aware","title":"Subsystems (policy-aware)","text":"<p>```{image} ../../_build/graphs/subsystems.svg :alt: Cross-subsystem import graph (cycles red; thickness ~ centrality) :width: 100%</p> <pre><code>\n```{include} ../../_build/graphs/subsystems_meta.md\n</code></pre>"},{"location":"reference/observability/config/","title":"Configuration surfaces (Pydantic Settings &amp; env usage)","text":"<p><code>{include} ../../_build/config.md</code></p>"},{"location":"reference/observability/logs/","title":"Log events (static scan)","text":"<p><code>{include} ../../_build/log_events.json :literal:</code></p>"},{"location":"reference/observability/metrics/","title":"Metrics (static scan)","text":"<p><code>{include} ../../_build/metrics.json :literal:</code> (auto-generated via <code>tools/docs/scan_observability.py</code>; run before building docs)</p>"},{"location":"reference/observability/traces/","title":"Traces (static scan)","text":"<p><code>{include} ../../_build/traces.json :literal:</code></p>"},{"location":"reference/schemas/","title":"Data Contract Schemas","text":"<pre><code>.. toctree::\n   :maxdepth: 1\n   :glob:\n\n   *.json\n</code></pre>"}]}